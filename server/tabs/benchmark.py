import streamlit as st
from streamlit import session_state as sts
import json
import traceback

from benchmark_utils import leanaide_io, llm_raw_io
from serv_utils import preview_text, lean_code_button
from logging_utils import log_write, load_env

st.warning("Under Constuction: This page is not fully functional yet. Please check back later.")

st.title("Benchmarking LeanAide")
st.write("Here you can benchmark the performace of Lean Code generated Using LeanAide vs raw LLM output.")
st.info("You can enter a single theorem and proof, or a dataset of theorems and proofs in JSON format.")

st.subheader("Input Dataset", divider =True)

load_env()  

sts.bm_input_opt = st.radio("Dataset Type", options = ["JSON Dataset", "Single Input"], index = 0, horizontal = True)
if sts.bm_input_opt == "JSON Dataset":
    uploaded_file = st.file_uploader(
        "Upload a JSON file containing a dataset of theorems and proofs.",
        type=["json"],
    )

    if uploaded_file:
        sts.bm_json_dataset = json.load(uploaded_file)
        st.success(f"Dataset loaded successfully!\nNumber of Problems in dataset: {len(sts.bm_json_dataset)}")
    else:
        sts.bm_json_dataset = {}
        st.warning("Please upload a JSON file containing a dataset of theorems and proofs.")

else:
    sts.bm_single_thm = st.text_area(
        "Enter theorem",
        placeholder="Enter a theorem here...",
        value = sts.get("bm_single_thm", ""),
    )
    preview_text("bm_single_thm", usage = "benchmark_thm", caption = "Theorem")
    
    sts.bm_single_proof = st.text_area(
        "Enter proof",
        placeholder="Enter a proof here...",
        value = sts.get("bm_single_proof", ""),
    )
    preview_text("bm_single_proof", usage = "benchmark_pf", caption = "Proof")
    
st.subheader("Evaluation", divider = True)
st.info("The LLM provided in the sidebar API Credentials will be used for the benchmark. Please ensure it is set up correctly.")

def _run_both_io(input_data, llm_provider, model):
    """Run both LeanAide and raw LLM IO functions."""
    result_ln = leanaide_io(
        input_data=input_data,
        llm_provider=llm_provider,
        model=model
    )
    result_ai = llm_raw_io(
        input_data=input_data,
        llm_provider=llm_provider,
        model=model
    )
    return {
        "time_taken_ln": result_ln.get("time_taken", -1),
        "result_ln": result_ln.get("lean_code", "sorry -- No output generated by LeanAide"),
        "time_taken_ai": result_ai.get("time_taken", -1),
        "result_ai": result_ai.get("lean_code", "sorry -- No output generated by the LLM")
    }

def run_benchmark():
    # Initialize/reset progress
    sts.bm_results = {}
    sts.bm_current_progress = 0
    sts.bm_total_problems = len(sts.get("bm_json_dataset", [])) if sts.bm_input_opt == "JSON Dataset" else 1

    # Create progress bar and status placeholder
    progress_bar = st.progress(0)
    status_text = st.empty()
    results_container = st.empty()  # Dynamic results display
    _update_ui(progress_bar, status_text, results_container)

    try:
        print("Initializing benchmark with JSON dataset...")
        log_write("Benchmark", "Initializing benchmark with JSON dataset...")
        if sts.bm_input_opt == "JSON Dataset":
            # Process each problem in the dataset
            for idx in sts.bm_json_dataset:
                problem = sts.bm_json_dataset[idx]
                input_data = {
                    "theorem": problem.get("theorem", ""),
                    "proof": problem.get("proof", "")
                }
                if not input_data["theorem"]:
                    print(f"Skipping problem {idx} due to missing theorem.")
                    idx = int(idx) + 1 
                    continue

                # result_ai should be a lean code
                # Update session state and UI
                print("Running for Problem - LeanAide: ", idx)
                result = _run_both_io(
                    input_data=input_data,
                    llm_provider=sts.get("llm_provider", "openai"),
                    model=sts.get("model_leanaide", "gpt-4o")
                )
                print(f"Result for Problem - AI: {idx}")
                sts.bm_results[idx] = {
                    "problem": input_data["theorem"],
                    "proof": input_data["proof"], 
                    "time_taken_ln": result["time_taken_ln"],
                    "result_ln": result["result_ln"],
                    "time_taken_ai": result["time_taken_ai"],
                    "result_ai": result["result_ai"],
                }
                sts.bm_current_progress = int(idx)
                _update_ui(progress_bar, status_text, results_container)
        else:
            # Single input case
            result = _run_both_io(
                input_data={
                    "theorem": sts.bm_single_thm,
                    "proof": sts.bm_single_proof
                },
                llm_provider=sts.get("llm_provider", "openai"),
                model=sts.get("model_leanaide", "gpt-4o")
            )
            sts.bm_results[idx] = {
                "problem": sts.bm_single_thm,
                "proof": sts.bm_single_proof,
                "time_taken_ln": result["time_taken_ln"],
                "result_ln": result["result_ln"],
                "time_taken_ai": result["time_taken_ai"],
                "result_ai": result["result_ai"],
            }
            _update_ui(progress_bar, status_text, results_container)

        st.success("Benchmark completed successfully!")
        sts.bm_result_success = True
    except Exception as e:
        st.error(f"Error during benchmark: {str(e)}")
        print(traceback.format_exc())
        sts.bm_result_success = False
    finally:
        return sts.bm_results

def bm_display_results(): 
    for idx in sts.bm_results:
        result_data = sts.bm_results[idx]
        print(result_data)
        # Show result_data in a formatted way
        st.markdown(f"### Problem: {sts.bm_current_progress}")

        # Timing table
        head_timetable = ["LeanAide Time (s)", "LLM Time (s)"] 
        time_table = [str(result_data["time_taken_ln"]), str(result_data["time_taken_ai"])]
        markdown_time_table = "| " + " | ".join(head_timetable) + " |\n"
        markdown_time_table += "| " + " | ".join(["---"] * len(head_timetable)) + " |\n"
        markdown_time_table += "| " + " | ".join(time_table) + " |\n  "

        # Print output
        st.markdown(f"LeanAide Lean Code:\n```lean\n{result_data['result_ln']}\n```")
        lean_code_button("bm_results", "results_ln", f"bm_lean_code_{idx}")
        st.markdown(f"**LLM Lean Code:**\n```lean\n{result_data['result_ai']}\n```")
        lean_code_button("bm_results", "results_ai", f"bm_llm_code_{idx}")
        st.markdown(markdown_time_table)
        st.markdown("---")  # Separator for each

def _update_ui(progress_bar, status_text, results_container):
    """Helper to update Streamlit UI components."""
    progress = sts.bm_current_progress / sts.bm_total_problems
    progress_bar.progress(progress)
    status_text.markdown(f"**Progress:** {sts.bm_current_progress}/{sts.bm_total_problems}")

    # Display results incrementally
    with results_container.container():
        bm_display_results()

# Button trigger
sts.bm_run_button = st.button("Run Benchmark")
if sts.bm_run_button:
    if sts.bm_json_dataset or (sts.bm_single_thm and sts.bm_single_proof):
        with st.spinner("Running benchmark..."):
            sts.bm_results = run_benchmark()
    else:
        st.warning("Please upload a dataset or enter a single theorem and proof before running the benchmark.")

if sts.bm_results and sts.bm_result_success:
    st.success("Benchmark completed successfully!")
    bm_display_results()

# Convert results to JSON and download
output_file = "benchmark_results.json"
with open(output_file, 'w') as f:
    json.dump(sts.bm_results, f, indent=4)

if sts.bm_results:
    st.download_button(
        label="Download Results",
        data=json.dumps(sts.bm_results, indent=4),
        file_name=output_file,
        mime="application/json"
    )
