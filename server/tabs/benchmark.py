import streamlit as st
from streamlit import session_state as sts
import json
import traceback

from benchmark_utils import leanaide_io, llm_raw_io
from serv_utils import preview_text
from logging_utils import log_write

st.warning("Under Constuction: This page is not fully functional yet. Please check back later.")

st.title("Benchmarking LeanAide")
st.write("Here you can benchmark the performace of Lean Code generated Using LeanAide vs raw LLM output.")
st.info("You can enter a single theorem and proof, or a dataset of theorems and proofs in JSON format.")

st.subheader("Input Dataset", divider =True)

sts.bm_input_opt = st.radio("Dataset Type", options = ["JSON Dataset", "Single Input"], index = 0, horizontal = True)
if sts.bm_input_opt == "JSON Dataset":
    uploaded_file = st.file_uploader(
        "Upload a JSON file containing a dataset of theorems and proofs.",
        type=["json"],
    )

    if uploaded_file:
        sts.bm_json_dataset = json.load(uploaded_file)
        st.success(f"Dataset loaded successfully!\nNumber of Problems in dataset: {len(sts.bm_json_dataset)}")
    else:
        sts.bm_json_dataset = {}
        st.warning("Please upload a JSON file containing a dataset of theorems and proofs.")

else:
    sts.bm_single_thm = st.text_area(
        "Enter theorem",
        placeholder="Enter a theorem here...",
        value = sts.get("bm_single_thm", ""),
    )
    preview_text("bm_single_thm", usage = "benchmark_thm", caption = "Theorem")
    
    sts.bm_single_proof = st.text_area(
        "Enter proof",
        placeholder="Enter a proof here...",
        value = sts.get("bm_single_proof", ""),
    )
    preview_text("bm_single_proof", usage = "benchmark_pf", caption = "Proof")
    
st.subheader("Evaluation", divider = True)
st.info("The LLM provided in the sidebar API Credentials will be used for the benchmark. Please ensure it is set up correctly.")

def _run_both_io(input_data, llm_provider, model):
    """Run both LeanAide and raw LLM IO functions."""
    result_ln = leanaide_io(
        input_data=input_data,
        llm_provider=llm_provider,
        model=model
    )
    result_ai = llm_raw_io(
        input_data=input_data,
        llm_provider=llm_provider,
        model=model
    )
    return {
        "time_taken_ln": result_ln.get("time_taken", -1),
        "result_ln": result_ln.get("lean_code", "sorry -- No output generated by LeanAide"),
        "time_taken_ai": result_ai.get("time_taken", -1),
        "result_ai": result_ai.get("lean_code", "sorry -- No output generated by the LLM")
    }

def run_benchmark():
    # Initialize/reset progress
    sts.bm_results = {}
    sts.bm_current_progress = 0
    sts.bm_total_problems = len(sts.get("bm_json_dataset", [])) if sts.bm_input_opt == "JSON Dataset" else 1

    # Create progress bar and status placeholder
    progress_bar = st.progress(0)
    status_text = st.empty()
    results_container = st.empty()  # Dynamic results display

    try:
        print("Initializing benchmark with JSON dataset...")
        log_write("Benchmark", "Initializing benchmark with JSON dataset...")
        if sts.bm_input_opt == "JSON Dataset":
            # Process each problem in the dataset
            for idx in sts.bm_json_dataset:
                problem = sts.bm_json_dataset[idx]
                input_data = {
                    "theorem": problem.get("theorem", ""),
                    "proof": problem.get("proof", "")
                }
                if not input_data["theorem"]:
                    print(f"Skipping problem {idx} due to missing theorem.")
                    continue

                # result_ai should be a lean code
                # Update session state and UI
                print("Running for Problem - LeanAide: ", idx)
                result = _run_both_io(
                    input_data=input_data,
                    llm_provider=sts.get("llm_provider", "openai"),
                    model=sts.get("model_leanaide", "gpt-4o")
                )
                print(f"Result for Problem - AI: {idx}")
                sts.bm_results[idx] = {
                    "problem": input_data["theorem"],
                    "proof": input_data["proof"], 
                    "time_taken_ln": result["time_taken_ln"],
                    "result_ln": result["result_ln"],
                    "time_taken_ai": result["time_taken_ai"],
                    "result_ai": result["result_ai"],
                }
                sts.bm_current_progress = int(idx) + 1
                _update_ui(progress_bar, status_text, results_container)
        else:
            # Single input case
            result = _run_both_io(
                input_data={
                    "theorem": sts.bm_single_thm,
                    "proof": sts.bm_single_proof
                },
                llm_provider=sts.get("llm_provider", "openai"),
                model=sts.get("model_leanaide", "gpt-4o")
            )
            sts.bm_results[idx] = {
                "problem": sts.bm_single_thm,
                "proof": sts.bm_single_proof,
                "time_taken_ln": result["time_taken_ln"],
                "result_ln": result["result_ln"],
                "time_taken_ai": result["time_taken_ai"],
                "result_ai": result["result_ai"],
            }
            _update_ui(progress_bar, status_text, results_container)

        st.success("Benchmark completed successfully!")
    except Exception as e:
        st.error(f"Error during benchmark: {str(e)}")
        print(traceback.format_exc())
    finally:
        return sts.bm_results

def _update_ui(progress_bar, status_text, results_container):
    """Helper to update Streamlit UI components."""
    progress = sts.bm_current_progress / sts.bm_total_problems
    progress_bar.progress(progress)
    status_text.markdown(f"**Progress:** {sts.bm_current_progress}/{sts.bm_total_problems}")

    # Display results incrementally
    with results_container.container():
        for result in sts.bm_results:
            result_data = sts.bm_results[result]
            # Show result_data in a formatted way
            st.markdown(f"### Problem: {result_data['problem']}")
            st.markdown(f"**LeanAide Time (s):** {result_data['time_taken_ln']}")
            st.markdown(f"**LeanAide Lean Code:**\n```lean\n{result_data['result_ln']}\n```")
            st.markdown(f"**LLM Time (s):** {result_data['time_taken_ai']}")
            st.markdown(f"**LLM Lean Code:**\n```lean\n{result_data['result_ai']}\n```")
            st.markdown("---")  # Separator for each

# Button trigger
sts.bm_run_button = st.button("Run Benchmark")
if sts.bm_run_button:
    if sts.bm_json_dataset or (sts.bm_single_thm and sts.bm_single_proof):
        with st.spinner("Running benchmark..."):
            sts.bm_results = run_benchmark()
    else:
        st.warning("Please upload a dataset or enter a single theorem and proof before running the benchmark.")

    