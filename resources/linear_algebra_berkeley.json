{
    "Problem 7.1.2": {
        "Problem": "_Let \\(GL_{2}(\\mathbb{Z}_{m})\\) denote the multiplicative group of invertible \\(2\\times 2\\) matrices over the ring of integers modulo \\(m\\). Find the order of \\(GL_{2}(\\mathbb{Z}_{p^{n}})\\) for each prime \\(p\\) and positive integer \\(n\\)._",
        "Solution": "If \\(p\\) is prime then the order of \\(GL*{2}(\\mathbb{Z}*{p})\\) is the number of ordered bases of a two-dimensional vector space over the field \\(\\mathbb{Z}\\_{p}\\), namely \\((p^{2}-1)(p^{2}-p)\\), as in the solution to Part 2 of Problem 7.1.1 above.\n\nA square matrix \\(A\\) over \\(\\mathbb{Z}_{p^{n}}\\) is invertible when \\(\\det(A)\\) is invertible modulo \\(p^{n}\\), which happens exactly when \\(\\det(A)\\) is not a multiple of \\(p\\). Let \\(\\rho(A)\\) denote the matrix over \\(\\mathbb{Z}_{p}\\) obtained from \\(A\\) by reducing all its entries modulo \\(p\\). We have \\(\\det(\\rho(A))\\equiv\\det(A)\\pmod{p}\\), thus\n\n\\[A\\in GL*{2}\\left(\\mathbb{Z}*{p^{n}}\\right)\\quad\\text{iff}\\quad\\rho(A)\\in GL*{ 2}\\left(\\mathbb{Z}*{p}\\right),\\]\n\ngiving a surjective homomorphism\n\n\\[\\rho:GL*{2}\\left(\\mathbb{Z}*{p^{n}}\\right)\\to GL*{2}\\left(\\mathbb{Z}*{p} \\right).\\]\n\nThe kernel of \\(\\rho\\) is composed of the \\(2\\times 2\\) matrices that reduce to the Identity modulo \\(p\\) so the diagonal entries come from the set \\(\\{1,p+1,2p+1,\\ldots,p^{n}-p+1\\}\\) and the off-diagonal are drawn from the set that reduce to \\(0\\) modulo \\(p\\), that is, \\(\\{0,p,2p,\\ldots,p^{n}-p\\}\\). Both sets have cardinality \\(p^{n-1}\\), so the order of the kernel is \\((p^{n-1})^{4}\\), and order of \\(GL*{2}\\left(\\mathbb{Z}*{p}\\right)\\) is\n\n\\[p^{4n-4}(p^{2}-1)(p^{2}-p)=p^{4n-3}(p-1)(p^{2}-1).\\]"
    },
    "Problem 7.1.3": {
        "Problem": "_Let \\(G\\) be the group of 2\\(\\times\\)2 matrices with determinant \\(1\\) over the four-element field \\(\\mathbf{F}\\). Let \\(S\\) be the set of lines through the origin in \\(\\mathbf{F}^{2}\\). Show that \\(G\\) acts faithfully on \\(S\\). (The action is faithful if the only element of \\(G\\) which fixes every element of \\(S\\) is the identity.)_",
        "Solution": "Let \\(\\mathbf{F}=\\{0,1,a,b\\}\\). The lines through the origin can have slopes \\(0\\), \\(1\\), \\(a\\), \\(b\\), or \\(\\infty\\), so \\(S\\) has cardinality \\(5\\). Let \\(L\\_{s}\\) be the line through the origin with slope \\(y\\). Suppose \\(\\gamma\\in G\\) fixes all these lines, to be specific say\n\n\\[\\gamma=\\left(\\begin{array}{cc}x&y\\\\ z&w\\end{array}\\right).\\]\n\nThen\n\n\\[\\gamma L*{0}=L*{0}\\]\n\nimplies that\n\n\\[\\gamma(1,0)^{t}=(x,z)^{t}=(c,0)^{t}\\]\n\nfor some \\(c\\neq 0\\). Thus, \\(z=0\\). Similarly, the invariance of \\(L*{\\infty}\\) implies \\(y=0\\) and of \\(L*{1}\\) implies \\(x=w\\). Then \\(\\det(\\gamma)=x^{2}=1\\) and since \\(\\mathbf{F}\\) has characteristic \\(2\\), we must have \\(x=1\\) and \\(\\gamma\\) is the identity."
    },
    "Problem 7.1.4": {
        "Problem": "_Prove the following statements about the polynomial ring \\(\\mathbf{F}[x]\\), where \\(\\mathbf{F}\\) is any field._\n\n1. \\(\\mathbf{F}[x]\\) _is a vector space over_ \\(\\mathbf{F}\\)2. _The subset_ \\(\\mathbf{F}_{n}[x]\\) \\_of polynomials of degree_ \\(\\leq n\\) _is a subspace of dimension_ \\(n+1\\) _in_ \\(\\mathbf{F}[x]\\)_._\n2. _The polynomials_ \\(1,x-a,\\ldots,(x-a)^{n}\\) _form a basis of_ \\(\\mathbf{F}_{n}[x]\\) \\_for any_ \\(a\\in\\mathbf{F}\\)_._",
        "Solution": "1. \\(\\mathbf{F}[x]\\) is a ring under polynomial addition and multiplication because \\(\\mathbf{F}\\) is a ring. The other three axioms of vector addition - associativity, uniqueness of the zero, and inverse - are trivial to verify; as for scalar multiplication, there is a unit (same as in \\(\\mathbf{F}\\)) and all four axioms are trivial to verify, making it a vector field.\n\n2. To see this, observe that the set \\(\\{1,x,x^{2},\\ldots,x^{n}\\}\\) form a basis for this space, because any linear combination will be zero, if and only if, all coefficients are zero, by looking at the degree on both sides.\n\n3. An argument as above shows that\n\n\\[a*{0}1+a*{1}(x-a)+\\cdots+a\\_{n}(x-a)^{n}=0\\]only if the coefficients are all zero."
    },
    "Problem 7.1.5": {
        "Problem": "_Suppose \\(V\\) is an \\(n\\)-dimensional vector space over the field \\(\\mathbf{F}\\). Let \\(W\\subset V\\) be a subspace of dimension \\(r<n\\). Show that_\n\n\\[W=\\bigcap\\left\\{U\\mid U\\,\\mbox{is}\\,an\\,(n-1)-dimensional\\mbox{ subspace of }V\\,\\mbox{and }W\\subset U\\right\\}.\\]",
        "Solution": "Let \\(Y\\) denote the given intersection. Then \\(Y\\) is a subspace of \\(V\\) and, clearly, \\(W\\subset Y\\). Suppose that there exists a nonzero vector \\(v\\in Y\\setminus W\\). Since \\(v\\) is not in \\(W\\), a set consisting of \\(v\\) and a basis for \\(W\\) is linearly independent. Extend this to a basis of \\(V\\), and let \\(Z\\) be the \\(n-1\\)-dimensional subspace obtained by omitting \\(v\\) from this basis. Then \\(W\\subset Z\\), so \\(Z\\) is a term in the intersection used to define \\(Y\\). However, \\(v\\) is not in \\(Z\\), so \\(v\\) cannot be an element of Y, a contradiction. Hence, \\(Y\\subset W\\) and we are done."
    },
    "Problem 7.1.7": {
        "Problem": "_Let \\(A\\) be a complex \\(n\\times n\\) matrix, and let \\(C(A)\\) be the commutant of \\(A\\); that is, the set of complex \\(n\\times n\\) matrices \\(B\\) such that \\(AB=BA\\). (It is obviously a subspace of \\(M_{n\\times n}\\), the vector space of all complex \\(n\\times n\\) matrices.) Prove that \\(\\dim C(A)\\geq n\\).\\_",
        "Solution": "Note first that if \\(A\\) and \\(B\\) are matrices and \\(C\\) is an invertible matrix, then\n\n\\[AB=BA\\quad\\text{iff}\\quad C^{-1}ACC^{-1}BC=C^{-1}BCC^{-1}AC.\\]\n\nAlso, if \\(D*{1},\\ldots,D*{n}\\) are linearly independent matrices, so are the matrices \\(C^{-1}D*{1}C,\\ldots,C^{-1}D*{n}C\\). We may then assume that \\(A\\) is in Jordan Canonical Form [11, pag. 247].\n\nA direct calculation shows that if \\(\\tilde{A}=\\left(\\begin{array}{cccc}a&1&\\ldots&0\\\\ &\\ddots&\\ddots&\\\\ &&&1\\\\ 0&&&a\\end{array}\\right)\\) is a \\(k\\times k\\)\n\nJordan block, then \\(\\tilde{A}\\) commutes with \\(\\tilde{B}=\\left(\\begin{array}{cccc}b*{1}&b*{2}&&b*{k}\\\\ &\\ddots&\\ddots&b*{2}\\\\ 0&&&b\\_{1}\\end{array}\\right)\\).\n\nTherefore, by block multiplication, \\(A\\) commutes with any matrix of the form\n\n\\[B=\\left(\\begin{array}{cccc}\\tilde{B}_{1}&&&\\\\ &\\ddots&\\\\ &&&\\tilde{B}_{r}\\end{array}\\right)\\]\n\nwhere the \\(\\tilde{B}\\_{r}\\)'s have the form of \\(\\tilde{B}\\) and the same dimension as the Jordan blocks of \\(A\\). Since there are \\(n\\) variables in \\(B\\), \\(\\dim C(A)\\geq n\\)."
    },
    "Problem 7.1.9": {
        "Problem": "_Let \\(A\\) and \\(B\\) be subspaces of a finite-dimensional vector space \\(V\\) such that \\(A+B=V\\). Write \\(n=\\dim V\\), \\(a=\\dim A\\), and \\(b=\\dim B\\). Let \\(S\\) be the set of those endomorphisms \\(f\\) of \\(V\\) for which \\(f(A)\\subset\\ A\\) and \\(f(B)\\subset B\\). Prove that \\(S\\) is a subspace of the set of all endomorphisms of \\(V\\), and express the dimension of \\(S\\) in terms of \\(n\\), \\(a\\), and \\(b\\)._",
        "Solution": "Let \\(f,g\\in S\\) and let \\(r\\) and \\(s\\) be scalars. Then, for any \\(v\\in A\\), \\((rf+sg)(v)=f(rv)+g(sv)\\in A\\), since \\(A\\) is a vector subspace and \\(f\\) and \\(g\\) fix \\(A\\). Similarly \\(rf+sg\\) fixes \\(B\\), so \\(rf+sg\\in S\\) and \\(S\\) is a vector space.\n\nTo determine the dimension of \\(S\\), it suffices to determine the dimension of the space of matrices which fix \\(A\\) and \\(B\\). To choose a basis for \\(V\\), let \\(A^{\\prime}\\) denote a complementary subspace of \\(A\\cap B\\) in \\(A\\) and let \\(B^{\\prime}\\) denote a complementary subspace of \\(A\\cap B\\) in \\(B\\). Then, since \\(A+B=V\\), \\(r=a+b-n\\) is the dimension of \\(A\\cap B\\). Further, \\(\\dim A^{\\prime}=a-r\\) and \\(\\dim B^{\\prime}=b-r\\). Take one basis in each of the spaces \\(A^{\\prime}\\), \\(B^{\\prime}\\), and \\(A\\cap B\\). The union of these bases form a basis for \\(V\\). Since any endomorphism which leaves \\(A\\) and \\(B\\) invariant must also fix \\(A\\cap B\\), its matrix in this basis must have the form\n\n\\[\\left(\\begin{array}{ccc}_&_&_\\\\ 0&_&0\\\\ 0&0&\\*\\end{array}\\right)\\]\n\nwhich has, at most, \\(a^{2}+b^{2}+n^{2}-an-bn\\) nonzero entries, so the dimension of \\(S\\) is \\(a^{2}+b^{2}+n^{2}-an-bn\\)."
    },
    "Problem 7.1.10": {
        "Problem": "_Let \\(T\\) be a linear transformation of a vector space \\(V\\) into itself. Suppose \\(x\\in V\\) is such that \\(T^{m}x=0\\), \\(T^{m-1}x\\neq 0\\) for some positive integer \\(m\\). Show that \\(x,\\,Tx,\\ldots,T^{m-1}x\\) are linearly independent._",
        "Solution": "Suppose there are scalars such that\n\n\\[a*{0}x+a*{1}Tx+\\cdots+a*{k}T^{k}x+\\cdots+a*{m-1}T^{m-1}x=0\\]\n\napplying \\(T^{m-1}\\) to both sides, we get, since \\(T0=0\\),\n\n\\[a*{0}T^{m-1}x+a*{1}T^{m}x+\\cdots+a*{k}T^{m-1+k}x+\\cdots+a*{m-1}T^{m-1+m-1}x=0\\]\n\nso\n\n\\[a\\_{0}T^{m-1}x=0\\]\n\nand \\(a*{0}=0\\). By the Induction Principle [11, pag. 7] (multiplying by \\(T^{m-k-1}\\)) we see that all \\(a*{k}=0\\) and the set is linearly independent."
    },
    "Problem 7.1.11": {
        "Problem": "_Let \\(\\alpha_{1},\\alpha*{2},\\ldots,\\alpha*{n}\\) be distinct real numbers. Show that the \\(n\\) exponential functions \\(e^{\\alpha*{1}t},e^{\\alpha*{2}t},\\ldots,e^{\\alpha*{n}t}\\) are linearly independent over the real numbers.*",
        "Solution": null
    },
    "Problem 7.1.12": {
        "Problem": "_Let \\(V\\) be a real vector space of dimension \\(n\\) with a positive definite inner product. We say that two bases \\((a_{i})\\) and \\((b*{i})\\) have the same orientation if the matrix of the change of basis from \\((a*{i})\\) to \\((b*{i})\\) has a positive determinant. Suppose now that \\((a*{i})\\) and \\((b*{i})\\) are orthonormal bases with the same orientation. Show that \\((a*{i}+2b*{i})\\) is again a basis of \\(V\\) with the same orientation as \\((a*{i})\\).\\_\n\n### Rank and Determinants",
        "Solution": "Let \\(P\\) be the change of basis matrix from \\((a*{i})\\) to \\((b*{i})\\). A straightforward calculation shows that \\(I+2P\\) is the matrix taking \\((a*{i})\\) to \\((a*{i}+2b*{i})\\). Now \\((I+2P)v=\\lambda v\\) implies that \\(\\mathit{Pv}=\\frac{1}{2}(\\lambda-1)v\\). So if \\(\\lambda\\) is an eigenvalue of \\(I+2P\\), then \\(\\frac{1}{2}(\\lambda-1)\\) is an eigenvalue of \\(P\\), and they correspond to the same eigenvectors. The reverse also holds, so there is a one-to-one correspondence between the eigenvalues of \\(P\\) and those of \\(I+2P\\). As \\((a*{i})\\) and \\((b*{i})\\) are orthonormal bases, \\(P\\) is orthogonal and therefore, all the eigenvalues of \\(P\\) are \\(\\pm 1\\). But this implies that the only possible eigenvalues of \\(I+2P\\) are \\(3\\) and \\(-1\\). Hence, \\(0\\) is not an eigenvalue of \\(I+2P\\), so it is an invertible matrix and, thus, \\((a*{i}+2b*{i})\\) is a basis. Further, \\(\\det P=(-1)^{\\alpha}1^{\\beta}\\), where \\(\\alpha\\) and \\(\\beta\\) are the algebraic multiplicities of \\(-1\\) and \\(1\\) as eigenvalues of \\(P\\). Thus, \\(\\det(I+2P)=(-1)^{\\alpha}3^{\\beta}\\). Since we are given that \\(\\det P>0\\), \\(\\alpha\\) is even and, thus, \\(\\det(I+2P)\\) is positive as well. Therefore, \\((a*{i}+2b*{i})\\) has the same orientation as \\((a*{i})\\).\n\n### 7.2 Rank and Determinants"
    },
    "Problem 7.2.3": {
        "Problem": "_Suppose that \\(P\\) and \\(Q\\) are \\(n\\times n\\) matrices such that \\(P^{2}=P\\), \\(Q^{2}=Q\\), and \\(1-(P+Q)\\) is invertible. Show that \\(P\\) and \\(Q\\) have the same rank._",
        "Solution": "Since \\(1-P-Q\\) is invertible, \\(P\\) has the same rank as\n\n\\[P(1-P-Q)=P-P^{2}-PQ=-PQ.\\]\n\nSimilarly, \\(Q\\) has the same rank as\n\n\\[(1-P-Q)Q=Q-PQ-Q^{2}=-PQ,\\]\n\nso \\(P\\) and \\(Q\\) have the same rank."
    },
    "Problem 7.2.4": {
        "Problem": "_Let \\(T\\) be a real, symmetric, \\(n\\times n\\), tridiagonal matrix:_\n\n\\[T=\\left(\\begin{array}{cccccc}a*{1}&b*{1}&0&0&\\cdots&0&0\\\\ b*{1}&a*{2}&b*{2}&0&\\cdots&0&0\\\\ 0&b*{2}&a*{3}&b*{3}&\\cdots&0&0\\\\ \\vdots&\\vdots&\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\ 0&0&0&0&\\cdots&a*{n-1}&b*{n-1}\\\\ 0&0&0&0&\\cdots&b*{n-1}&a*{n}\\end{array}\\right)\\]\n\n_(All entries not on the main diagonal or the diagonals just above and below the main one are zero.) Assume \\(b_{j}\\neq 0\\) for all \\(j\\).\\_\n\n_Prove:_\n\n1. \\(\\operatorname{rank}T\\geq n-1\\)_._\n2. \\(T\\) _has_ \\(n\\) _distinct eigenvalues._",
        "Solution": "1. _and_ 2. Since \\(T\\) is symmetric it is diagonalizable, so \\(\\mathbb{R}^{n}\\) can be written as the direct sum of the eigenspaces of \\(T\\). It suffices to show that any eigenspace has dimension, at most, 1. For if this is the case, then the kernel has dimension, at most, 1, and, by the Rank-'null'ity Theorem [13, pag. 71], \\(T\\) has rank at least \\(n-1\\), and there must be \\(n\\) distinct eigenspaces, so there are \\(n\\) distinct eigenvalues associated with them.\n\nLet \\(\\lambda\\in\\mathbb{R}\\), and consider the system of equations \\(Tx=\\lambda x\\). The first equation is \\(a*{1}x*{1}+b*{1}x*{2}=\\lambda x*{1}\\). Since \\(b*{1}\\neq 0\\), we can solve for \\(x*{2}\\) in terms of \\(x*{1}\\). Suppose that we can solve the first \\(i-1\\) equations for \\(x*{2},\\ldots,x*{i}\\) in terms of \\(x*{1}\\). Then, since \\(b*{i}\\neq 0\\), we can solve the \\(i^{th}\\) equation \\(b*{i-1}x*{i-1}+a*{i}x*{i}+b*{i}x*{i+1}=\\lambda x*{i}\\) for \\(x*{i+1}\\) in terms of \\(x*{1}\\). Therefore, by the Induction Principle [12, pag. 7], we can solve the first \\(n-1\\) equations for \\(x*{2},\\ldots,x*{n}\\) in terms of \\(x*{1}\\).\n\nThe last equation, \\(b*{n-1}x*{n-1}+a*{n}x*{n}=\\lambda x\\_{n}\\), is either consistent with this or is not. If not, \\(\\lambda\\) is not an eigenvalue; if it is, then \\(\\lambda\\) is an eigenvalue and we have one degree of freedom in determining eigenvectors. Hence, in either case the associated eigenspace has dimension, at most, 1 and we are done.\n\n_Solution 2._ 1. The submatrix one obtains by deleting the first row and the first column is upper triangular with nonzero diagonal entries, so its determinant is nonzero. Thus, the first \\(n-1\\) columns of \\(T\\) are linearly independent.\n\n2. By the Spectral Theorem [13, pag. 335], [14, pag. 235], \\(\\mathbb{R}^{n}\\) has a basis consisting of eigenvectors of \\(T\\). If \\(\\lambda\\) is an eigenvalue of \\(T\\), then \\(T-\\lambda I\\) has rank \\(n-1\\) by Part 1, so \\(\\ker(T-\\lambda I)\\) has dimension 1. Since the eigenspaces span \\(\\mathbb{R}^{n}\\) and each has dimension 1, there must be \\(n\\) of them."
    },
    "Problem 7.2.5": {
        "Problem": "_Let \\(A=(a_{ij})\\) be an n\\(\\times\\)n real matrix satisfying the conditions:\\_\n\n\\[a*{ii}>0\\quad(1\\leq i\\leq n),\\]\\[a*{ij}\\leq 0\\quad(i\\neq j,\\,1\\leq i,j\\leq n),\\]\n\n\\[\\sum*{i=1}^{n}a*{ij}>0\\quad(1\\leq j\\leq n).\\]\n\n_Show that \\(\\det(A)>0\\)._",
        "Solution": null
    },
    "Problem 7.2.6": {
        "Problem": "_Let \\(A=(a_{ij})_{i,j=1}^{r}\\) be a square matrix with integer entries._\n\n1. _Prove that if an integer_ \\(n\\) _is an eigenvalue of_ \\(A\\)_, then_ \\(n\\) _is a divisor of_ \\(\\det A\\)_, the determinant of_ \\(A\\)_._\n2. _Suppose that_ \\(n\\) _is an integer and that each row of_ \\(A\\) _has sum_ \\(n\\)_:_ \\[\\sum*{j=1}^{r}a*{ij}=n,\\qquad 1\\leq i\\leq r.\\] _Prove that_ \\(n\\) _is a divisor of_ \\(\\det A\\)_._",
        "Solution": "1. Write the characteristic polynomial of \\(A\\), \\(\\det(A-\\lambda I)\\), as \\((-1)^{r}\\lambda^{r}+c*{1}\\lambda^{r-1}+\\cdots+c*{r}\\). Since the entries of \\(A\\) are integers, each \\(c*{k}\\) is an integer, and \\(c*{r}=\\det A\\). If \\(\\lambda\\) is an integer eigenvalue, then \\(\\det(A-nI)=0\\), so\n\n\\[\\det A=(-1)^{r-1}n^{r}+c*{1}n^{r-1}+\\cdots+c*{r-1}n\\]showing that \\(n\\) divides \\(\\det A\\).\n\n2. Under the given hypotheses, \\(n\\) is an eigenvalue with eigenvector \\((1,1,\\ldots,1)^{t}\\), so Part 1 applies."
    },
    "Problem 7.2.7": {
        "Problem": "_Let \\(\\mathbb{R}[x_{1},\\ldots,x*{n}]\\) be the polynomial ring over the real field \\(\\mathbb{R}\\) in the \\(n\\) variables \\(x*{1},\\ldots,x*{n}\\). Let the matrix \\(A\\) be the \\(n\\times n\\) matrix whose \\(i^{th}\\) row is \\((1,x*{i},x*{i}^{2},\\ldots,x*{i}^{n-1})\\), \\(i=1,\\ldots,n\\). Show that\\_\n\n\\[\\det A=\\prod*{i>j}(x*{i}-x\\_{j}).\\]",
        "Solution": "We use the Induction Principle [11, pag. 32] in the order of the matrix. If \\(n=2\\),\n\n\\[A=\\left(\\begin{array}{cc}1&x*{1}\\\\ 1&x*{2}\\end{array}\\right)\\]\n\nwhich has determinant \\((x*{2}-x*{1})\\).\n\nSuppose the result holds for all \\(k<n\\), and let \\(A\\) be the \\(n\\times n\\) Vandermonde matrix [12, pag. 125]. Treating the indeterminates \\(x*{1},\\ldots,x*{n-1}\\) as _constants_ and expanding the determinant of \\(A\\) along the last row, we see that \\(\\det A\\) is an \\((n-1)^{th}\\) degree polynomial in \\(x*{n}\\), which can have, at most, \\(n-1\\) roots. If we let \\(x*{n}=x*{i}\\) for \\(1\\leq i\\leq n-1\\), \\(A\\) would have two identical rows, so \\(\\det A\\) would equal \\(0\\). Hence, the \\(x*{i}\\)'s are the roots of \\(\\det A\\) as a polynomial in \\(x\\_{n}\\). In other words, there exists a constant \\(c>0\\) such that\n\n\\[\\det A=c\\prod*{i=1}^{n-1}(x*{n}-x\\_{i}).\\]\n\n\\(c\\) is the coefficient of the \\(x\\_{n}^{n-1}\\) term, which, when we expand the determinant, is equal to the determinant of the \\((n-1)\\times(n-1)\\) Vandermonde matrix. So, by the induction hypothesis,\n\n\\[\\det A=\\prod*{j<i\\leq n-1}(x*{i}-x*{j})\\prod*{i=1}^{n-1}(x*{n}-x*{i})=\\prod*{i >j}(x*{i}-x\\_{j}).\\]"
    },
    "Problem 7.2.8": {
        "Problem": "_A matrix of the form_\n\n\\[\\left(\\begin{array}{cccc}1&a*{0}&a*{0}^{2}&\\ldots&a*{0}^{n}\\\\ 1&a*{1}&a*{1}^{2}&\\ldots&a*{1}^{n}\\\\ \\vdots&\\vdots&\\vdots&\\ddots&\\vdots\\\\ 1&a*{n}&a*{n}^{2}&\\ldots&a\\_{n}^{n}\\end{array}\\right)\\]\n\n_where the \\(a_{i}\\) are complex numbers, is called a Vandermonde matrix.\\_\n\n1. _Prove that the Vandermonde matrix is invertible if_ \\(a*{0},a*{1},\\ldots,a*{n}\\) \\_are all different.*\n2. _If_ \\(a*{0},a*{1},\\ldots,a*{n}\\) \\_are all different, and* \\(b*{0},b*{1},\\ldots,b*{n}\\) \\_are complex numbers, prove that there is a unique polynomial* \\(f\\) _of degree_ \\(n\\) _with complex coefficients such that_ \\(f(a*{0})=b*{0}\\)_,_ \\(f(a*{1})=b*{1}\\)_,...,_ \\(f(a*{n})=b*{n}\\)_._",
        "Solution": "1. As shown in the solution of Problem 7.2.7, the determinant of the matrix is\n\n\\[\\prod*{i>j}(a*{i}-a\\_{j})\\]\n\nwhich is nonzero if the \\(a\\_{i}\\) are all different.\n\n2. The function \\(f\\) given by\n\n\\[f(x)=\\sum*{i=0}^{n}\\frac{(x-a*{0})\\cdots(x-a*{i-1})b*{i}(x-a*{i+1})\\cdots(x-a *{n})}{(a*{i}-a*{0})\\cdots(a*{i}-a*{i-1})b*{i}(a*{i}-a*{i+1})\\cdots(a*{i}-a\\_{n })}\\]\n\nhas degree \\(n\\) and takes \\(f(a*{i})\\) into \\(b*{i}\\). Now, if \\(\\psi(x)\\) is another such polynomial of degree \\(n\\), the polynomial\n\n\\[f(x)-\\psi(x)\\]has degree \\(n\\) with \\(n+1\\) different roots (the \\(a\\_{i}\\)'s), so it has to be the zero polynomial and \\(f\\) is unique."
    },
    "Problem 7.2.9": {
        "Problem": "_Give an example of a continuous function \\(v:\\mathbb{R}\\to\\mathbb{R}^{3}\\) with the property that \\(v(t_{1})\\), \\(v(t*{2})\\), and \\(v(t*{3})\\) form a basis for \\(\\mathbb{R}^{3}\\) whenever \\(t*{1}\\), \\(t*{2}\\), and \\(t*{3}\\) are distinct points of \\(\\mathbb{R}\\).*",
        "Solution": "Consider the function \\(v(t)=(1,t,t^{2})\\). To show that \\(v(t*{1})\\), \\(v(t*{2})\\), and \\(v(t*{3})\\) form a basis for \\(\\mathbb{R}^{3}\\) whenever the \\(t*{i}\\)'s are distinct, it will suffice to show that the matrix which has these vectors as rows has nonzero determinant. But this matrix is\n\n\\[\\left(\\begin{array}{ccc}1&t*{1}&t*{1}^{2}\\\\ 1&t*{2}&t*{2}^{2}\\\\ 1&t*{3}&t*{3}^{2}\\end{array}\\right)\\]\n\nwhich is the \\(3\\times 3\\) Vandermonde matrix [10, pag. 125]. Its determinant is given by\n\n\\[(t*{3}-t*{2})(t*{3}-t*{1})(t*{2}-t*{1})\\]\n\nwhich is nonzero whenever the \\(t\\_{i}\\)'s are distinct."
    },
    "Problem 7.2.10": {
        "Problem": "_Let \\(f_{1},f*{2},\\ldots,f*{n}\\) be continuous real valued functions on \\([a,b]\\). Show that the set \\(\\{f*{1},\\ldots,f*{n}\\}\\) is linearly dependent on \\([a,b]\\) if and only if\\_\n\n\\[\\det\\left(\\int*{a}^{b}f*{i}(x)f\\_{j}(x)dx\\right)=0\\;.\\]",
        "Solution": "Let \\(G\\) be the matrix with entries\n\n\\[G*{ij}=\\int*{a}^{b}f*{i}(x)f*{j}(x)dx.\\]\n\nIf the determinant of \\(G\\) vanishes, then \\(G\\) is singular; let \\(a\\) be a nonzero \\(n\\)-vector with \\(Ga=0\\). Then\n\n\\[0=a^{T}Ga=\\sum*{i=1}^{n}\\sum*{i=j}^{n}\\int*{a}^{b}a*{i}f*{i}(x)a*{j}f*{j}(x)dx =\\int*{a}^{b}\\left(\\sum*{i=1}^{n}a*{i}f\\_{i}(x)\\right)^{2}dx\\,\\]\n\nso, since the \\(f*{i}\\)'s are continuous functions, the linear combination \\(\\sum a*{i}f*{i}\\) must vanish identically. Hence, the set \\(\\{f*{i}\\}\\) is linearly dependent on \\([a,b]\\). Conversely, if \\(\\{f*{i}\\}\\) is linearly dependent, some \\(f*{i}\\) can be expressed as a linear combination of the rest, so some row of \\(G\\) is a linear combination of the rest and \\(G\\) is singular."
    },
    "Problem 7.2.11": {
        "Problem": "_Let \\(M_{2\\times 2}\\) be the vector space of all real 2\\(\\times\\)2 matrices. Let\\_\n\n\\[A=\\left(\\begin{array}{cc}1&2\\\\ -1&3\\end{array}\\right)\\qquad\\qquad B=\\left(\\begin{array}{cc}2&1\\\\ 0&4\\end{array}\\right)\\]\n\n_and define a linear transformation \\(L:M_{2\\times 2}\\to M*{2\\times 2}\\) by L(X) = AXB. Compute the trace and the determinant of \\(L\\).*",
        "Solution": "Identify \\(M\\_{2\\times 2}\\) with \\(\\mathbb{R}^{4}\\) via\n\n\\[\\left(\\begin{array}{cc}a&b\\\\ c&d\\end{array}\\right)\\leftrightarrow\\left(\\begin{array}{c}a\\\\ b\\\\ c\\\\ d\\end{array}\\right)\\]\n\nand decompose \\(L\\) into the multiplication of two linear transformations,\n\n\\[M*{2\\times 2}\\simeq\\mathbb{R}^{4}\\stackrel{{ L*{A}}}{{\\longrightarrow}} \\mathbb{R}^{4}\\stackrel{{ L_{B}}}{{\\longrightarrow}}\\mathbb{R}^{4} \\simeq M\\_{2\\times 2}\\]\n\nwhere \\(L*{A}(X)=AX\\) and \\(L*{B}(X)=XB\\).\n\nThe matrices of these two linear transformations on the canonical basis of \\(\\mathbb{R}^{4}\\) is\n\n\\[L*{A}=\\left(\\begin{array}{rrrr}1&0&2&0\\\\ 0&1&0&2\\\\ -1&0&3&0\\\\ 0&-1&0&3\\end{array}\\right)\\quad\\text{and}\\quad L*{B}=\\left(\\begin{array}{rrrr} 2&0&0&0\\\\ 1&4&0&0\\\\ 0&0&2&0\\\\ 0&0&1&4\\end{array}\\right)\\]\n\nthen \\(\\det L=\\det L*{A}\\cdot\\det L*{B}=(9+6+2(2+3))\\cdot(2\\cdot 32)=2^{6}\\cdot 5^{2}\\), and to compute the trace of \\(L\\), we only need the diagonal elements of \\(L*{A}\\cdot L*{B}\\), that is,\n\n\\[\\operatorname{tr}L=2+4+6+12=24.\\]"
    },
    "Problem 7.2.12": {
        "Problem": "_Let \\(V\\) be the vector space of all real 3\\(\\times\\)3 matrices and let \\(A\\) be the diagonal matrix_\n\n\\[\\left(\\begin{array}{ccc}1&0&0\\\\ 0&2&0\\\\ 0&0&1\\end{array}\\right).\\]\n\n_Calculate the determinant of the linear transformation \\(T\\) on \\(V\\) defined by \\(T(X)=\\frac{1}{2}(AX+XA)\\)._",
        "Solution": "Let \\(X=(x*{ij})\\) be any element of \\(M*{3}(\\mathbb{R})\\). A calculation gives\n\n\\[T(X)=\\left(\\begin{array}{rrrr}x*{11}&3x*{12}/2&x*{13}\\\\ 3x*{21}/2&2x*{22}&3x*{23}/2\\\\ x*{31}&3x*{32}/2&x\\_{33}\\end{array}\\right).\\]\n\nIt follows that the basis matrices \\(M\\_{ij}\\) are eigenvectors of \\(T\\). Taking the product of their associated eigenvalues, we get \\(\\det T=2(3/2)^{4}=81/8\\)."
    },
    "Problem 7.2.13": {
        "Problem": "_Let \\(M_{3\\times 3}\\) denote the vector space of real 3\\(\\times\\)3 matrices. For any matrix \\(A\\in M*{3\\times 3}\\), define the linear operator \\(L*{A}:M*{3\\times 3}\\to M*{3\\times 3}\\), \\(L*{A}(B)=AB\\). Suppose that the determinant of \\(A\\) is \\(32\\) and the minimal polynomial is \\((t-4)(t-2)\\). What is the trace of \\(L*{A}\\)?\\_",
        "Solution": "Since the minimal polynomial of \\(A\\) splits into distinct linear factors, \\(\\mathbb{R}^{3}\\) has a basis \\(\\{v*{1},v*{2},v*{3}\\}\\) of eigenvectors of \\(A\\). Since \\(\\det A=32\\), two of those, say \\(v*{1}\\) and \\(v*{2}\\), are associated with the eigenvalue \\(4\\), and one, \\(v*{3}\\), is associated with the eigenvalue \\(2\\). Now consider the nine matrices \\(E*{ij}\\), \\(1\\leq i,j\\leq 3\\), whose \\(i^{th}\\) column is the vector \\(v*{j}\\) and whose other columns are zero. Since the \\(v*{i}\\)'s are linearly independent, the matrices \\(E*{ij}\\) are linearly independent in \\(M*{3\\times 3}\\) and form a basis of \\(M*{3\\times 3}\\). Further, a calculation shows that \\(AE*{ij}=\\lambda*{j}E*{ij}\\), where \\(\\lambda*{1}=\\lambda*{2}=4\\) and \\(\\lambda*{3}=2\\). Hence, \\(M*{3\\times 3}\\) has a basis of eigenvectors of \\(L*{A}\\), so it follows that \\(\\operatorname{tr}L\\_{A}=6\\cdot 4+3\\cdot 2=30\\)."
    },
    "Problem 7.2.14": {
        "Problem": "_Let \\(S\\) denote the vector space of real \\(n\\times n\\) skew-symmetric matrices. For a nonsingular matrix \\(A\\), compute the determinant of the linear map \\(T_{A}:S\\to S\\), \\(T*{A}(X)=AXA^{-1}\\). Hint: First consider the special cases where (i) \\(A\\) is orthogonal and (ii) \\(A\\) is symmetric.*",
        "Solution": null
    },
    "Problem 7.2.15": {
        "Problem": "_Let \\(M_{7\\times 7}\\) denote the vector space of real 7\\(\\times\\)7 matrices. Let \\(A\\) be a diagonal matrix in \\(M*{7\\times 7}\\) that has \\(+1\\) in four diagonal positions and \\(-1\\) in three diagonal positions. Define the linear transformation \\(T\\) on \\(M*{7\\times 7}\\) by \\(T(X)=AX-XA\\). What is the dimension of the range of \\(T\\)?\\_",
        "Solution": "We have\n\n\\[\\dim\\operatorname{range}T=\\dim M\\_{7\\times 7}-\\dim\\ker T=49-\\dim\\ker T\\]\n\nso it suffices to find the dimension of \\(\\ker T\\); in other words, the dimension of the subspace of matrices that commute with \\(A\\). Let \\(E*{+}\\) be the eigenspace of \\(A\\) for the eigenvalue \\(1\\) and \\(E*{-}\\) be the eigenspace of \\(A\\) for the eigenvalue \\(-1\\). Then \\(\\mathbb{R}^{7}=E*{+}\\oplus E*{-}\\). A matrix that commutes with \\(A\\) leaves \\(E*{+}\\) and \\(E*{-}\\) invariant, so, as linear transformations on \\(\\mathbb{R}^{7}\\), can be expressed as the direct sum of a linear transformation on \\(E*{+}\\) with a linear transformation on \\(E*{-}\\). Moreover, any matrix that can be so expressed commutes with \\(A\\). Hence, the space of matrices that commute with \\(A\\) is isomorphic to \\(M*{4\\times 4}\\oplus M*{3\\times 3}\\)and so has dimension \\(16+9=25\\). It follows that \\(\\dim\\operatorname{range}T=49-25=24\\)."
    },
    "Problem 7.2.16": {
        "Problem": "_Let \\(\\mathbf{F}\\) be a field. For m and n positive integers, let \\(M_{m\\times n}\\) be the vector space of \\(m\\times n\\) matrices over \\(\\mathbf{F}\\). Fix \\(m\\) and \\(n\\), and fix matrices \\(A\\) and \\(B\\) in \\(M*{m\\times n}\\). Define the linear transformation \\(T\\) from \\(M*{n\\times m}\\) to \\(M*{m\\times n}\\) by*\n\n\\[T(X)=AXB.\\]\n\n_Prove that if \\(m\\neq n\\), then \\(T\\) is not invertible._\n\n### Systems of Equations",
        "Solution": "\\(m>n\\). We write \\(T=T*{1}T*{2}\\), where \\(T*{2}:M*{n\\times m}\\to M*{m\\times m}\\) is defined by \\(T*{2}(X)=BX\\) and \\(T*{1}:M*{n\\times n}\\to M*{m\\times n}\\) is defined by \\(T*{1}(Y)=AY\\). Since \\(\\dim M*{n\\times m}=nm>n^{2}=\\dim M*{n\\times n}\\), the transformation \\(T\\_{2}\\) has a nontrivial kernel, by the Rank-'null'ity Theorem [11, pag. 71]. Hence, \\(T\\) also has a nontrivial kernel and is not invertible.\n\nWe write \\(T=T*{2}T*{1}\\), where \\(T*{1}:M*{n\\times m}\\to M*{m\\times m}\\) is defined by \\(T*{1}(X)=AX\\) and \\(T*{2}:M*{m\\times m}\\to M*{m\\times n}\\) is defined by \\(T*{2}(Y)=BY\\). Now we have \\(\\dim M*{n\\times m}=nm>m^{2}=\\dim M*{m\\times m}\\), so \\(T\\_{1}\\) has a nontrivial kernel, and we conclude as before that \\(T\\) is not invertible.\n\n### 7.3 Systems of Equations"
    },
    "Problem 7.3.1": {
        "Problem": "_Determine all solutions to the following infinite system of linear equations in the infinitely many unknowns \\(x_{1},x*{2},\\ldots\\):*\n\n\\[\\begin{array}{ccccccc}x*{1}&+&x*{3}&+&x*{5}&=&0\\\\ x*{2}&+&x*{4}&+&x*{6}&=&0\\\\ x*{3}&+&x*{5}&+&x\\_{7}&=&0\\\\ \\vdots&&\\vdots&&\\vdots&&\\vdots\\end{array}\\]\n\n_How many free parameters are required?_",
        "Solution": null
    },
    "Problem 7.4.1": {
        "Problem": "_Let \\(E\\) and \\(F\\) be vector spaces (not assumed to be finite-dimensional). Let \\(S:E\\to F\\) be a linear transformation._\n\n1. _Prove_ \\(S(E)\\) _is a vector space._\n2. _Show_ \\(S\\) _has a kernel_ \\(\\{0\\}\\) _if and only if_ \\(S\\) _is injective (i.e., one-to-one)._\n3. _Assume_ \\(S\\) _is injective; prove_ \\(S^{-1}:S(E)\\to E\\) _is linear._",
        "Solution": "1. We need to show that vector addition and scalar multiplication are closed in \\(S(E)\\), but this is a trivial verification because if \\(v=S(x)\\) and \\(w=S(y)\\) are vectors in \\(S(E)\\), then\n\n\\[v+w=S(x+y)\\ \\ \\ \\text{and}\\ \\ \\ cv=S(cx)\\]\n\nare also in \\(S(E)\\).\n\n2. If \\(S\\) is not injective, then two different vectors \\(x\\) and \\(y\\) have the same image \\(S(x)=S(y)=v\\), so\n\n\\[S(x-y)=S(x)-S(y)=v-v=0\\]\n\nthat is, \\(x-y\\neq 0\\) is a vector in the kernel of \\(S\\). On the other hand, if \\(S\\) is injective, it only takes \\(0\\in E\\) into \\(0\\in F\\), showing the result.\n\n3. Assuming that \\(S\\) is injective, the application \\(S^{-1}:S(E)\\to E\\) is well defined. Given \\(av+bw\\in S(E)\\) with \\(v=S(x)\\) and \\(w=S(y)\\), we have\n\n\\[S^{-1}(av+w) =S^{-1}(aS(x)+bS(y))\\] \\[=S^{-1}(S(ax+by))\\] \\[=ax+by\\] \\[=aS^{-1}(v)+bS^{-1}(w)\\]\n\ntherefore, \\(S^{-1}\\) is linear."
    },
    "Problem 7.4.2": {
        "Problem": "_Let \\(T:V\\to W\\) be a linear transformation between finite-dimensional vector spaces. Prove that_\n\n\\[\\dim(\\ker T)+\\dim(\\operatorname{range}T)=\\dim V\\,.\\]",
        "Solution": "Let \\(\\{\\alpha*{1},\\dots,\\alpha*{k}\\}\\) be a basis for \\(\\ker T\\) and extend it to \\(\\{\\alpha*{1},\\dots,\\alpha*{k},\\dots,\\alpha*{n}\\}\\), a basis of \\(V\\). We will show that \\(\\{T\\alpha*{k+1},\\dots,T\\alpha*{n}\\}\\) is a basis for the range of \\(T\\). It is obvious they span the range since \\(T\\alpha*{j}=0\\) for \\(j\\leq k\\). Assume\n\n\\[\\sum*{i=k+1}^{n}c*{i}(T\\alpha\\_{i})=0\\]\n\nwhich is equivalent to\n\n\\[T\\left(\\sum*{i=k+1}^{n}c*{i}T\\alpha\\_{i}\\right)=0\\]\n\nthat is, \\(\\alpha=\\sum*{i:=k+1}^{n}c*{i}\\alpha*{i}\\) is in the kernel of \\(T\\). We can then write \\(\\alpha\\) as \\(\\alpha=\\sum*{i=1}^{k}b*{i}\\alpha*{i}\\) and have\n\n\\[\\sum*{i=1}^{k}b*{i}\\alpha*{i}-\\sum*{i=k+1}^{n}c*{i}T\\alpha*{i}=0\\]\n\nwhich implies all \\(c*{i}=0\\), and the vectors \\(T\\alpha*{k+1},\\dots,T\\alpha\\_{n}\\) form a basis for the range of \\(T\\)."
    },
    "Problem 7.4.3": {
        "Problem": "_Suppose that \\(W\\subset V\\) are finite-dimensional vector spaces over a field, and let \\(L\\colon V\\to V\\) be a linear transformation with \\(L(V)\\subset W\\). Denote the restriction of \\(L\\) to \\(W\\) by \\(L_{W}\\). Prove that \\(\\det(1-tL)=\\det(1-tL*{W})\\).*",
        "Solution": "Let \\(v*{1}\\),..., \\(v*{n}\\) be a basis for \\(V\\) such that \\(v*{1}\\),..., \\(v*{k}\\) is a basis for \\(W\\). Then the matrix for \\(L\\) in terms of this basis has the form \\(\\left(\\begin{smallmatrix}M&N\\\\ 0&0\\end{smallmatrix}\\right)\\), where \\(M\\) is a \\(k\\times k\\) matrix and \\(N\\) is \\(k\\times(n-k)\\). It follows that \\(M\\) is the matrix of \\(L*{W}\\) with respect to the basis \\(v*{1}\\),..., \\(v*{k}\\). As the matrix of \\(1-tL\\) is \\(\\left(\\begin{smallmatrix}1-tM&-tN\\\\ 0&1\\end{smallmatrix}\\right)\\), it follows that \\(\\det(1-tL)=\\det(1-tM)=\\det(1-tL*{W})\\)."
    },
    "Problem 7.4.4": {
        "Problem": "_Let \\(V\\) be a finite-dimensional vector space over a field \\(\\mathbf{F}\\), and let \\(L:V\\to V\\) be a linear transformation. Suppose that the characteristic polynomial \\(\\chi\\) of \\(L\\) is written as \\(\\chi=\\chi_{1}\\chi*{2}\\), where \\(\\chi*{1}\\) and \\(\\chi*{2}\\) are two relatively prime polynomials with coefficients in \\(\\mathbf{F}\\). Show that \\(V\\) can be written as the direct sum of two subspaces \\(V*{1}\\) and \\(V*{2}\\) with the property that \\(\\chi*{i}(L)V*{i}=0\\) (for \\(i=1\\) and \\(2\\)).*",
        "Solution": "Let \\(V*{i}=\\{v\\in V\\,|\\,\\chi*{i}(L)(v)=0\\}\\), for \\(i=1\\), \\(2\\). Clearly, each \\(V*{i}\\) is a subspace of \\(V\\) with \\(\\chi*{i}(L)V*{i}=0\\). To show that \\(V\\) is the direct sum of \\(V*{1}\\) and \\(V*{2}\\), choose polynomials \\(a\\) and \\(b\\) over \\(\\mathbf{F}\\) for which \\(a\\chi*{1}+b\\chi*{2}=1\\). Then \\(a(L)\\chi*{1}(L)+b(L)\\chi*{2}(L)=1\\). If \\(v\\in V*{1}\\cap V*{2}\\), then \\(v=1\\cdot v=a(L)\\chi*{1}(L)+b(L)\\chi*{2}(L)v=a(L)0+b(L)0=0\\), so \\(V*{1}\\cap V*{2}=\\{0\\}\\). If \\(v\\in V\\), then by Cayley-Hamilton Theorem [13, pag. 194], we have \\(\\chi(L)v=0\\). Hence, \\(v*{1}=a(L)\\chi*{1}(L)v\\) is annihilated by \\(\\chi*{2}(L)\\) and, therefore, belongs to \\(V*{2}\\). Likewise, \\(v*{2}=b(L)\\chi*{2}(L)v\\) belongs to \\(V*{1}\\). Since \\(v=v*{1}+v*{2}\\), this shows that \\(V=V*{1}+V*{2}\\)."
    },
    "Problem 7.4.5": {
        "Problem": "_Let \\(E\\) be a three-dimensional vector space over \\(\\mathbb{Q}\\). Suppose \\(T:E\\to E\\) is a linear transformation and \\(Tx=y\\), \\(Ty=z\\), \\(Tz=x+y\\), for certain \\(x,y,z\\in E\\), \\(x\\neq 0\\). Prove that \\(x\\), \\(y\\), and \\(z\\) are linearly independent._",
        "Solution": null
    },
    "Problem 7.4.6": {
        "Problem": "_Let \\(T:V\\to V\\) be an invertible linear transformation of a vector space \\(V\\). Denote by \\(G\\) the group of all maps \\(f_{k,a}:V\\to V\\) where \\(k\\in\\mathbb{Z}\\), \\(a\\in V\\), and for \\(x\\in V\\),\\_\n\n\\[f\\_{k,a}(x)=T^{k}x+a\\ \\ \\ \\ (x\\in V).\\]\n\n_Prove that the commutator subgroup \\(G^{\\prime}\\) of \\(G\\) is isomorphic to the additive group of the vector space \\((T-I)V\\), the image of \\(T-I\\). (\\(G^{\\prime}\\) is generated by all \\(ghg^{-1}h^{-1}\\), \\(g\\) and \\(h\\) in \\(G\\).)_",
        "Solution": null
    },
    "Problem 7.4.7": {
        "Problem": "_Let \\(V\\) be a finite-dimensional vector space and \\(A\\) and \\(B\\) two linear transformations of \\(V\\) into itself such that \\(A^{2}=B^{2}=0\\) and \\(AB+BA=I\\)._\n\n1. _Prove that if_ \\(N*{A}\\) \\_and* \\(N*{B}\\) \\_are the respective 'null' spaces of* \\(A\\) _and_ \\(B,\\) _then_ \\(N*{A}=AN*{B}\\)_,_ \\(N*{B}=BN*{A}\\)_, and_ \\(V=N*{A}\\oplus N*{B}\\)_._\n2. _Prove that the dimension of_ \\(V\\) _is even._\n3. _Prove that if the dimension of_ \\(V\\) _is_ \\(2\\)_, then_ \\(V\\) _has a basis with respect to which_ \\(A\\) _and_ \\(B\\) _are represented by the matrices_ \\[\\left(\\begin{array}{cc}0&1\\\\ 0&0\\end{array}\\right)\\ \\ \\ and\\ \\ \\ \\left(\\begin{array}{cc}0&0\\\\ 1&0\\end{array}\\right).\\]",
        "Solution": null
    },
    "Problem 7.4.8": {
        "Problem": "_Let \\(f:\\mathbb{R}^{m}\\to\\mathbb{R}^{n},\\ n\\geq 2\\), be a linear transformation of rank \\(n-1\\). Let \\(f(v)=(f_{1}(v),f*{2}(v),\\ldots,f*{n}(v))\\) for \\(v\\in\\mathbb{R}^{m}\\). Show that a necessary and sufficient condition for the system of inequalities \\(f*{i}(v)>0\\), \\(i=1,\\ldots,n\\), to have no solution is that there exist real numbers \\(\\lambda*{i}\\geq 0\\), not all zero, such that\\_\n\n\\[\\sum*{i=1}^{n}\\lambda*{i}f\\_{i}=0.\\]",
        "Solution": "Since the linear transformation \\(f\\) has rank \\(n-1\\), we know that \\(f(\\mathbb{R}^{m})\\) is an \\(n-1\\)-dimensional subspace of \\(\\mathbb{R}^{n}\\). Hence, there exist real constants \\(\\lambda*{1},\\ldots,\\lambda*{n}\\), not all zero, such that\n\n\\[\\sum*{i=1}^{n}\\lambda*{i}f\\_{i}(v)=0\\]\n\nfor all \\(v\\in\\mathbb{R}^{m}\\). The \\(\\lambda\\_{i}\\)'s are unique up to constant multiples. Further, this equation determines the subspace: If \\(w\\in\\mathbb{R}^{n}\\) satisfies it, then \\(w\\in f(\\mathbb{R}^{m})\\).\n\nNow suppose that the \\(\\lambda*{i}\\)'s all have the same sign, or, without loss of generality, that they are all nonnegative. Then if there existed \\(v\\in\\mathbb{R}^{m}\\) with \\(f*{i}(v)>0\\) for all \\(i\\), we would have \\(\\sum\\lambda*{i}f*{i}(v)>0\\), a contradiction. Hence, there can be no such \\(v\\).\n\nConversely, suppose that two of the \\(\\lambda*{i}\\)'s, say \\(\\lambda*{1}\\) and \\(\\lambda*{2}\\), have different signs. Let \\(x*{3}=x*{4}=\\cdots=x*{n}=1\\), and choose \\(x\\_{1}>0\\) sufficiently large so that\n\n\\[\\sum*{i\\neq 2}\\lambda*{i}x\\_{i}>0.\\]\n\nThen there is a real number \\(x\\_{2}>0\\) such that\n\n\\[\\sum*{i=1}^{n}\\lambda*{i}x\\_{i}=0.\\]\n\nBut then we know that there exists \\(v\\in f(\\mathbb{R}^{m})\\) such that \\(f(v)=(x*{1},\\ldots,x*{n})\\). Since each of the \\(x\\_{i}\\)'s is positive, we have found the desired point \\(v\\)."
    },
    "Problem 7.4.9": {
        "Problem": "_Let \\(n\\) be a positive integer, and let \\(S\\subset\\mathbb{R}^{n}\\) a finite subset with \\(0\\in S\\). Suppose that \\(\\varphi:S\\to S\\) is a map satisfying_\n\n\\[\\varphi(0) =0,\\] \\[d(\\varphi(s),\\varphi(t)) =d(s,t)\\qquad for\\ all\\quad s,t\\in S,\\]\n\n_where \\(d(\\,\\ )\\) denotes Euclidean distance. Prove that there is a linear map \\(f:\\mathbb{R}^{n}\\to\\mathbb{R}^{n}\\) whose restriction to \\(S\\) is \\(\\varphi\\)._",
        "Solution": "Let \\(\\langle\\,\\ \\rangle\\) denote the ordinary inner product. From \\(d(s,t)^{2}=d(s,0)^{2}+d(t,0)^{2}-2\\langle s,t\\rangle\\) and the hypothesis, it follows that\n\n\\[\\langle\\varphi(s),\\varphi(t)\\rangle=\\langle s,t\\rangle\\qquad\\text{for all }\\ s,t\\in S.\\]\n\nLet \\(V\\subset\\mathbb{R}^{n}\\) denote the subspace spanned by \\(S\\), and choose a subset \\(T\\subset S\\) that is a basis of \\(V\\). Clearly, there is a unique linear map \\(f:V\\to V\\) that agrees with \\(\\varphi\\) on \\(T\\). Then one has \\(\\langle f(t),f(t^{\\prime})\\rangle=\\langle t,t^{\\prime}\\rangle\\) for all \\(t\\) and \\(t^{\\prime}\\in T\\). By bilinearity, it follows that \\(\\langle f(v),f(v^{\\prime})\\rangle=\\langle v,v^{\\prime}\\rangle\\) for all \\(v\\) and \\(v^{\\prime}\\in V\\). Taking \\(v=v^{\\prime}\\), one finds that \\(f(v)\\neq 0\\) for \\(v\\neq 0\\), so \\(f\\) is injective, and \\(f(V)=V\\). Taking \\(v=s\\in S\\) and \\(v^{\\prime}=t\\in T\\), one finds that\n\n\\[\\langle f(s),f(t)\\rangle=\\langle s,t\\rangle=\\langle\\varphi(s),\\varphi(t) \\rangle=\\langle\\varphi(s),f(t)\\rangle,\\]\n\nso \\(f(s)-\\varphi(s)\\) is orthogonal to \\(f(t)\\) for all \\(t\\in T\\), and hence to all of \\(f(V)=V\\). That is, for all \\(s\\in S\\), one has \\(f(s)-\\varphi(s)\\in V^{\\perp}\\); but also \\(f(s)-\\varphi(s)\\in V\\), so \\(f(s)-\\varphi(s)=0\\). This shows that \\(f\\) agrees with \\(\\varphi\\) on \\(S\\). It now suffices to extend \\(f\\) to a linear map \\(\\mathbb{R}^{n}\\to\\mathbb{R}^{n}\\), which one can do by supplementing \\(T\\) to a basis for \\(\\mathbb{R}^{n}\\) and defining \\(f\\) arbitrarily on the new basis vectors."
    },
    "Problem 7.4.10": {
        "Problem": "_Consider \\(\\mathbb{R}^{2}\\) be equipped with the Euclidean metric \\(d(x,y)=\\|x-y\\|\\). Let \\(T\\) be an isometry of \\(\\mathbb{R}^{2}\\) into itself. Prove that \\(T\\) can be represented as \\(T(x)=a+U(x)\\), where \\(a\\) is a vector in \\(\\mathbb{R}^{2}\\) and \\(U\\) is an orthogonal linear transformation._",
        "Solution": null
    },
    "Problem 7.4.11": {
        "Problem": "_Let \\(X\\) be a set and \\(V\\) a real vector space of real valued functions on \\(X\\) of dimension \\(n\\), \\(0<n<\\infty\\). Prove that there are \\(n\\) points \\(x_{1},x*{2},\\ldots,x*{n}\\) in \\(X\\) such that the map \\(f\\to(f(x*{1}),\\ldots,f(x*{n}))\\) of \\(V\\) to \\(\\mathbb{R}^{n}\\) is an isomorphism (i.e., one-to-one and onto). (The operations of addition and scalar multiplication in \\(V\\) are assumed to be the natural ones.)\\_",
        "Solution": "We use Complete Induction [11, pag. 32] on the dimension of \\(V\\). If \\(\\dim V=1\\), then \\(V\\) has a single basis vector \\(f*{1}\\neq 0\\), so there is \\(x*{1}\\in X\\) such that \\(f*{1}(x*{1})\\neq 0\\). Hence, the map \\(f\\mapsto f(x\\_{1})\\) is the desired isomorphism.\n\nNow suppose the result is true for dimensions less than \\(n\\) and let \\(\\dim V=n\\). Fix a basis \\(f*{1},f*{2},\\ldots,f*{n}\\) of \\(V\\). Then, by the induction hypothesis, there are points \\(x*{1},x*{2},\\ldots,x*{n-1}\\) such that the map \\(f\\mapsto(f(x*{1}),\\ldots,f(x*{n-1}),0)\\) is an isomorphism of the subspace of \\(V\\) spanned by \\(f*{1},\\ldots,f*{n-1}\\) onto \\(\\mathbb{R}^{n-1}\\subset\\mathbb{R}^{n}\\). In particular, the vector \\((f*{n}(x*{1}),\\ldots,f*{n}(x*{n-1}),0)\\) is a linear combination of the basis vectors \\(\\{(f*{i}(x*{1}),\\ldots,f*{i}(x*{n-1}),0),1\\leq i\\leq n-1\\}\\), so there exists a unique set of \\(\\lambda\\_{i}\\)'s, \\(1\\leq i\\leq n\\), such that\n\n\\[\\sum*{i=1}^{n}\\lambda*{i}f*{i}(x*{j})=0,\\ \\ \\ \\ 1\\leq j\\leq n-1.\\]\n\nSuppose there is no point \\(x\\in X\\) such that the given map is an isomorphism from \\(V\\) onto \\(\\mathbb{R}^{n}\\). This implies that the set \\(\\{(f*{i}(x*{1}),\\ldots,f*{i}(x*{n-1}),f*{i}(x)),\\)\\(1\\leq i\\leq n\\}\\) is linearly dependent for all \\(x\\). But because of the uniqueness of the \\(\\lambda*{i}\\)'s, this, in turn, implies that for all \\(x\\),\n\n\\[\\sum*{i=1}^{n}\\lambda*{i}f\\_{i}(x)=0.\\]\n\nHence, the \\(f*{i}\\)'s are linearly dependent in \\(V\\), a contradiction. Therefore, such an \\(x*{n}\\) exists and we are done."
    },
    "Problem 7.4.12": {
        "Problem": "_Suppose that \\(X\\) is a topological space and \\(V\\) is a finite-dimensional subspace of the vector space of continuous real valued functions on \\(X\\). Prove that there exist a basis \\((f_{1},\\ldots,f*{n})\\) for \\(V\\) and points \\(x*{1},\\ldots,x*{n}\\) in \\(X\\) such that \\(f*{i}(x*{j})=\\delta*{ij}\\).\\_",
        "Solution": null
    },
    "Problem 7.4.13": {
        "Problem": "_Let \\(n\\) be a positive integer and let \\(P_{2n+1}\\) be the vector space of real polynomials whose degrees are, at most, \\(2n+1\\). Prove that there exist unique real numbers \\(c*{1},\\ldots,c*{n}\\) such that\\_\n\n\\[\\int*{-1}^{1}p(x)\\,dx=2p(0)+\\sum*{k=1}^{n}c\\_{k}(p(k)+p(-k)-2p(0))\\]\n\n_for all \\(p\\in P_{2n+1}\\).\\_",
        "Solution": "Since the formula holds, irrespective of the values of \\(c*{k}\\), for the polynomials \\(x^{2n+1}\\), it suffices, by linearity, to restrict to the vector space \\(P*{2n}\\) of polynomials of degree, at most, \\(2n\\). This vector space has dimension \\(2n+1\\) and the map \\(P*{2n}\\to\\mathbb{R}^{2n+1}\\) given by \\(p\\mapsto(p(-n),p(-n+1),\\ldots,p(n))\\) is an isomorphism. As the integral is a linear function on \\(\\mathbb{R}^{2n+1}\\), there exist unique real numbers \\(c*{-n},c*{-n+1},\\ldots,c*{n}\\) such that\n\n\\[\\int*{-1}^{1}p(x)dx=\\sum*{k=-n}^{n}c*{k}p(k)\\qquad\\mbox{for all $p\\in P*{2n}$.}\\]\n\nWe have\n\n\\[\\int*{-1}^{1}p(x)dx=\\int*{-1}^{1}p(-x)dx=\\sum*{k=-n}^{n}c*{k}p(k)\\quad\\mbox{ for all $p\\in P_{2n}$,}\\]\n\nso \\(c*{k}=c*{-k}\\) by uniqueness of the \\(c\\_{k}\\), and, therefore,\n\n\\[\\int*{-1}^{1}p(x)dx=c*{0}p(0)+\\sum*{k=1}^{n}c*{k}\\left(p(k)+p(-k)\\right)\\qquad \\mbox{for all $p\\in P_{2n}$.}\\]Setting \\(p=1\\), we find that\n\n\\[2=c*{0}+\\sum*{k=1}^{n}2c\\_{k}\\]\n\nso, upon eliminating \\(c\\_{0}\\),\n\n\\[\\int*{-1}^{1}p(x)dx=2p(0)+\\sum*{k=1}^{n}c\\_{k}\\left(p(k)+p(-k)-2p(0)\\right).\\]"
    },
    "Problem 7.4.14": {
        "Problem": "_Let \\(T:\\mathbb{R}^{n}\\to\\mathbb{R}^{n}\\) be a diagonalizable linear transformation. Prove that there is an orthonormal basis for \\(\\mathbb{R}^{n}\\) with respect to which \\(T\\) has an upper-triangular matrix._",
        "Solution": "Let \\(v*{1},v*{2},\\ldots,v*{n}\\) be a basis for \\(\\mathbb{R}^{n}\\) consisting of eigenvectors of \\(T\\), say \\(Tv*{n}=\\lambda*{n}v*{n}\\). Let \\(u*{1},u*{2},\\ldots,u*{n}\\) be the orthonormal basis one obtains from \\(v*{1},v*{2},\\ldots,v*{n}\\) by the Gram-Schmidt Procedure [1, pag. 280]. Then, for each index \\(k\\), the vector \\(u*{k}\\) is a linear combination of \\(v*{1},\\ldots,v\\_{k}\\), say\n\n\\[u*{k}=c*{k1}v*{1}+c*{k2}v*{2}+\\cdots+c*{kk}v\\_{k}\\.\\]\n\nAlso, each \\(v*{k}\\) is a linear combination of \\(u*{1},\\ldots,u*{k}\\). (This is guaranteed by the Gram-Schmidt Procedure; in fact, \\(u*{1},\\ldots,u*{k}\\) is an orthonormal basis for the subspace generated by \\(v*{1},\\ldots,v\\_{k}\\).) We have\n\n\\[Tu*{k} =c*{k1}Tv*{1}+c*{k2}Tv*{2}+\\cdots+c*{kk}Tv*{k}\\] \\[=c*{k1}\\lambda*{1}v*{1}+c*{k2}\\lambda*{2}v*{2}+\\cdots+c*{kk} \\lambda*{k}v*{k}\\.\\]\n\nIn view of the preceding remark, it follows that \\(Tu*{k}\\) is a linear combination of \\(u*{1},\\ldots,u*{k}\\), and, thus, \\(T\\) has an upper-triangular matrix in the basis \\(u*{1},u*{2},\\ldots,u*{n}\\)."
    },
    "Problem 7.4.15": {
        "Problem": "_Let \\(P\\) be a linear operator on a finite-dimensional vector space over a finite field. Show that if \\(P\\) is invertible, then \\(P^{n}=I\\) for some positive integer \\(n\\)._",
        "Solution": null
    },
    "Problem 7.4.16": {
        "Problem": "_Let \\(A\\) be an \\(n\\times n\\) complex matrix, and let \\(B\\) be the Hermitian transpose of \\(A\\) (i.e., \\(b_{ij}=\\overline{a}_{ji}\\)). Suppose that \\(A\\) and \\(B\\) commute with each other. Consider the linear transformations \\(\\alpha\\) and \\(\\beta\\) on \\(\\mathbb{C}\\,^{n}\\) defined by \\(A\\) and \\(B\\). Prove that \\(\\alpha\\) and \\(\\beta\\) have the same image and the same kernel._",
        "Solution": null
    },
    "Problem 7.4.18": {
        "Problem": "_Let \\(A\\) be a linear transformation on \\(\\mathbb{R}^{3}\\) whose matrix (relative to the usual basis for \\(\\mathbb{R}^{3}\\)) is both symmetric and orthogonal. Prove that \\(A\\) is either plus or minus the identity, or a rotation by \\(180^{\\circ}\\) about some axis in \\(\\mathbb{R}^{3}\\), or a reflection about some two-dimensional subspace of \\(\\mathbb{R}^{3}\\)._",
        "Solution": null
    },
    "Problem 7.4.19": {
        "Problem": "_Let \\(\\theta\\) and \\(\\varphi\\) be fixed, \\(0\\leq\\theta\\leq 2\\pi\\), \\(0\\leq\\varphi\\leq 2\\pi\\) and let \\(R\\) be the linear transformation from \\(\\mathbb{R}^{3}\\) to \\(\\mathbb{R}^{3}\\) whose matrix in the standard basis \\(\\vec{i}\\), \\(\\vec{j}\\), and \\(\\vec{k}\\) is_\n\n\\[\\left(\\begin{array}{ccc}1&0&0\\\\ 0&\\cos\\theta&\\sin\\theta\\\\ 0&-\\sin\\theta&\\cos\\theta\\end{array}\\right)\\]\n\n_Let \\(S\\) be the linear transformation of \\(\\mathbb{R}^{3}\\) to \\(\\mathbb{R}^{3}\\) whose matrix in the basis_\n\n\\[\\frac{1}{\\sqrt{2}}(\\vec{i}+\\vec{k}),\\ \\ \\vec{j},\\ \\ \\frac{1}{\\sqrt{2}}(\\vec{i}- \\vec{k})\\]\n\n_is_\n\n\\[\\left(\\begin{array}{ccc}\\cos\\varphi&\\sin\\varphi&0\\\\ -\\sin\\varphi&\\cos\\varphi&0\\\\ 0&0&1\\end{array}\\right)\\]\n\n_Prove that \\(T=R\\circ S\\) leaves a line invariant._",
        "Solution": "Clearly, both \\(R\\) and \\(S\\) are rotations and so have rank 3. Therefore, \\(T\\), their composition, is a rank 3 operator. In particular, it must have trivial kernel. Since \\(T\\) is an operator on \\(\\mathbb{R}^{3}\\), its characteristic polynomial is of degree 3, and so it has a real root. This root is an eigenvalue, which must be nontrivial since \\(T\\) has trivial kernel. Hence, the associated eigenspace must contain a line which is fixed by \\(T\\)."
    },
    "Problem 7.4.20": {
        "Problem": "_Let \\(e=(a,b,c)\\) be a unit vector in \\(\\mathbb{R}^{3}\\) and let \\(T\\) be the linear transformation on \\(\\mathbb{R}^{3}\\) of rotation by \\(180^{\\circ}\\) about e. Find the matrix for \\(T\\) with respect to the standard basis \\(e_{1}=(1,0,0)\\), \\(e*{2}=(0,1,0)\\), and \\(e*{3}=(0,0,1)\\).\\_",
        "Solution": "Let \\(x=(x*{1},x*{2},x*{3})\\) in the standard basis of \\(\\mathbb{R}^{3}\\). The line joining the points \\(x\\) and \\(Tx\\) intersects the line containing \\(e\\) at the point \\(f=\\langle e,x\\rangle e\\) and is perpendicular to it. We then have \\(Tx=2(f-x)+x=2f-x\\), or, in the standard basis, \\(Tx=(2\\langle e,x\\rangle a-x*{1},2\\langle e,x\\rangle b-x*{2},2\\langle e,x\\rangle c -x*{3})\\). With respect to the standard basis for \\(\\mathbb{R}^{3}\\), the columns of the matrix of \\(T\\) are \\(Te*{1}\\), \\(Te*{2}\\), and \\(Te*{3}\\). Applying our formula and noting that \\(\\langle e,e*{1}\\rangle=a\\), \\(\\langle e,e*{2}\\rangle=b\\), and \\(\\langle e,e*{3}\\rangle=c\\), we get that the matrix for \\(T\\) is\n\n\\[\\left(\\begin{array}{ccc}2a^{2}-1&2ab&2ac\\\\ 2ab&2b^{2}-1&2bc\\\\ 2ac&2bc&2c^{2}-1\\end{array}\\right).\\]"
    },
    "Problem 7.4.21": {
        "Problem": "_Exhibit a real 3\\(\\times\\)3 matrix having minimal polynomial \\((t^{2}+1)(t-10)\\), which, as a linear linear transformation of \\(\\mathbb{R}^{3}\\), leaves invariant the line \\(L\\) through \\((0,0,0)\\) and \\((1,1,1)\\) and the plane through \\((0,0,0)\\) perpendicular to \\(L\\)._",
        "Solution": "Since the minimal polynomial divides the characteristic polynomial and this last one has degree \\(3\\), it follows that the characteristic polynomial of \\(T\\) is \\((t^{2}+1)(t-10)\\) and the eigenvalues \\(\\pm i\\) and \\(10\\).\n\nNow \\(T(1,1,1)=\\lambda(1,1,1)\\) implies that \\(\\lambda=10\\) because \\(10\\) is the unique real eigenvalue of \\(T\\).\n\nThe plane perpendicular to \\((1,1,1)\\) is generated by \\((1,-1,0)\\) and \\((\\frac{1}{2},\\frac{1}{2},-1)\\) since these are perpendicular to each other and to \\((1,1,1)\\).\n\nLet\n\n\\[\\begin{array}{rcl}f*{1}&=&(1,1,1)\\\\ f*{2}&=&(1,-1,0)/\\sqrt{2}\\\\ f\\_{3}&=&\\left(\\frac{1}{2},\\frac{1}{2},-1\\right)/\\sqrt{\\frac{1}{4}+\\frac{1}{4}+ 1}=\\left(\\frac{1}{2},\\frac{1}{2},-1\\right)/\\sqrt{\\frac{3}{2}}\\end{array}\\]\n\nwe have \\(Tf*{1}=10f*{1}\\) and, for \\(\\pm i\\) to be the other eigenvalues of \\(T\\), \\(Tf*{2}=f*{3}\\), and \\(Tf*{3}=-f*{2}\\).\n\nThe matrix of \\(T\\) in the basis \\(\\{f*{1},f*{2},f\\_{3}\\}=\\beta\\) is then\n\n\\[[T]\\_{\\beta}=\\left(\\begin{array}{ccc}10&0&0\\\\ 0&0&1\\\\ 0&-1&0\\end{array}\\right)\\]\n\nThe matrix that transforms the coordinates relative to the basis \\(\\beta\\) into the coordinates relative to the canonical basis is\n\n\\[P=\\left(\\begin{array}{ccc}1&1/\\sqrt{2}&\\sqrt{6}/4\\\\ 1&-1/\\sqrt{2}&\\sqrt{6}/4\\\\ 1&0&-\\sqrt{6}/2\\end{array}\\right)\\]\n\nand a calculation gives\n\n\\[P^{-1}=\\left(\\begin{array}{ccc}1/3&1/3&1/3\\\\ \\sqrt{2}/2&-\\sqrt{2}/2&0\\\\ 2/3\\sqrt{6}&2/3\\sqrt{6}&-4/3\\sqrt{6}\\end{array}\\right).\\]\n\nTherefore, the matrix of \\(T\\) in the canonical basis is\n\n\\[[T]=P[T]\\_{\\beta}P^{-1}=\\left(\\begin{array}{ccc}\\frac{10}{3}+\\frac{1}{3\\sqrt {3}}&\\frac{10}{3}+\\frac{13}{36}\\sqrt{3}&\\frac{10}{3}-\\frac{2}{3\\sqrt{3}}\\\\ \\frac{10}{3}-\\frac{1}{3\\sqrt{3}}&\\frac{10}{3}+\\frac{5\\sqrt{3}}{36}&\\frac{10}{ 3}+\\frac{2}{3\\sqrt{3}}\\\\ \\frac{10}{3}+\\frac{\\sqrt{3}}{2}&\\frac{10}{3}-\\frac{\\sqrt{3}}{2}&\\frac{10}{3} \\end{array}\\right).\\]"
    },
    "Problem 7.4.22": {
        "Problem": "_Show that every rotation of \\(\\mathbb{R}^{3}\\) has an axis; that is, given a 3\\(\\times\\)3 real matrix \\(A\\) such that \\(A^{t}=A^{-1}\\) and \\(\\det A>0\\), prove that there is a nonzero vector v such that Av = v._",
        "Solution": null
    },
    "Problem 7.4.23": {
        "Problem": "_Let \\(P\\) be the vector space of polynomials over \\(\\mathbb{R}\\). Let the linear transformation \\(E:P\\to P\\) be defined by \\(Ef=f+f^{\\prime}\\), where \\(f^{\\prime}\\) is the derivative of \\(f\\). Prove that \\(E\\) is invertible._",
        "Solution": "For \\(n=1,2,\\ldots\\), let \\(P*{n}\\) be the space of polynomials whose degrees are, at most, \\(n\\). The subspaces \\(P*{n}\\) are invariant under \\(E\\), they increase with \\(n\\), and their union is \\(P\\). To prove \\(E\\) is invertible (i.e., one-to-one and onto), it will suffice to prove that each restriction \\(E|_{P_{n}}\\) is invertible. The subspace \\(P*{n}\\) is of dimension \\(n+1\\), it has the basis \\(1,x,x^{2},\\ldots,x^{n}\\), with respect to which the matrix of \\(E|*{P\\_{n}}\\) is\n\n\\[\\left(\\begin{array}{cccccc}1&1&0&0&\\cdots&0\\\\ 0&1&2&0&\\cdots&0\\\\ 0&0&1&3&\\cdots&0\\\\ \\vdots&\\vdots&\\vdots&\\ddots&\\ddots&\\vdots\\\\ 0&0&0&0&1&n\\\\ 0&0&0&0&1\\end{array}\\right)\\.\\]\n\nIn particular, the matrix is upper-triangular, with \\(1\\) at every diagonal entry, so its determinant is \\(1\\). Thus, \\(E|_{P_{n}}\\) is invertible, as desired. Alternatively, since \\(\\deg\\,Ef=\\deg\\,f\\), the kernel of \\(E\\) is trivial, so its restriction to any finite dimensional invariant subspace is invertible.\n\n_Solution 2._ We can describe \\(E\\) to be \\(I+D\\), where \\(I\\) is the identity operator and \\(D\\) is the derivative operator, on the vector space of all real polynomials \\(P\\). For any element \\(f\\) of \\(P\\), there exists \\(n\\) such that \\(D^{n}(f)=0\\); namely \\(n=\\deg p+1\\). Thus, the inverse of \\(E\\) can be described as \\(I-D+D^{2}-D^{3}+\\cdots\\).\n\nSpecifically, writing elements of \\(P\\) as polynomials in \\(x\\), we have \\(E^{-1}(1)=1\\), \\(E^{-1}(x)=x-1\\), \\(E^{-1}(x^{2})=x^{2}-2x+2\\), etc."
    },
    "Problem 7.4.24": {
        "Problem": "_Let \\(P_{n}\\) be the vector space of all real polynomials with degrees at most n. Let \\(D:P*{n}\\to P*{n}\\) be given by differentiation: \\(D(p)=p^{\\prime}\\). Let \\(\\pi\\) be a real polynomial. What is the minimal polynomial of the transformation \\(\\pi(D)\\)?\\_",
        "Solution": "Given the polynomial \\(\\pi(x)\\), there are constants \\(a\\) and \\(r>0\\) and a polynomial \\(\\varphi(x)\\) such that \\(\\pi(x)=x^{r}\\varphi(x)+a\\). If \\(\\varphi(x)\\equiv 0\\), then \\(\\pi(D)=aI\\), it follows that the minimal polynomial of the operator \\(\\pi(D)\\) is \\(x-a\\). If \\(\\varphi(x)\\) is not zero, then for any polynomial \\(f\\in P*{n}\\), by the definition of \\(D\\), \\((\\pi(D)-aI)(f(x))=g(x)\\), where \\(g(x)\\) is some polynomial such that \\(\\deg g=\\max(\\deg f-r,0)\\). Hence, letting \\(E=\\pi(D)-aI\\), we have \\(e^{\\lfloor n/r\\rfloor+1}(f)=0\\) for all \\(f\\in P*{n}\\). (\\(\\lfloor n/r\\rfloor\\) denotes the greatest integer less than or equal to \\(n/r\\).) The polynomial \\(f(x)=x^{n}\\) shows that \\(\\lfloor n/r\\rfloor+1\\) is the minimal degree such that this is true. It follows from this that the minimal polynomial of \\(\\pi(D)\\) is \\((x-a)^{\\lfloor n/r\\rfloor+1}\\).\n\n### 7.5 Eigenvalues and Eigenvectors"
    },
    "Problem 7.4.25": {
        "Problem": "_Let \\(V\\) be the vector space of all polynomials of degree \\(\\leq 10\\), and let \\(D\\) be the differentiation operator on \\(V\\) (i.e., \\(Dp(x)=p^{\\prime}(x)\\))._\n\n1. _Show that_ \\(\\operatorname{tr}\\,D=0\\)_._\n2. _Find all eigenvectors of_ \\(D\\) _and_ \\(e^{D}\\)_._\n\n### Eigenvalues and Eigenvectors",
        "Solution": null
    },
    "Problem 7.5.1": {
        "Problem": "_Let \\(M\\) be a real 3\\(\\times\\)3 matrix such that \\(M^{3}=I\\), \\(M\\neq I\\)._\n\n1. _What are the eigenvalues of_ \\(M\\)_?_\n2. _Give an example of such a matrix._",
        "Solution": "1. The minimal polynomial of \\(M\\) divides \\(x^{3}-1=(x-1)(x^{2}+x+1)\\); since \\(M\\neq I\\), the minimal polynomial (and characteristic as well) is \\((x-1)(x^{2}+x+1)\\) and the only possible real eigenvalue is \\(1\\).\n\n2.\n\n\\[\\left(\\begin{array}{ccc}1&0&0\\\\ 0&\\cos\\frac{2\\pi}{3}&\\sin\\frac{2\\pi}{3}\\\\ 0&-\\sin\\frac{2\\pi}{3}&\\cos\\frac{2\\pi}{3}\\end{array}\\right)\\]"
    },
    "Problem 7.5.2": {
        "Problem": "_Let \\(N\\) be a linear operator on an n-dimensional vector space, \\(n>1\\), such that \\(N^{n}=0,\\ N^{n-1}\\ \\neq 0.\\) Prove there is no operator X with \\(X^{2}=N\\)._",
        "Solution": "Suppose such \\(X\\) exists; then the characteristic polynomial for \\(X\\) is \\(\\chi\\_{X}(t)=t^{n}\\), but this is a contradiction since \\(X^{2n-1}\\neq 0\\) and \\(2n-1>n\\)."
    },
    "Problem 7.5.3": {
        "Problem": "_Let \\(\\mathbf{F}\\) be a field, \\(n\\) and \\(m\\) positive integers, and \\(A\\) an \\(n\\times n\\) matrix with entries in \\(\\mathbf{F}\\) such that \\(A^{m}=0\\). Prove that \\(A^{n}=0\\)._",
        "Solution": "Since \\(A^{m}=0\\) for some \\(m\\), \\(A\\) is a root of the polynomial \\(p(x)=x^{m}\\). By the definition of the minimal polynomial, \\(\\mu*{A}(t)|p(t)\\), so \\(\\mu*{A}(t)=t^{k}\\) for some \\(k\\leq n\\), then \\(A^{n}=A^{k}=0\\)."
    },
    "Problem 7.5.5": {
        "Problem": "_Let \\(\\mathbf{F}\\) be a field, \\(V\\) a finite-dimensional vector space over \\(\\mathbf{F}\\), and \\(T\\) a linear transformation of \\(V\\) into \\(V\\) whose minimum polynomial, \\(\\mu\\), is irreducible over \\(\\mathbf{F}\\)._\n\n1. _Let_ \\(v\\) _be a nonzero vector in_ \\(V\\) _and let_ \\(V*{1}\\) \\_be the subspace spanned by* \\(v\\) _and its images under the positive powers of_ \\(T\\)_. Prove that_ \\(\\dim V*{1}=\\deg\\mu\\)*.\\_\n2. _Prove that_ \\(\\deg\\mu\\) _divides_ \\(\\dim V\\)_._",
        "Solution": "1. Let \\(d=\\deg\\,\\mu\\). Since \\(\\mu(T)v=0\\), the vector \\(T^{d}v\\) is linearly dependent on the vectors \\(v,Tv,\\ldots,T^{d-1}v\\). Hence, \\(T^{d+n}v\\) is linearly dependent on \\(T^{n}v,T^{n+1}v,\\ldots,T^{n+d-1}v\\) and so, by the Induction Principle [11, pag. 7], on \\(v,Tv,\\ldots,T^{d-1}v\\) (\\(n=1,2,\\ldots\\)). Thus, \\(v,Tv,\\ldots,T^{d-1}v\\) span \\(V*{1}\\), so \\(\\dim\\,V*{1}\\leq d\\).\n\nOn the other hand, the minimum polynomial of \\(T|_{V_{1}}\\) must divide \\(\\mu\\) (since \\(\\mu(T|_{V_{1}})=0\\)), so it equals \\(\\mu\\) because \\(\\mu\\) is irreducible. Thus, \\(\\dim\\,V*{1}\\geq d\\). The desired equality, \\(\\dim\\,V*{1}=d\\), now follows.\n\n2. In the case \\(V*{1}\\neq V\\), let \\(T*{1}\\) be the linear transformation on the quotient space \\(V/V*{1}\\) induced by \\(T\\). (It is well defined because \\(V*{1}\\) is \\(T\\)-invariant.) Clearly, \\(\\mu(T*{1})=0\\), so the minimum polynomial of \\(T*{1}\\) divides \\(\\mu\\), hence equals \\(\\mu\\). Therefore, by Part 1, \\(V/V*{1}\\) has a \\(T*{1}\\)-invariant subspace of dimension \\(d\\), whose inverse image under the quotient map is a \\(T\\)-invariant subspace \\(V*{2}\\) of \\(V\\) of dimension \\(2d\\). In the case \\(V*{2}\\neq V\\), we can repeat the argument to show that \\(V\\) has a \\(T\\)-invariant subspace of dimension \\(3d\\), and so on. After finitely many repetitions, we find \\(\\dim\\,V=kd\\) for some integers \\(k\\)."
    },
    "Problem 7.5.7": {
        "Problem": "_Prove that the matrix_\n\n\\[\\left(\\begin{array}{ccc}1&1.00001&1\\\\ 1.00001&1&1.00001\\\\ 1&1.00001&1\\end{array}\\right)\\]\n\n_has one positive eigenvalue and one negative eigenvalue._",
        "Solution": "A calculation shows that the characteristic polynomial of the given matrix is\n\n\\[-x(x^{2}-3x-2(1.00001^{2}-1))\\]\n\nso one of the eigenvalues is 0 and the product of the other two is \\(-2(1.00001^{2}-1))<0\\), so one is negative and the other is positive."
    },
    "Problem 7.5.8": {
        "Problem": "_For arbitrary elements \\(a\\), \\(b\\), and \\(c\\) in a field \\(\\mathbf{F}\\), compute the minimal polynomial of the matrix_\n\n\\[\\left(\\begin{array}{ccc}0&0&a\\\\ 1&0&b\\\\ 0&1&c\\end{array}\\right).\\]",
        "Solution": "Denote the matrix by \\(A\\). A calculation shows that \\(A\\) is a root of the polynomial \\(p(t)=t^{3}-ct^{2}-bt-a\\). In fact, this is the minimal polynomial of \\(A\\). To prove this, it suffices to find a vector \\(x\\in\\mathbf{F}^{3}\\) such that \\(A^{2}x\\), \\(Ax\\), and \\(x\\) are linearly independent. Let \\(x=(1,0,0)\\). Then \\(Ax=(0,1,0)\\) and \\(A^{2}x=(0,0,1)\\); these three vectors are linearly independent, so we are done."
    },
    "Problem 7.5.11": {
        "Problem": "_Let \\(S\\) be a nonempty commuting set of \\(n\\times n\\) complex matrices \\((n\\geq 1)\\). Prove that the members of \\(S\\) have a common eigenvector._",
        "Solution": "We use the Induction Principle [11, pag. 7]. As the space \\(M*{n}(\\mathbb{C}\\,)\\) is finite dimensional, we may assume that \\(S\\) is finite. If \\(S\\) has one element, the result is trivial. Now suppose any commuting set of \\(n\\) elements has a common eigenvector, and let \\(S\\) have \\(n+1\\) elements \\(A*{1},\\ldots,A*{n+1}\\). By induction hypothesis, the matrices \\(A*{1},\\ldots,A*{n}\\) have a common eigenvector \\(v\\). Let \\(E\\) be the vector space spanned by the common eigenvectors of \\(A*{1},\\ldots,A*{n}\\). If \\(v\\in E\\), \\(A*{i}A*{n+1}v=A*{n+1}A*{i}v=\\lambda*{i}A*{n+1}v\\) for all \\(i\\), so \\(A*{n+1}v\\in E\\). Hence, \\(A*{n+1}\\) fixes \\(E\\). Let \\(B\\) be the restriction of \\(A*{n+1}\\) to \\(E\\). The minimal polynomial of \\(B\\) splits into linear factors (since we are dealing with complex matrices), so \\(B\\) has an eigenvector in \\(E\\), which must be an eigenvector of \\(A*{n+1}\\) by the definition of \\(B\\), and an eigenvector for each of the other \\(A*{i}\\)'s by the definition of \\(E\\)."
    },
    "Problem 7.5.12": {
        "Problem": "_Let \\(A\\) and \\(B\\) be complex \\(n\\times n\\) matrices such that \\(AB=BA^{2}\\), and assume \\(A\\) has no eigenvalues of absolute value \\(1\\). Prove that \\(A\\) and \\(B\\) have a common (nonzero) eigenvector._",
        "Solution": null
    },
    "Problem 7.5.13": {
        "Problem": "_Let \\(V\\) be a finite-dimensional vector space over an algebraically closed field. A linear operator \\(T:V\\to V\\) is called completely reducible if whenever a linear subspace \\(E\\subset V\\) is invariant under \\(T\\) (i.e., \\(T(E)\\subset E\\)), there is a linear subspace \\(F\\subset V\\) which is invariant under \\(T\\) and such that \\(V=E\\oplus F\\). Prove that \\(T\\) is completely reducible if and only if \\(V\\) has a basis of eigenvectors._",
        "Solution": null
    },
    "Problem 7.5.15": {
        "Problem": "_Let \\(T\\) be a linear transformation on a finite-dimensional \\(\\mathbb{C}\\) -vector space V, and let \\(f\\) be a polynomial with coefficients in \\(\\mathbb{C}\\). If \\(\\lambda\\) is an eigenvalue of \\(T\\), show that \\(f(\\lambda)\\) is an eigenvalue of \\(f(T)\\). Is every eigenvalue of \\(f(T)\\) necessarily obtained in this way?_",
        "Solution": null
    },
    "Problem 7.5.17": {
        "Problem": "_Let \\(A\\) and \\(B\\) be two \\(n\\times n\\) self-adjoint (i.e., Hermitian) matrices over \\(\\mathbb{C}\\) and assume \\(A\\) is positive definite. Prove that all eigenvalues of \\(AB\\) are real._",
        "Solution": "Since \\(A\\) is positive definite, there is an invertible Hermitian matrix \\(C\\) such that \\(C^{2}=A\\). Thus, we have \\(C^{-1}(AB)C=C^{-1}C^{2}BC=CBC\\). By taking adjoints, we see that \\(CBC\\) is Hermitian, so it has real eigenvalues. Since similar matrices have the same eigenvalues, \\(AB\\) has real eigenvalues."
    },
    "Problem 7.5.18": {
        "Problem": "_Let \\(a\\), \\(b\\), \\(c\\), and \\(d\\) be real numbers, not all zero. Find the eigenvalues of the following 4\\(\\times\\)4 matrix and describe the eigenspace decomposition of \\(\\mathbb{R}^{4}\\):_\n\n\\[\\left(\\begin{array}{cccc}aa&ab&ac&ad\\\\ ba&bb&bc&bd\\\\ ca&cb&cc&cd\\\\ da&db&dc&dd\\end{array}\\right).\\]",
        "Solution": null
    },
    "Problem 7.5.19": {
        "Problem": "_Show that the following three conditions are all equivalent for a real 3\\(\\times\\)3 symmetric matrix \\(A\\), whose eigenvalues are \\(a\\), \\(b\\), and \\(c\\):_\n\n1. \\(\\mathrm{tr}A\\) _is not an eigenvalue of_ \\(A\\)_._\n2. \\((a+b)(b+c)(a+c)\\neq 0\\)_._\n3. _The map_ \\(L:S\\to S\\) _is an isomorphism, where_ \\(S\\) _is the space of 3_\\(\\times\\)_3 real skew-symmetric matrices and_ \\(L(W)=AW+WA\\)_._",
        "Solution": null
    },
    "Problem 7.5.20": {
        "Problem": "_Let_\n\n\\[A=\\left(\\begin{array}{cc}a&b\\\\ c&d\\end{array}\\right)\\]\n\n_be a real matrix with \\(a,b,c,d>0\\). Show that \\(A\\) has an eigenvector_\n\n\\[\\left(\\begin{array}{c}x\\\\ y\\end{array}\\right)\\in\\mathbb{R}^{2}\\]\n\n_with \\(x,y>0\\)._",
        "Solution": "The characteristic polynomial of \\(A\\) is\n\n\\[\\chi\\_{A}(t)=t^{2}-(a+d)t+(ad-bc).\\]\n\nwhich has roots\n\n\\[t=\\frac{1}{2}(a+d)\\pm\\frac{1}{2}\\sqrt{(a-d)^{2}+4bc}=\\frac{1}{2}\\left(a+d\\pm \\sqrt{\\Delta}\\right).\\]\\(\\Delta\\) is positive, so \\(A\\) has real eigenvalues. Let \\(\\lambda=\\frac{1}{2}\\left(a+d+\\sqrt{\\Delta}\\right)\\) and let \\(v=(x,y)\\) be an eigenvector associated with this eigenvalue with \\(x>0\\). Expanding the first entry of \\(Av\\), we get\n\n\\[ax+by=\\frac{1}{2}\\left(a+d+\\sqrt{\\Delta}\\right)x\\]\n\nor\n\n\\[2by=\\left(d-a+\\sqrt{\\Delta}\\right)x.\\]\n\nSince \\(b>0\\), to see that \\(y>0\\) it suffices to show that \\(d-a+\\sqrt{\\Delta}>0\\), or \\(\\sqrt{\\Delta}>a-d\\). But this is immediate from the definition of \\(\\sqrt{\\Delta}\\) and we are done."
    },
    "Problem 7.5.21": {
        "Problem": "_Let \\(n\\) be a positive integer, and let \\(A=(a_{ij})_{i,j=1}^{n}\\) be the n\\(\\times\\)n matrix with \\(a_{ii}=2,\\)\\(a*{i\\,i\\pm 1}=-1\\), and \\(a*{ij}=0\\) otherwise; that is,\\_\n\n\\[A=\\left(\\begin{array}{cccccccc}2&-1&0&0&\\cdots&0&0&0\\\\ -1&2&-1&0&\\cdots&0&0&0\\\\ 0&-1&2&-1&\\cdots&0&0&0\\\\ 0&0&-1&2&\\cdots&0&0&0\\\\ \\vdots&\\vdots&\\vdots&\\vdots&\\ddots&\\vdots&\\vdots&\\vdots\\\\ 0&0&0&0&\\cdots&2&-1&0\\\\ 0&0&0&\\cdots&-1&2&-1\\\\ 0&0&0&0&\\cdots&0&-1&2\\end{array}\\right).\\]\n\n_Prove that every eigenvalue of \\(A\\) is a positive real number._",
        "Solution": "It suffices to show that \\(A\\) is positive definite. Let \\(x=(x*{1},\\ldots,x*{n})\\), we have\n\n\\[\\langle Ax,x\\rangle =2x*{1}^{2}-x*{1}x*{2}-x*{1}x*{2}+2x*{2}^{2}-x*{2}x*{3}-\\cdots-x* {n-1}x*{n}+2x*{n}^{2}\\] \\[=x*{1}^{2}+(x*{1}-x*{2})^{2}+(x*{2}-x*{3})^{2}+\\cdots+(x*{n-1}-x *{n})^{2}+x\\_{n}^{2}.\\]\n\nThus, for all nonzero \\(x\\), \\(\\langle Ax,x\\rangle\\geq 0\\). In fact, it is strictly positive, since one of the center terms is greater than \\(0\\), or \\(x*{1}=x*{2}=\\cdots=x*{n}\\) and all the \\(x*{i}\\)'s are nonzero, so \\(x\\_{1}^{2}>0\\). Hence, \\(A\\) is positive definite and we are done.\n\n_Solution 2._ Since \\(A\\) is symmetric, all eigenvalues are real. Let \\(x=(x*{i})*{1}^{n}\\) be an eigenvector with eigenvalue \\(\\lambda\\). Since \\(x\\neq 0\\), we have \\(\\max*{i}|x*{i}|>0\\). Let \\(k\\) be the least \\(i\\) with \\(|x*{i}|\\) maximum. Replacing \\(x\\) by \\(-x\\), if necessary, we may assume \\(x*{k}>0\\). We have\n\n\\[\\lambda x*{k}=-x*{k-1}+2x*{k}-x*{k+1}\\]\n\nwhere nonexistent terms are taken to be zero. By the choice of \\(x*{k}\\), we have \\(x*{k-1}<x*{k}\\) and \\(x*{k+1}\\leq x*{k}\\), so we get \\(\\lambda x*{k}>0\\) and \\(\\lambda>0\\)."
    },
    "Problem 7.5.22": {
        "Problem": "_Let \\(A\\) be a real symmetric \\(n\\times n\\) matrix with nonnegative entries. Prove that \\(A\\) has an eigenvector with nonnegative entries._",
        "Solution": "Let \\(\\lambda\\_{0}\\) be the largest eigenvalue of \\(A\\). We have\n\n\\[\\lambda\\_{0}=\\max\\left\\{\\langle Ax,x\\rangle\\,|\\,x\\in\\mathbb{R},\\,\\|x\\|=1\\right\\}\\,,\\]\n\nand the maximum it attained precisely when \\(x\\) is an eigenvector of \\(A\\) with eigenvalue \\(\\lambda\\_{0}\\). Suppose \\(v\\) is a unit vector for which the maximum is attained, and let \\(u\\) be the vector whose coordinates are the absolute values of the coordinates of \\(v\\). Since the entries of \\(A\\) are nonnegative, we have\n\n\\[\\langle Au,u\\rangle\\geq\\langle Av,v\\rangle=\\lambda\\_{0},\\]\n\nimplying that \\(\\langle Au,u\\rangle=\\lambda*{0}\\) and so that \\(u\\) is an eigenvector of \\(A\\) for the eigenvalue \\(\\lambda*{0}\\)."
    },
    "Problem 7.5.23": {
        "Problem": "_Let \\(A=(a_{ij})_{i,j=1}^{n}\\) be a real \\(n\\times n\\) matrix with nonnegative entries such that_\n\n\\[\\sum*{j=1}^{n}a*{ij}=1\\qquad\\qquad(1\\leq i\\leq n).\\]\n\n_Prove that no eigenvalue of \\(A\\) has an absolute value greater than \\(1\\)._",
        "Solution": "Let \\(\\lambda\\) be an eigenvalue of \\(A\\) and \\(x=(x*{1},\\ldots,x*{n})^{t}\\) a corresponding eigenvector. Let \\(x\\_{i}\\) be the entry of \\(x\\) whose absolute value is greatest. We have\n\n\\[\\lambda x*{i}=\\sum*{j=1}^{n}a*{ij}x*{j}\\]\n\nso\n\n\\[|\\lambda||x*{i}|\\leq\\sum*{j=1}^{n}a*{ij}|x*{j}|\\leq|x*{j}|\\sum*{j=1}^{n}a*{ij} =|x*{i}|.\\]\n\nHence, \\(|\\lambda|\\leq 1\\)."
    },
    "Problem 7.5.25": {
        "Problem": "_Let \\(k\\) be real, \\(n\\) an integer \\(\\geq 2\\), and let \\(A=(a_{ij})\\) be the \\(n\\times n\\) matrix such that all diagonal entries \\(a*{ii}=k\\), all entries \\(a*{i\\,i\\pm 1}\\) immediately above or below the diagonal equal \\(1\\), and all other entries equal \\(0\\). For example, if \\(n=5\\),\\_\n\n\\[A=\\left(\\begin{array}{ccccc}k&1&0&0&0\\\\ 1&k&1&0&0\\\\ 0&1&k&1&0\\\\ 0&0&1&k&1\\\\ 0&0&0&1&k\\end{array}\\right)\\]\n\n_Let \\(\\lambda_{min}\\) and \\(\\lambda*{max}\\) denote the smallest and largest eigenvalues of \\(A\\), respectively. Show that \\(\\lambda*{min}\\leq k-1\\) and \\(\\lambda*{max}\\geq k+1\\).*",
        "Solution": "Let \\(v=(1,1,0,\\ldots,0)\\). A calculation shows that \\(Av=(k+1,k+1,1,0,\\ldots,0)\\), so\n\n\\[\\frac{\\langle Av,v\\rangle}{\\langle v,v\\rangle}=k+1.\\]\n\nSimilarly, for \\(u=(1,-1,0,\\ldots,0)\\), we have \\(Au=(k-1,1-k,-1,0,\\ldots,0)\\) and so\n\n\\[\\frac{\\langle Au,u\\rangle}{\\langle u,u\\rangle}=k-1.\\]\n\nBy Rayleigh's Theorem [19, pag. 418], we know that\n\n\\[\\lambda*{\\min}\\leq\\frac{\\langle Av,v\\rangle}{\\langle v,v\\rangle}\\leq\\lambda*{ \\max}.\\]for all nonzero vectors \\(v\\), and the desired conclusion follows."
    },
    "Problem 7.5.26": {
        "Problem": "_Let \\(A\\) and \\(B\\) be real n\\(\\times\\)n symmetric matrices with \\(B\\) positive definite. Consider the function defined for \\(x\\neq 0\\) by_\n\n\\[G(x)=\\frac{\\langle Ax,x\\rangle}{\\langle Bx,x\\rangle}.\\]1. _Show that_ \\(G\\) _attains its maximum value._ 2. _Show that any maximum point_ \\(U\\) _for_ \\(G\\) _is an eigenvector for a certain matrix related to_ \\(A\\) _and_ \\(B\\) _and show which matrix._",
        "Solution": "As \\(B\\) is positive definite, there is an invertible matrix \\(C\\) such that \\(B=C^{t}C\\), so\n\n\\[\\frac{\\langle Ax,x\\rangle}{\\langle Bx,x\\rangle}=\\frac{\\langle Ax,x\\rangle}{ \\langle C^{t}Cx,x\\rangle}=\\frac{\\langle Ax,x\\rangle}{\\langle Cx,Cx\\rangle}.\\]\n\nLet \\(Cx=y\\). The right-hand side equals\n\n\\[\\frac{\\langle AC^{-1}y,C^{-1}y\\rangle}{\\langle y,y\\rangle}=\\frac{\\langle(C^{-1 })^{t}AC^{-1}y,y\\rangle}{\\langle y,y\\rangle}.\\]\n\nSince the matrix \\(\\left(C^{-1}\\right)^{t}AC^{-1}\\) is symmetric, by Rayleigh's Theorem [18, pag. 418], the right-hand side is bounded by \\(\\lambda\\), where \\(\\lambda\\) is the largest eigenvalue of \\(\\left(C^{-1}\\right)^{t}AC^{-1}\\). Further, the maximum is attained at the associated eigenvector. Let \\(y*{0}\\) be such an eigenvector. Then \\(G(x)\\) attains its maximum at \\(x=C^{-1}y*{0}\\), which is an eigenvector of the matrix \\(\\left(C^{-1}\\right)^{t}A\\)."
    },
    "Problem 7.5.27": {
        "Problem": "_Let \\(A\\) be a real symmetric \\(n\\times n\\) matrix that is positive definite. Let \\(y\\in\\mathbb{R}^{n}\\), \\(y\\neq 0\\). Prove that the limit_\n\n\\[\\lim\\_{m\\to\\infty}\\frac{y^{t}A^{m+1}y}{y^{t}A^{m}y}\\]\n\n_exists and is an eigenvalue of \\(A\\)._\n\n### 7.6 Canonical Forms",
        "Solution": "Let \\(y\\neq 0\\) in \\(\\mathbb{R}^{n}\\). \\(A\\) is real symmetric, so there is an orthogonal matrix, \\(P\\), such that \\(B=P^{t}AP\\) is diagonal. Since \\(P\\) is invertible, there is a nonzero vector \\(z\\) such that \\(y=Pz\\). Therefore,\n\n\\[\\frac{\\langle A^{m+1}y,y\\rangle}{\\langle A^{m}y,y\\rangle}=\\frac{\\langle A^{m+ 1}Pz,Pz\\rangle}{\\langle A^{m}Pz,Pz\\rangle}=\\frac{\\langle P^{t}A^{m+1}Pz,z \\rangle}{\\langle P^{t}A^{n}Pz,z\\rangle}=\\frac{\\langle B^{m+1}z,z\\rangle}{ \\langle B^{m}z,z\\rangle}.\\]\n\nSince \\(A\\) is positive definite, we may assume without loss of generality that \\(B\\) has the form\n\n\\[\\left(\\begin{array}{cccc}\\lambda*{1}&0&\\cdots&0\\\\ 0&\\lambda*{2}&\\cdots&0\\\\ \\vdots&\\vdots&\\ddots&\\vdots\\\\ 0&0&\\cdots&\\lambda\\_{n}\\end{array}\\right)\\]\n\nwhere \\(\\lambda*{1}\\geq\\lambda*{2}\\geq\\cdots\\geq\\lambda*{n}>0\\). Let \\(z=(z*{1},\\ldots,z*{n})\\neq 0\\), and \\(i\\leq n\\) be such that \\(z*{i}\\) is the first nonzero coordinate of \\(z\\). Then\n\n\\[\\frac{\\langle B^{m+1}z,z\\rangle}{\\langle B^{m}z,z\\rangle} = \\frac{\\lambda*{i}^{m+1}z*{i}^{2}+\\cdots+\\lambda*{n}^{m+1}z*{n}^{2}} {\\lambda*{i}^{m}z*{i}^{2}+\\cdots+\\lambda*{n}^{m}z*{n}^{2}}\\] \\[= \\lambda*{i}\\left(\\frac{z*{i}^{2}+(\\lambda*{i+1}/\\lambda*{i})^{m+1 }z*{i+1}^{2}+\\cdots+(\\lambda*{n}/\\lambda*{i})^{m+1}z*{n}^{2}}{z*{i}^{2}+( \\lambda*{i+1}/\\lambda*{i})^{m}z*{i+1}^{2}+\\cdots+(\\lambda*{n}/\\lambda*{1})^{m} z*{n}^{2}}\\right)\\] \\[\\sim \\lambda*{i}\\qquad(m\\to\\infty).\\]\n\n### 7.6 Canonical Forms"
    },
    "Problem 7.6.2": {
        "Problem": "_Prove, or supply a counterexample: If \\(A\\) is an invertible \\(n\\times n\\) complex matrix and some power of \\(A\\) is diagonal, then \\(A\\) can be diagonalized._",
        "Solution": "Assume \\(A^{m}\\) is diagonalizable. Then its minimal polynomial, \\(\\mu\\_{A^{m}}(x)\\), has no repeated roots, that is,\n\n\\[\\mu*{A^{m}}(x)=(x-a*{1})\\cdots(x-a\\_{k})\\]\n\nwhere \\(a*{i}\\neq a*{j}\\) for \\(i\\neq j\\).\n\nThe matrix \\(A^{m}\\) satisfies the equation\n\n\\[(A^{m}-a*{1}I)\\cdots(A^{m}-a*{k}I)=0\\]\n\nso \\(A\\) is a root of the polynomial \\((x^{m}-a*{1})\\cdots(x^{m}-a*{k})\\), therefore, \\(\\mu*{A}(x)\\) divides this polynomial. To show that \\(A\\) is diagonalizable, it is enough to show this polynomial has no repeated roots, which is clear, because the roots of the factors \\(x^{m}-a*{i}\\) are different, and different factors have different roots.\n\nThis proves more than what was asked; it shows that if \\(A\\) is an invertible linear transformation on a finite dimensional vector space over a field \\(\\mathbf{F}\\) of characteristic not dividing \\(n\\), the characteristic polynomial of \\(A\\) factors completely over \\(\\mathbf{F}\\), and if \\(A^{n}\\) is diagonalizable, then \\(A\\) is diagonalizable.\n\nOn this footing, we can rewrite the above proof as follows: We may suppose that the vector space \\(V\\) has positive dimension \\(m\\). Let \\(\\lambda\\) be an eigenvalue of \\(A\\). Then \\(\\lambda\\neq 0\\). We may replace \\(V\\) by the largest subspace of \\(V\\) on which \\(A-\\lambda I\\) is nilpotent, so that we may suppose the characteristic polynomial of \\(A\\) is \\((x-\\lambda)^{m}\\). Since \\(A^{n}\\) is diagonalizable, we must have \\(A^{n}=\\lambda^{n}I\\) since \\(\\lambda^{n}\\) is the only eigenvalue of \\(A^{n}\\). Thus, \\(A\\) satisfies the equation \\(x^{n}-\\lambda^{n}=0\\). Since the only common factor of \\(x^{n}-\\lambda^{n}\\) and \\((x-\\lambda)^{n}\\) is \\(x-\\lambda\\), and as the characteristic of \\(\\mathbf{F}\\) does not divide \\(n\\), \\(A=\\lambda I\\) and, hence, is diagonal."
    },
    "Problem 7.6.3": {
        "Problem": "_Let_\n\n\\[A=\\left(\\begin{array}{rrr}2&-1&0\\\\ -1&2&-1\\\\ 0&-1&2\\end{array}\\right)\\.\\]\n\n_Show that every real matrix \\(B\\) such that \\(AB=BA\\) has the form_\n\n\\[B=aI+bA+cA^{2}\\]\n\n_for some real numbers \\(a\\), \\(b\\), and \\(c\\)._",
        "Solution": null
    },
    "Problem 7.6.4": {
        "Problem": "_Let_\n\n\\[A=\\left(\\begin{array}{rrr}1&2\\\\ 1&-1\\end{array}\\right).\\]\n\n_Express \\(A^{-1}\\) as a polynomial in \\(A\\) with real coefficients._",
        "Solution": "The characteristic polynomial of \\(A\\) is \\(\\chi\\_{A}(x)=x^{2}-3\\), so \\(A^{2}=3I\\), and multiplying both sides by \\(A^{-1}\\), we have\n\n\\[A^{-1}=\\frac{1}{3}A.\\]"
    },
    "Problem 7.6.5": {
        "Problem": "_For \\(x\\in\\mathbb{R}\\), let_\n\n\\[A*{x}=\\left(\\begin{array}{rrr}x&1&1&1\\\\ 1&x&1&1\\\\ 1&1&x&1\\\\ 1&1&1&x\\end{array}\\right).\\]1. \\_Prove that* \\(\\det(A*{x})=(x-1)^{3}(x+3)\\)*._ 2. \\_Prove that if_ \\(x\\neq 1,-3\\)_, then_ \\(A*{x}^{-1}=-(x-1)^{-1}(x+3)^{-1}A*{-x-2}\\)_._",
        "Solution": "1. Subtracting the second line from the other three and expanding along the first line, we have\n\n\\[\\det A*{x} =(x-1)\\left|\\begin{array}{ccc}x&1&1\\\\ 1-x&x-1&0\\\\ 1-x&0&x-1\\end{array}\\right|+(x-1)\\left|\\begin{array}{ccc}1&1&1\\\\ 0&x-1&0\\\\ 0&0&x-1\\end{array}\\right|\\] \\[=(x-1)^{3}(x+3).\\]2. Suppose now that \\(x\\neq 1\\) and \\(-3\\). Then \\(A*{x}\\) is invertible and the characteristic polynomial is given by:\n\n\\[\\chi*{A*{x}}(t) =\\left|\\begin{array}{cccc}t-x&-1&-1&-1\\\\ -1&t-x&-1&-1\\\\ -1&-1&t-x&-1\\\\ -1&-1&-1&t-x\\end{array}\\right|=\\left|\\begin{array}{cccc}x-t&1&1&1\\\\ 1&x-t&1&1\\\\ 1&1&x-t&1\\\\ 1&1&1&x-t\\end{array}\\right|\\] \\[=(x-t-1)^{3}(x-t+3)\\]\n\nNow an easy substitution shows that the minimal polynomial is\n\n\\[\\mu*{A*{x}}(t)=(x-t-1)(x-t-3)\\]\n\nso substituting \\(t\\) by \\(A\\_{x}\\), we have\n\n\\[((x-1)I*{4}-A*{x})((x+3)I*{4}-A*{x}) =0\\] \\[(x-1)(x+3)I*{4}-2(x+1)A*{x}-A\\_{x}^{2} =0\\]\n\nmultiplying both sides by \\(A\\_{x}^{-1}\\),\n\n\\[(x-1)(x+3)A*{x}^{-1} =2(x+1)I*{4}-A*{x}\\] \\[=-A*{-x-2}\\]\n\nso\n\n\\[A*{x}^{-1}=-(x-1)^{-1}(x+3)^{-1}A*{-x-2}.\\]\n\nSolution to 7.6.6:The characteristic polynomial of \\(A\\) is \\(\\chi*{A}(t)=t^{3}-8t^{2}-20t-16=(t-4)(t-2)^{2}\\) and the minimal polynomial is \\(\\mu*{A}(t)=(t-2)(t-4)\\). By the Euclidean Algorithm [1, pag. 155], there is a polynomial \\(p(t)\\) and constants \\(a\\) and \\(b\\) such that\n\n\\[t^{10}=p(t)\\mu\\_{A}(t)+at+b.\\]\n\nSubstituting \\(t=2\\) and \\(t=4\\) and solving for \\(a\\) and \\(b\\) yields \\(a=2^{9}(2^{10}-1)\\) and \\(b=-2^{11}(2^{9}-1)\\). Therefore, since \\(A\\) is a root of its minimal polynomial,\n\n\\[A^{10}=aA+bI=\\left(\\begin{array}{cccc}3a+b&a&a\\\\ 2a&4a+b&2a\\\\ -a&-a&a+b\\end{array}\\right).\\]\n\nSolution to 7.6.7:The characteristic polynomial of \\(A\\) is \\(\\chi\\_{A}(t)=t^{2}-2t+1=(t-1)^{2}\\). By the Euclidean Algorithm [1, pag. 155], there is a polynomial \\(q(t)\\) and constants \\(a\\) and \\(b\\) such that \\(t^{100}=q(t)(t-1)^{2}+at+b\\).\n\nDifferentiating both sides of this equation, we get \\(100t^{99}=q^{\\prime}(t)(t-1)^{2}+2q(t)(t-1)+a\\). Substituting \\(t=1\\) into each equation and solving for \\(a\\) and \\(b\\), we get \\(a=100\\) and \\(b=-99\\). Therefore, since \\(A\\) satisfies its characteristic equation, substituting it into the first equation yields \\(A^{100}=100A-99I\\), or\n\n\\[A^{100}=\\left(\\begin{array}{cc}51&50\\\\ -50&-49\\end{array}\\right).\\]\n\nAn identical calculation shows that \\(A^{7}=7A-6I\\), so\n\n\\[A^{7}=\\left(\\begin{array}{cc}9/2&7/2\\\\ -7/2&-5/2\\end{array}\\right).\\]\n\nFrom this it follows immediately that\n\n\\[A^{-7}=\\left(\\begin{array}{cc}-5/2&-7/2\\\\ 7/2&9/2\\end{array}\\right).\\]"
    },
    "Problem 7.6.6": {
        "Problem": "_Compute \\(A^{10}\\) for the matrix_\n\n\\[A=\\left(\\begin{array}{rr}3&1&1\\\\ 2&4&2\\\\ -1&-1&1\\end{array}\\right).\\]",
        "Solution": null
    },
    "Problem 7.6.7": {
        "Problem": "_Calculate \\(A^{100}\\) and \\(A^{-7}\\), where_\n\n\\[A=\\left(\\begin{array}{rr}3/2&1/2\\\\ -1/2&1/2\\end{array}\\right).\\]",
        "Solution": null
    },
    "Problem 7.6.8": {
        "Problem": "_Prove or disprove: For any 2\\(\\times\\)2 matrix \\(A\\) over \\(\\mathbb{C}\\), there is a 2\\(\\times\\)2 matrix \\(B\\) such that \\(A=B^{2}\\)._",
        "Solution": "Counterexample: Let\n\n\\[A=\\left(\\begin{array}{cc}0&1\\\\ 0&0\\end{array}\\right)=B^{2}=\\left(\\begin{array}{cc}a&b\\\\ c&d\\end{array}\\right)\\left(\\begin{array}{cc}a&b\\\\ c&d\\end{array}\\right)=\\left(\\begin{array}{cc}a^{2}+bc&ab+bd\\\\ ca+dc&cb+d^{2}\\end{array}\\right).\\]\n\nEquating entries, we find that \\(c(a+d)=0\\) and \\(b(a+d)=1\\), so \\(b\\neq 0\\) and \\(a+d\\neq 0\\). Thus, \\(c=0\\). The vanishing of the diagonal entries of \\(B^{2}\\) then implies that \\(a^{2}=d^{2}=0\\) and, thus, \\(a+d=0\\). This contradiction proves that no such \\(B\\) can exist, so \\(A\\) has no square root.\n\n_Solution 2_. Let\n\n\\[A=\\left(\\begin{array}{cc}0&1\\\\ 0&0\\end{array}\\right).\\]\n\nAny square root \\(B\\) of \\(A\\) must have zero eigenvalues, and since it cannot be the zero matrix, it must have Jordan Canonical Form [11, pag. 247]\\(JBJ^{-1}=A\\). But then \\(B^{2}=J^{-1}A^{2}J=0\\) since \\(A^{2}=0\\), so no such \\(B\\) can exist."
    },
    "Problem 7.6.9": {
        "Problem": "1. _Show that a real 2\\(\\times\\)2 matrix_ \\(A\\) _satisfies_ \\(A^{2}=-I\\) _if and only if_ \\[A=\\left(\\begin{array}{rr}\\pm\\sqrt{pq-1}&-p\\\\ q&\\mp\\sqrt{pq-1}\\end{array}\\right)\\] _where_ \\(p\\) _and_ \\(q\\) _are real numbers such that_ \\(pq\\geq 1\\) _and both upper or both lower signs should be chosen in the double signs._\n2. _Show that there is no real 2\\(\\times\\)2 matrix_ \\(A\\) _such that_ \\[A^{2}=\\left(\\begin{array}{rr}-1&0\\\\ 0&-1-\\varepsilon\\end{array}\\right)\\] _with_ \\(\\varepsilon>0\\)_._",
        "Solution": "1. Let\n\n\\[A=\\left(\\begin{array}{cc}a&b\\\\ c&d\\end{array}\\right)\\]\n\nthen\n\n\\[A^{2}=\\left(\\begin{array}{cc}a^{2}+bc&(a+d)b\\\\ (a+d)c&bc+d^{2}\\end{array}\\right).\\]\n\nTherefore, \\(A^{2}=-I\\) is equivalent to the system\n\n\\[\\left\\{\\begin{array}{ccc}a^{2}+bc&=&-1\\\\ (a+d)b&=&0\\\\ (a+d)c&=&0\\\\ bc+d^{2}&=&-1\\end{array}\\right.\\]if \\(a+d\\neq 0\\), the second equation above gives \\(b=0\\), and from the fourth, we obtain \\(d^{2}=-1\\), which is absurd. We must then have \\(a=-d\\) and the result follows.\n\n2. The system that we get in this case is\n\n\\[\\left\\{\\begin{array}{ccc}a^{2}+bc&=&-1\\\\ (a+d)b&=&0\\\\ (a+d)c&=&0\\\\ bc+d^{2}&=&-1-\\varepsilon\\end{array}\\right.\\]\n\nAs above, we cannot have \\(a\\neq-d\\). But combining \\(a=-d\\) with the first and fourth equations of the system, we get \\(\\varepsilon=0\\), a contradiction. Therefore, no such matrix exists."
    },
    "Problem 7.6.10": {
        "Problem": "_Is there a real 2\\(\\times\\)2 matrix \\(A\\) such that_\n\n\\[A^{20}=\\left(\\begin{array}{rr}-1&0\\\\ 0&-1-\\varepsilon\\end{array}\\right)\\,?\\]\n\n_Exhibit such an \\(A\\) or prove there is none._",
        "Solution": "Suppose such a matrix \\(A\\) exists. One of the cingenvalues of \\(A\\) would be \\(w\\) and the other \\((1+\\varepsilon)^{1/20}w\\) where \\(w\\) is a twentieth root of \\(-1\\). From the fact that \\(A\\) is real we can see that both eigenvalues are real or form a complex conjugate pair, but neither can occur because none the twentieth root of \\(-1\\) are real and the fact that\n\n\\[|w|=1\\neq(1+\\varepsilon)^{1/20}\\]\n\nmake it impossible for them to be a conjugate pair, so no such a matrix exist."
    },
    "Problem 7.6.11": {
        "Problem": "_For which positive integers \\(n\\) is there a 2\\(\\times\\)2 matrix_\n\n\\[A=\\left(\\begin{array}{rr}a&b\\\\ c&d\\end{array}\\right)\\]\n\n_with integer entries and order \\(n\\); that is, \\(A^{n}=I\\) but \\(A^{k}\\neq I\\) for \\(0<k<n\\)?_\n\n_See also Problem 7.7.9._",
        "Solution": "\\(A^{n}=I\\) implies that the minimal polynomial of \\(A\\), \\(\\mu(x)\\in\\mathbb{Z}[x]\\), satisfies \\(\\mu(x)|(x^{n}-1)\\). Let \\(\\zeta*{1},\\ldots,\\zeta*{n}\\) be the distinct roots of \\(x^{n}-1\\) in \\(\\mathbb{C}\\). We will separate the two possible cases for the degree of \\(\\mu\\):\n\n- \\(\\deg\\mu=1\\). We have \\(\\mu(x)=x-1\\) and \\(A=I\\), or \\(\\mu(x)=x+1\\) and \\(A=-I\\), \\(A^{2}=I\\).\n- \\(\\deg\\mu=2\\). \\(\\zeta*{i}\\) and \\(\\zeta*{j}\\) are roots of \\(\\mu\\) for some \\(i\\neq j\\), in which case \\(\\zeta*{j}=\\overline{\\zeta*{i}}=\\zeta\\) say, since \\(\\mu\\) has real coefficients. Thus, \\(\\mu(x)=(x-\\zeta)\\left(x-\\overline{\\zeta}\\right)=x^{2}-2\\Re(\\zeta)x+1\\). In particular, \\(2\\Re(\\zeta)\\in\\mathbb{Z}\\), so the possibilities are \\(\\Re(\\zeta)=0\\), \\(\\pm 1/2\\), and \\(\\pm 1\\). We cannot have \\(\\Re(\\zeta)=\\pm 1\\) because the corresponding polynomials, \\((x-1)^{2}\\) and \\((x+1)^{2}\\), have repeated roots, so they are not divisors of \\(x^{n}-1\\). \\[\\Re(\\zeta)=0\\]. We have \\[\\mu(x)=x^{2}+1\\] and \\[A^{2}=-I\\], \\[A^{4}=I\\]. \\[\\Re(\\zeta)=1/2\\]. In this case \\[\\mu(x)=x^{2}-x+1\\]. \\[\\zeta\\] is a primitive sixth root of unity, so \\[A^{6}=I\\]. \\[\\Re(\\zeta)=-1/2\\]. We have \\[\\mu(x)=x^{2}+x+1\\]. \\[\\zeta\\] is a primitive third root of unity, so \\[A^{3}=I\\].\n\nFrom the above, we see that if \\(A^{n}=I\\) for some \\(n\\in\\mathbb{Z}\\_{+}\\), then one of the following holds:\n\n\\[A=I,\\ A^{2}=I,\\ A^{3}=I,\\ A^{4}=I,\\ A^{6}=I.\\]Further, for each \\(n=2\\), \\(3\\), \\(4\\), and \\(6\\) there is a matrix \\(A\\) such that \\(A^{n}=I\\) but \\(A^{k}\\neq I\\) for \\(0<k<n\\):\n\n- \\(n=2\\). \\[\\left(\\begin{array}{cc}1&0\\\\ 0&1\\end{array}\\right)\\]\n- \\(n=3\\). \\[\\left(\\begin{array}{cc}0&1\\\\ -1&-1\\end{array}\\right)\\]\n- \\(n=4\\). \\[\\left(\\begin{array}{cc}0&1\\\\ -1&0\\end{array}\\right)\\]\n- \\(n=6\\). \\[\\left(\\begin{array}{cc}0&1\\\\ -1&1\\end{array}\\right)\\]"
    },
    "Problem 7.6.12": {
        "Problem": "_Find a square root of the matrix_\n\n\\[\\left(\\begin{array}{ccc}1&3&-3\\\\ 0&4&5\\\\ 0&0&9\\end{array}\\right).\\]\n\n_How many square roots does this matrix have?_",
        "Solution": "Since \\(A\\) is upper-triangular, its eigenvalues are its diagonal entries, that is, \\(1\\), \\(4\\), and \\(9\\). It can, thus, be diagonalized, and in, fact, we will have\n\n\\[S^{-1}AS=\\left(\\begin{array}{ccc}1&0&0\\\\ 0&4&0\\\\ 0&0&9\\end{array}\\right)\\]\n\nwhere \\(S\\) is a matrix whose columns are eigenvectors of \\(A\\) for the respective eigenvalues \\(1\\), \\(4\\), and \\(9\\). The matrix\n\n\\[B=S\\left(\\begin{array}{ccc}1&0&0\\\\ 0&2&0\\\\ 0&0&3\\end{array}\\right)S^{-1}\\]\n\nwill then be a square root of \\(A\\).\n\nCarrying out the computations, one obtains\n\n\\[S=\\left(\\begin{array}{ccc}1&1&1\\\\ 0&1&1\\\\ 0&0&1\\end{array}\\right)\\quad\\mbox{and}\\quad S^{-1}=\\left(\\begin{array}{ccc} 1&-1&0\\\\ 0&1&-1\\\\ 0&0&1\\end{array}\\right)\\]\n\ngiving\n\n\\[B=\\left(\\begin{array}{ccc}1&1&-1\\\\ 0&2&1\\\\ 0&0&3\\end{array}\\right).\\]\n\nThe number of square roots of \\(A\\) is the same as the number of square roots of its diagonalization, \\(D=S^{-1}AS\\). Any matrix commuting with preserves its eigenspaces and so is diagonal. In particular, any square root of \\(D\\) is diagonal. Hence, \\(D\\) has exactly eight square roots, namely\n\n\\[\\sqrt{\\left(\\begin{array}{ccc}1&0&0\\\\ 0&4&0\\\\ 0&0&9\\end{array}\\right)}=\\left(\\begin{array}{ccc}\\pm 1&0&0\\\\ 0&\\pm 4&0\\\\ 0&0&\\pm 9\\end{array}\\right).\\]"
    },
    "Problem 7.6.13": {
        "Problem": "_Let \\(A\\) denote the matrix_\n\n\\[\\left(\\begin{array}{cccc}0&0&0&1\\\\ 0&0&0&0\\\\ 0&0&0&0\\\\ 0&0&0&0\\end{array}\\right).\\]\n\n_For which positive integers \\(n\\) is there a complex 4\\(\\times\\)4 matrix \\(X\\) such that \\(X^{n}=A\\)?_",
        "Solution": "\\(n=1\\). There is the solution \\(X=A\\).\n\n\\(n=2\\). \\(A\\) is similar to the matrix\n\n\\[\\left(\\begin{array}{cccc}0&0&1&0\\\\ 0&0&0&0\\\\ 0&0&0&0\\\\ 0&0&0&0\\end{array}\\right)\\]\n\nunder the transformation that interchanges the third and fourth basis vectors and leaves the first and second basis vectors fixed. The latter matrix is the square of\n\n\\[\\left(\\begin{array}{cccc}0&1&0&0\\\\ 0&0&1&0\\\\ 0&0&0&0\\\\ 0&0&0&0\\end{array}\\right).\\]\n\nHence, \\(A\\) is the square of\n\n\\[\\left(\\begin{array}{cccc}0&1&0&0\\\\ 0&0&0&1\\\\ 0&0&0&0\\\\ 0&0&0&0\\end{array}\\right).\\]\n\n\\(n=3\\). The Jordan matrix [HK61, pag. 247]\n\n\\[X=\\left(\\begin{array}{cccc}0&1&0&0\\\\ 0&0&1&0\\\\ 0&0&0&1\\\\ 0&0&0&0\\end{array}\\right)\\]\n\nis a solution.\n\n\\(n\\geq 4\\). If \\(X^{k}=A\\), then \\(X\\) is nilpotent since \\(A\\) is. Then the characteristic polynomial of \\(X\\) divides \\(x^{4}\\), so that \\(X^{4}=0\\), and, a fortiori, \\(X^{n}=0\\) for \\(n\\geq 4\\). There is, thus, no solution for \\(n\\geq 4\\)."
    },
    "Problem 7.6.14": {
        "Problem": "_Prove or disprove: There is a real \\(n\\times n\\) matrix \\(A\\) such that_\n\n\\[A^{2}+2A+5I=0\\]\n\n_if and only if \\(n\\) is even._",
        "Solution": "Suppose such a matrix \\(A\\) exists. Its minimal polynomial must divide \\(t^{2}+2t+5\\). However, this polynomial is irreducible over \\(\\mathbb{R}\\), so \\(\\mu*{A}(t)=t^{2}+2t+5\\). Since the characteristic and minimal polynomials have the same irreducible factors, \\(\\chi*{A}(t)=\\mu*{A}(t)^{k}\\). Therefore, \\(\\deg\\chi*{A}(t)=n\\) must be even.\n\nConversely, a calculation shows that the \\(2\\times 2\\) real matrix\n\n\\[A\\_{0}=\\left(\\begin{array}{cc}0&-5\\\\ 1&-2\\end{array}\\right)\\]\n\nis a root of this polynomial. Therefore, any \\(2n\\times 2n\\) block diagonal matrix which has \\(n\\) copies of \\(A\\_{0}\\) on the diagonal will satisfy this equation as well."
    },
    "Problem 7.6.15": {
        "Problem": "_Let \\(A\\) be an \\(n\\times n\\) Hermitian matrix satisfying the condition_\n\n\\[A^{5}+A^{3}+A=3I\\]\n\n_Show that \\(A=I\\)._",
        "Solution": "Let \\(p(t)=t^{5}+t^{3}+t-3\\). As \\(p(A)=0\\), we have \\(\\mu*{A}(t)|p(t)\\). However, since \\(A\\) is Hermitian, its minimal polynomial has only real roots. Taking the derivative of \\(p\\), we see that \\(p^{\\prime}(t)=5t^{4}+3t^{2}+1>0\\) for all \\(t\\), so \\(p(t)\\) has exactly one real root. A calculation shows that \\(p(1)=0\\), but \\(p^{\\prime}(1)\\neq 0\\). Therefore, \\(p(t)=(t-1)q(t)\\), where \\(q(t)\\) has only nonreal complex roots. It follows that \\(\\mu*{A}(t)|(t-1)\\). Since \\(t-1\\) is irreducible, \\(\\mu\\_{A}(t)=t-1\\) and \\(A=I\\)."
    },
    "Problem 7.6.16": {
        "Problem": "_Which of the following matrix equations have a real matrix solution \\(X\\)? (It is not necessary to exhibit solutions.)_\n\n_1._\n\n\\[X^{3}=\\left(\\begin{array}{ccc}0&0&0\\\\ 1&0&0\\\\ 2&3&0\\end{array}\\right),\\]\n\n_2._\n\n\\[2X^{5}+X=\\left(\\begin{array}{ccc}3&5&0\\\\ 5&1&9\\\\ 0&9&0\\end{array}\\right),\\]\n\n_3._\n\n\\[X^{6}+2X^{4}+10X=\\left(\\begin{array}{ccc}0&-1\\\\ 1&0\\end{array}\\right),\\]_._ 4. \\[X^{4}=\\left(\\begin{array}{ccc}3&4&0\\\\ 0&3&0\\\\ 0&0&-3\\end{array}\\right).\\]",
        "Solution": null
    },
    "Problem 7.6.17": {
        "Problem": "_Find a real matrix \\(B\\) such that_\n\n\\[B^{4}=\\left(\\begin{array}{ccc}2&0&0\\\\ 0&2&0\\\\ 0&-1&1\\end{array}\\right).\\]",
        "Solution": "Note that\n\n\\[A=\\left(\\begin{array}{ccc}2&0&0\\\\ 0&2&0\\\\ 0&-1&1\\end{array}\\right)\\]\n\ncan be decomposed into the two blocks (2) and \\(\\left(\\begin{smallmatrix}2&0\\\\ -1&1\\end{smallmatrix}\\right)\\), since the space spanned by \\(\\left(1\\,0\\,0\\right)^{t}\\) is invariant. We will find a \\(2\\times 2\\) matrix \\(C\\) such that \\(C^{4}=\\left(\\begin{smallmatrix}2&0\\\\ -1&1\\end{smallmatrix}\\right)=D\\), say.\n\nThe eigenvalues of \\(D\\) are \\(2\\) and \\(1\\), and the corresponding Lagrange Polynomials [10, pag. 286] are \\(p*{1}(x)=(x-2)/(1-2)=2-x\\) and \\(p*{2}(x)=(x-1)/(2-1)=x-1\\). Therefore, the spectral projection of \\(D\\) can be given by\n\n\\[P\\_{1}=-\\left(\\begin{array}{cc}2&0\\\\ -1&1\\end{array}\\right)+2\\left(\\begin{array}{cc}1&0\\\\ 0&1\\end{array}\\right)=\\left(\\begin{array}{cc}0&0\\\\ 1&1\\end{array}\\right)\\]\n\n\\[P\\_{2}=-\\left(\\begin{array}{cc}1&0\\\\ 0&1\\end{array}\\right)+\\left(\\begin{array}{cc}2&0\\\\ -1&1\\end{array}\\right)=\\left(\\begin{array}{cc}1&0\\\\ -1&0\\end{array}\\right)\\]\n\nWe have\n\n\\[D=\\left(\\begin{array}{cc}0&0\\\\ 1&1\\end{array}\\right)+2\\left(\\begin{array}{cc}1&0\\\\ -1&0\\end{array}\\right).\\]\n\nAs \\(P*{1}\\cdot P*{2}=P*{2}\\cdot P*{1}=0\\) and \\(P*{1}^{2}=P*{1}\\), \\(P*{2}^{2}=P*{2}\\), letting \\(C=\\left(\\begin{array}{cc}0&0\\\\ 1&1\\end{array}\\right)+2^{1/4}\\left(\\begin{array}{cc}1&0\\\\ -1&0\\end{array}\\right)=1P*{1}+2^{1/4}P*{2}\\), we get\n\n\\[C^{4}=P*{1}^{4}+\\underbrace{\\cdots}*{0}+(2^{1/4}P*{2})^{4}=P*{1}+2P\\_{2}=D.\\]Then\n\n\\[C=\\left(\\begin{array}{cc}2^{1/4}&0\\\\ 1-2^{1/4}&1\\end{array}\\right)\\]\n\nand \\(B\\) is\n\n\\[\\left(\\begin{array}{cc}2^{1/4}&0&0\\\\ 0&2^{1/4}&0\\\\ 0&1-2^{1/4}&1\\end{array}\\right).\\]"
    },
    "Problem 7.6.18": {
        "Problem": "_Let \\(V\\) be a finite-dimensional vector space and \\(T:V\\to V\\) a diagonalizable linear transformation. Let \\(W\\subset V\\) be a linear subspace which is mapped into itself by \\(T\\). Show that the restriction of \\(T\\) to \\(W\\) is diagonalizable._",
        "Solution": "It suffices to show that every element \\(w\\in W\\) is a sum of eigenvectors of \\(T\\) in \\(W\\). Let \\(a*{1},\\ldots,a*{n}\\) be the distinct eigenvalues of \\(T\\). We may write\n\n\\[w=v*{1}+\\cdots+v*{n}\\]\n\nwhere each \\(v*{i}\\) is in \\(V\\) and is an eigenvector of \\(T\\) with eigenvalue \\(a*{i}\\). Then\n\n\\[\\prod*{i\\neq j}(T-a*{j})w=\\prod*{i\\neq j}(a*{i}-a*{j})v*{i}.\\]\n\nThis element lies in \\(W\\) since \\(W\\) is \\(T\\) invariant. Hence, \\(v\\_{i}\\in W\\) for all \\(i\\) and the result follows.\n\n_Solution 2._ To see this in a matrix form, take an ordered basis of \\(W\\) and extend it to a basis of \\(V\\); on this basis, a matrix representing \\(T\\) will have the block form\n\n\\[[T]\\_{\\mathcal{B}}=\\left(\\begin{array}{cc}A&C\\\\ 0&B\\end{array}\\right)\\]\n\nbecause of the invariance of the subspace \\(W\\) with respect to \\(T\\).\n\nUsing the block structure of \\(T\\), we can see that the characteristic and minimal polynomials of \\(A\\) divide the ones for \\(T\\). For the characteristic polynomial, it is immediate from the fact that\n\n\\[\\det(xI-[T]\\_{\\mathcal{B}})=\\det(xI-A)\\,\\det(xI-B)\\]\n\nFor the minimal polynomial, observe that\n\n\\[[T]_{\\mathcal{B}}^{k}=\\left(\\begin{array}{cc}A^{k}&C_{k}\\\\ 0&B^{k}\\end{array}\\right)\\]\n\nwhere \\(C\\_{k}\\) is some \\(r\\times(n-r)\\) matrix. Therefore, any polynomial that annihilates \\([T]\\) also annihilates \\(A\\) and \\(B\\); so the minimal polynomial of \\(A\\) divides the one for \\([T]\\).\n\nNow, since \\(T\\) is diagonalizable, the minimal polynomial factors out in different linear terms and so does the one for \\(A\\), proving the result."
    },
    "Problem 7.6.19": {
        "Problem": "_Let \\(A\\) and \\(B\\) be diagonalizable linear transformations of \\(\\mathbb{R}^{n}\\) into itself such that \\(AB=BA\\). Let \\(E\\) be an eigenspace of \\(A\\). Prove that the restriction of \\(B\\) to \\(E\\) is diagonalizable._",
        "Solution": null
    },
    "Problem 7.6.21": {
        "Problem": "_Let \\(A\\) and \\(B\\) be \\(n\\times n\\) complex matrices. Prove or disprove each of the following statements:_\n\n1. _If_ \\(A\\) _and_ \\(B\\) _are diagonalizable, so is_ \\(A+B\\)_._\n2. _If_ \\(A\\) _and_ \\(B\\) _are diagonalizable, so is_ \\(AB\\)_._\n3. _If_ \\(A^{2}=A\\)_, then_ \\(A\\) _is diagonalizable._\n4. _If_ \\(A\\) _is invertible and_ \\(A^{2}\\) _is diagonalizable, then_ \\(A\\) _is diagonalizable._",
        "Solution": null
    },
    "Problem 7.6.22": {
        "Problem": "_Let_\n\n\\[A=\\left(\\begin{array}{ccc}7&15\\\\ -2&-4\\end{array}\\right).\\]\n\n_Find a real matrix \\(B\\) such that \\(B^{-1}AB\\) is diagonal._",
        "Solution": "The characteristic polynomial of \\(A\\) is\n\n\\[\\chi\\_{A}(x)=\\left|\\begin{array}{cc}x-7&-15\\\\ 2&x+4\\end{array}\\right|=(x-1)(x-2)\\]\n\nso \\(A\\) is diagonalizable and a short calculation shows that eigenvectors associated with the eigenvalues \\(1\\) and \\(2\\) are \\((5,-2)^{t}\\) and \\((3,-1)^{t}\\), so the matrix \\(B\\) is \\(\\binom{5\\ 3}{-2\\ -1}\\). Indeed, in this case, \\(B^{-1}AB=\\binom{1\\ 0}{0\\ 2}\\)."
    },
    "Problem 7.6.23": {
        "Problem": "_Let \\(A:\\mathbb{R}^{6}\\rightarrow\\mathbb{R}^{6}\\) be a linear transformation such that \\(A^{26}=I\\). Show that \\(\\mathbb{R}^{6}=V_{1}\\oplus V*{2}\\oplus V*{3}\\), where \\(V*{1}\\), \\(V*{2}\\), and \\(V*{3}\\) are two-dimensional invariant subspaces for \\(A\\).*",
        "Solution": null
    },
    "Problem 7.6.25": {
        "Problem": "_Find the eigenvalues, eigenvectors, and the Jordan Canonical Form of_\n\n\\[A=\\left(\\begin{array}{ccc}2&1&1\\\\ 1&2&1\\\\ 1&1&2\\end{array}\\right),\\]\n\n_considered as a matrix with entries in \\(\\mathbf{F}_{3}=\\mathbb{Z}/3\\mathbb{Z}\\).\\_",
        "Solution": null
    },
    "Problem 7.6.26": {
        "Problem": "_Let \\(A\\) be an \\(n\\times n\\) complex matrix, and let \\(\\chi\\) and \\(\\mu\\) be the characteristic and minimal polynomials of \\(A\\). Suppose that_\n\n\\[\\chi(x)=\\mu(x)(x-i),\\]\n\n\\[\\mu(x)^{2}=\\chi(x)(x^{2}+1).\\]\n\n_Determine the Jordan Canonical Form of \\(A\\)._",
        "Solution": "Combining the equations, we get \\(\\mu(x)^{2}=\\mu(x)(x-i)(x^{2}+1)\\) and, thus, \\(\\mu(x)=(x-i)^{2}(x+i)\\). So the Jordan blocks of the Jordan Canonical Form [11, pag. 247]\\(J\\_{A}\\), correspond to the eigenvalues \\(\\pm i\\). There is at least one block of size \\(2\\) corresponding to the eigenvalue \\(i\\) and no larger block corresponding to \\(i\\). Similarly, there is at least one block of size \\(1\\) corresponding to \\(-i\\). We have \\(\\chi(x)=(x-i)^{3}(x+i)\\), so \\(n=\\deg\\chi=4\\), and the remaining block is a block of size \\(1\\) corresponding to \\(i\\), since the total dimension of the eigenspace is the degree with which the factor appears in the characteristic polynomial. Therefore,\n\n\\[J\\_{A}=\\left(\\begin{array}{cccc}i&1&0&0\\\\ 0&i&0&0\\\\ 0&0&i&0\\\\ 0&0&0&-i\\end{array}\\right).\\]"
    },
    "Problem 7.6.28": {
        "Problem": "_Let \\(M_{2\\times 2}\\) denote the vector space of complex \\(2\\times 2\\) matrices. Let\\_\n\n\\[A=\\left(\\begin{array}{ccc}0&1\\\\ 0&0\\end{array}\\right)\\]\n\n_and let the linear transformation \\(T:M_{2\\times 2}\\to M*{2\\times 2}\\) be defined by \\(T(X)=XA-AX\\). Find the Jordan Canonical Form for \\(T\\).*",
        "Solution": "A computation gives\n\n\\[T\\left(\\begin{array}{cc}x*{11}&x*{12}\\\\ x*{21}&x*{22}\\end{array}\\right)=\\left(\\begin{array}{cc}-x*{21}&x*{11}-x*{22} \\\\ 0&x*{21}\\end{array}\\right).\\]\n\nIn particular, for the basis elements\n\n\\[E*{1}=\\left(\\begin{array}{cc}1&0\\\\ 0&0\\end{array}\\right),\\quad E*{2}=\\left(\\begin{array}{cc}0&1\\\\ 0&0\\end{array}\\right),\\quad E*{3}=\\left(\\begin{array}{cc}0&0\\\\ 1&0\\end{array}\\right),\\quad E*{4}=\\left(\\begin{array}{cc}0&0\\\\ 0&1\\end{array}\\right)\\]\n\nwe have\n\n\\[TE*{1}=\\left(\\begin{array}{cc}0&1\\\\ 0&0\\end{array}\\right)=E*{2},\\quad TE\\_{2}=0\\,,\\]\n\n\\[TE*{3}=\\left(\\begin{array}{cc}-1&0\\\\ 0&1\\end{array}\\right)=-E*{1}+E*{4},\\quad TE*{4}=\\left(\\begin{array}{cc}0&-1 \\\\ 0&0\\end{array}\\right)=-E\\_{2}.\\]\n\nThe matrix for \\(T\\) with respect to the basis \\(\\{E*{1},E*{2},E*{3},E*{4}\\}\\) is then\n\n\\[S=\\left(\\begin{array}{cccc}0&0&-1&0\\\\ 1&0&0&-1\\\\ 0&0&0&0\\\\ 0&0&1&0\\end{array}\\right).\\]\n\nA calculation shows that the characteristic polynomial of \\(S\\) is \\(\\lambda^{4}\\). Thus, \\(S\\) is nilpotent. Moreover, the index of nilpotency is \\(3\\), since we have\n\n\\[T^{2}E*{1}=T^{2}E*{2}=T^{2}E*{4}=0\\,,\\quad T^{2}E*{3}=-2E\\_{2}.\\]\n\nThe only \\(4\\times 4\\) nilpotent Jordan matrix [HK61, pag. 247] with index of nilpotency \\(3\\) is\n\n\\[\\left(\\begin{array}{cccc}0&0&0&0\\\\ 0&0&1&0\\\\ 0&0&0&1\\\\ 0&0&0&0\\end{array}\\right)\\]\n\nwhich is, therefore, the Jordan Canonical Form of \\(T\\). A basis in which \\(T\\) is represented by the preceding matrix is\n\n\\[\\left\\{E*{1}+E*{4}\\,,\\,E*{2}\\,,\\,\\frac{E*{1}-E*{4}}{2}\\,,\\,-\\frac{E*{3}}{2} \\right\\}.\\]"
    },
    "Problem 7.6.29": {
        "Problem": "_Find the Jordan Canonical Form of the matrix_\n\n\\[\\left(\\begin{array}{cccccc}1&0&0&0&0&0\\\\ 1&1&0&0&0&0\\\\ 1&0&1&0&0&0\\\\ 1&0&0&1&0&0\\\\ 1&0&0&0&1&0\\\\ 1&1&1&1&1&1\\end{array}\\right).\\]",
        "Solution": "A direct calculation shows that \\((A-I)^{3}=0\\) and this is the least positive exponent for which this is true. Hence, the minimal polynomial of \\(A\\) is \\(\\mu*{A}(t)=(t-1)^{3}\\). Thus, its characteristic polynomial must be \\(\\chi*{A}(t)=(t-1)^{6}\\). Therefore, the Jordan Canonical Form [11, pag. 247] of \\(A\\) must contain one \\(3\\times 3\\) Jordan block associated with 1. The number of blocks is the dimension of the eigenspace associated with 1. Letting \\(x=(x*{1},\\ldots,x*{6})^{t}\\) and solving \\(Ax=x\\), we get the two equations \\(x*{1}=0\\) and \\(x*{2}+x*{3}+x*{4}+x*{5}=0\\). Since \\(x*{6}\\) is not determined, these give four degrees of freedom, so the eigenspace has dimension 4. Therefore, the Jordan Canonical Form of \\(A\\) must contain four Jordan blocks and so it must be\n\n\\[\\left(\\begin{array}{cccccc}1&1&0&0&0&0\\\\ 0&1&1&0&0&0\\\\ 0&0&1&0&0&0\\\\ 0&0&0&1&0&0\\\\ 0&0&0&0&1&0\\\\ 0&0&0&0&0&1\\end{array}\\right).\\]"
    },
    "Problem 7.6.30": {
        "Problem": "_Let \\(A\\) be a real, upper-triangular, \\(n\\times n\\) matrix that commutes with its transpose. Prove that \\(A\\) is diagonal._",
        "Solution": null
    },
    "Problem 7.6.31": {
        "Problem": "1. _Prove that a linear operator_ \\(T:\\mathbb{C}\\,^{n}\\to\\mathbb{C}\\,^{n}\\) _is diagonalizable if for all_ \\(\\lambda\\in\\mathbb{C}\\,,\\,\\ker(T-\\lambda I)^{n}=\\ker(T-\\lambda I)\\)_, where_ \\(I\\) _is the_ \\(n\\times n\\) _identity matrix._\n2. _Show that_ \\(T\\) _is diagonalizable if_ \\(T\\) _commutes with its conjugate transpose_ \\(T^{\\star}\\) _(i.e.,_ \\((T^{\\star})_{jk}=\\overline{T_{kj}}\\)_)._",
        "Solution": null
    },
    "Problem 7.6.32": {
        "Problem": "_Let \\(A\\) be an \\(n\\times n\\) complex matrix. Prove there is a unitary matrix \\(U\\) such that \\(B=UAU^{-1}\\) is upper triangular: \\(B_{jk}=0\\) for \\(j>k\\).\\_",
        "Solution": null
    },
    "Problem 7.6.33": {
        "Problem": "_Let \\(b\\) be a real nonzero \\(n\\times 1\\) matrix (a column vector). Set \\(M=bb^{t}\\) (an \\(n\\times n\\) matrix) where \\(b^{t}\\) denotes the transpose of \\(b\\)._\n\n1. _Prove that there is an orthogonal matrix_ \\(Q\\) _such that_ \\(QMQ^{-1}=D\\) _is diagonal, and find_ \\(D\\)_._\n2. _Describe geometrically the linear transformation_ \\(M:\\mathbb{R}^{n}\\to\\mathbb{R}^{n}\\)_._",
        "Solution": null
    },
    "Problem 7.6.34": {
        "Problem": "_Let \\(M\\) be an invertible real \\(n\\times n\\) matrix. Show that there is a decomposition \\(M=UT\\) in which \\(U\\) is an \\(n\\times n\\) real orthogonal matrix and \\(T\\) is upper triangular with positive diagonal entries. Is this decomposition unique?_",
        "Solution": null
    },
    "Problem 7.6.35": {
        "Problem": "_Let \\(A\\) be a nonsingular real \\(n\\times n\\) matrix. Prove that there exists a unique orthogonal matrix \\(Q\\) and a unique positive definite symmetric matrix \\(B\\) such that \\(A=QB\\)._",
        "Solution": "Since \\(A\\) is nonsingular, \\(A^{t}A\\) is positive definite. Let \\(B=\\sqrt{A^{t}A}\\). Consider \\(P=BA^{-1}\\). Then \\(PA=B\\), so it suffices to show that \\(P\\) is orthogonal, for in that case, \\(Q=P^{-1}=P^{\\star}\\) will be orthogonal and \\(A=QB\\). We have\n\n\\[P^{t}P=(A^{t})^{-1}B^{t}BA^{-1}=(A^{t})^{-1}B^{2}A^{-1}=(A^{t})^{-1}A^{t}AA^{- 1}=1.\\]\n\nSuppose that we had a second factorization \\(A=Q*{1}B*{1}\\). Then\n\n\\[B^{2}=A^{t}A=B*{1}^{t}Q*{1}^{t}Q*{1}B*{1}=B\\_{1}^{2}.\\]\n\nSince a positive matrix has a unique positive square root, it follows that \\(B=B*{1}\\). As \\(A\\) is invertible, \\(B\\) is invertible, and canceling gives \\(Q=Q*{1}\\)."
    },
    "Problem 7.6.36": {
        "Problem": "_Let \\(A\\) be the 3\\(\\times\\)3 matrix_\n\n\\[\\left(\\begin{array}{ccc}1&-1&0\\\\ -1&2&-1\\\\ 0&-1&1\\end{array}\\right).\\]\n\n_Determine all real numbers a for which the limit \\(\\lim_{n\\to\\infty}a^{n}A^{n}\\) exists and is nonzero (as a matrix).\\_",
        "Solution": "An easy calculation shows that \\(A\\) has eigenvalues 0, 1, and 3, so \\(A\\) is similar to the diagonal matrix with entries 0, 1, and 3. Since clearly the problem does not change when \\(A\\) is replaced by a similar matrix, we may replace \\(A\\) by that diagonal matrix. Then the condition on \\(a\\) is that each of the sequences \\((0^{n})\\), \\((a^{n})\\), and \\(((3a)^{n})\\) has a limit, and that at least one of these limits is nonzero. This occurs if and only if \\(a=1/3\\)."
    },
    "Problem 7.6.37": {
        "Problem": "_Suppose \\(p\\) is a prime. Show that every element of \\(GL_{2}(\\mathbb{F}_{p})\\) has order dividing either \\(p^{2}-1\\) or \\(p(p-1)\\)._\n\n### Similarity",
        "Solution": "Let \\(g\\) be an element of the group. Consider the Jordan Canonical Form [11, pag. 247] of the matrix \\(g\\) in \\(\\mathbf{F}_{p^{2}}^{\\star}\\) a quadratic extension of \\(\\mathbf{F}_{p}\\). The characteristic polynomial has degree 2 and is either irreducible in \\(\\mathbf{F}\\_{p}\\) and the canonical form is diagonal with two conjugate entries in the extension or reducible with the Jordan Canonical Form havingthe same diagonal elements and a \\(1\\) in the upper right-hand corner. In the first case, we can see that \\(g^{p^{2}-1}=I\\) and in the second \\(g^{p(p-1)}=I\\).\n\n### 7.7 Similarity"
    },
    "Problem 7.7.2": {
        "Problem": "_Which pairs of the following matrices are similar?_\n\n\\[\\left(\\begin{array}{cc}1&0\\\\ 0&1\\end{array}\\right),\\ \\ \\ \\ \\left(\\begin{array}{cc}0&1\\\\ 1&0\\end{array}\\right),\\ \\ \\ \\ \\left(\\begin{array}{cc}1&0\\\\ 1&1\\end{array}\\right),\\ \\ \\ \\ \\left(\\begin{array}{cc}0&-1\\\\ 1&0\\end{array}\\right),\\]\n\n\\[\\left(\\begin{array}{cc}1&0\\\\ 1&-1\\end{array}\\right),\\ \\ \\ \\ \\left(\\begin{array}{cc}1&5\\\\ 0&-1\\end{array}\\right),\\ \\ \\ \\ \\left(\\begin{array}{cc}1&5\\\\ 0&1\\end{array}\\right).\\]",
        "Solution": null
    },
    "Problem 7.7.3": {
        "Problem": "_Which of the following matrices are similar as matrices over \\(\\mathbb{R}\\)?_\n\n\\[(a)\\left(\\begin{array}{ccc}1&0&0\\\\ 0&1&0\\\\ 0&0&1\\end{array}\\right),\\ \\ (b)\\left(\\begin{array}{ccc}0&0&1\\\\ 0&1&0\\\\ 1&0&0\\end{array}\\right),\\ \\ (c)\\left(\\begin{array}{ccc}1&0&0\\\\ 1&1&0\\\\ 0&0&1\\end{array}\\right),\\]\n\n\\[(d)\\left(\\begin{array}{ccc}1&0&0\\\\ 1&1&0\\\\ 0&1&1\\end{array}\\right),\\ \\ (e)\\left(\\begin{array}{ccc}1&1&0\\\\ 0&1&0\\\\ 0&0&1\\end{array}\\right),\\ \\ (f)\\left(\\begin{array}{ccc}0&1&1\\\\ 0&1&0\\\\ 1&0&0\\end{array}\\right).\\]",
        "Solution": null
    },
    "Problem 7.7.4": {
        "Problem": "_Let \\(M\\) be an \\(n\\times n\\) complex matrix. Let \\(G_{M}\\) be the set of complex numbers \\(\\lambda\\) such that the matrix \\(\\lambda M\\) is similar to \\(M\\).\\_\n\n1. _What is_ \\(G*{M}\\) \\_if* \\[M=\\left(\\begin{array}{ccc}0&0&4\\\\ 0&0&0\\\\ 0&0&0\\end{array}\\right)?\\]\n2. _Assume_ \\(M\\) _is_ not _nilpotent. Prove_ \\(G*{M}\\) \\_is finite.*",
        "Solution": null
    },
    "Problem 7.7.7": {
        "Problem": "_Prove that if \\(A\\) is a 2\\(\\times\\)2 matrix over the integers such that \\(A^{n}=I\\) for some strictly positive integer \\(n\\), then \\(A^{12}=I\\)._",
        "Solution": null
    },
    "Problem 7.7.8": {
        "Problem": "_Exhibit a set of 2\\(\\times\\)2 real matrices with the following property: A matrix \\(A\\) is similar to exactly one matrix in \\(S\\) provided \\(A\\) is a 2\\(\\times\\)2 invertible matrix of integers with all the roots of its characteristic polynomial on the unit circle._",
        "Solution": null
    },
    "Problem 7.7.9": {
        "Problem": "_Let \\(G\\) be a finite multiplicative group of 2\\(\\times\\)2 integer matrices._\n\n1. _Let_ \\(A\\in G\\)_. What can you prove about_ 1. \\(\\det A\\)_?_ 2. _the (real or complex) eigenvalues of A?_ 3. _the Jordan or Rational Canonical Form of A?_ 4. _the order of A?_\n2. _Find all such groups up to isomorphism._\n\n_See also Problem 7.6.11._",
        "Solution": "\\(1\\). Let \\(A\\) be any element of the group:\n\n1. Every element in a finite group has finite order, so there is an \\(n>0\\) such that \\(A^{n}=I\\). Therefore, \\((\\det A)^{n}=\\det(A^{n})=1\\). But \\(A\\) is an integer matrix, so \\(\\det A\\) must be \\(\\pm 1\\).\n2. If \\(\\lambda\\) is an eigenvalue of \\(A\\), then \\(\\lambda^{n}=1\\), so each eigenvalue has modulo \\(1\\), and at the same time, \\(\\lambda\\) is a root of a second degree monic characteristic polynomial \\(\\chi\\_{A}(x)=x^{2}+ax+b\\) for \\(A\\). If \\(|\\lambda|=1\\) then \\(b=\\pm 1\\) and \\(a=0\\), \\(\\pm 1\\), and \\(\\pm 2\\) since all roots are in the unit circle. Writing out all \\(10\\) polynomials and eliminating the ones whose roots are not in the unit circle, we are left with \\(x^{2}\\pm 1\\), \\(x^{2}\\pm x+1\\), and \\(x^{2}\\pm 2x+1\\), and the possible roots are \\(\\lambda=\\pm 1\\), \\(\\pm i\\), and \\(\\frac{1\\pm\\sqrt{3}i}{2}\\) and \\(\\frac{-1\\pm\\sqrt{3}i}{2}\\), the sixth roots of unity.\n\n3. The Jordan Canonical Form [11, pag. 247] of \\(A\\), \\(J*{A}\\), must be diagonal, otherwise it would be of the form \\(J*{A}=\\left(\\begin{smallmatrix}x&1\\\\ 0&x\\end{smallmatrix}\\right)\\), and the subsequent powers \\((J\\_{A})^{k}=\\left(\\begin{smallmatrix}x^{k}&kx^{k-1}\\\\ 0&x^{k}\\end{smallmatrix}\\right)\\), which is never the identity matrix since \\(kx^{k-1}\\neq 0\\) (remember \\(|x|=1\\)). So the Jordan Canonical Form of \\(A\\) is diagonal, with the root above and the complex roots occurring in conjugate pairs only. The Rational Canonical Form [11, pag. 238] can be read off from the possible polynomials.\n4. \\(A\\) can only have order 1, 2, 3, 4, or 6, depending on \\(\\lambda\\)."
    },
    "Problem 7.7.11": {
        "Problem": "_Let \\(A\\) be a linear transformation on an n-dimensional vector space over \\(\\mathbb{C}\\) such that \\(\\det(xI-A)=(x-1)^{n}\\). Prove that \\(A\\) is similar to \\(A^{-1}\\)._",
        "Solution": "The minimal polynomial of \\(A\\) divides \\((x-1)^{n}\\), so \\(I-A\\) is nilpotent, say of order \\(r\\). Thus, \\(A\\) is invertible with\n\n\\[A^{-1}=(I-(I-A))^{-1}=\\sum\\_{j=0}^{r-1}\\left(I-A\\right)^{j}.\\]Suppose first that \\(A\\) is just a single Jordan block [11, pag. 247], say with matrix\n\n\\[\\left(\\begin{array}{ccccc}1&1&0&\\cdots&0\\\\ 0&1&1&\\cdots&0\\\\ \\vdots&\\vdots&\\vdots&\\ddots&\\vdots\\\\ 0&0&0&\\cdots&1\\end{array}\\right)\\]\n\nrelative to the basis \\(\\{v*{1},v*{2},\\ldots,v*{n}\\}\\). Then \\(A^{-1}\\) has the same matrix relative to the basis \\(\\{v*{1},v*{1}+v*{2},\\cdots,v*{1}+v*{2}+\\cdots+v\\_{n}\\}\\), so \\(A\\) and \\(A^{-1}\\) are similar.\n\nIn the general case, by the theory of Jordan Canonical Form, the vector space can be written as a direct sum of \\(A\\)-invariant subspaces on each of which \\(A\\) acts as a single Jordan block. By the formula above for \\(A^{-1}\\), each subspace in the decomposition is \\(A^{-1}\\)-invariant, so the direct sum decomposition of \\(A\\) is also one of \\(A^{-1}\\). The general case thus reduces to the case where \\(A\\) is a single Jordan block."
    },
    "Problem 7.7.12": {
        "Problem": "_Prove or disprove: A square complex matrix, \\(A\\), is similar to its transpose, \\(A^{t}\\)._",
        "Solution": "The statement is true. First of all, \\(A\\) is similar to a Jordan matrix [11, pag. 247], \\(A=S^{-1}JS\\), where \\(S\\) is invertible and \\(J\\) is a direct sum of Jordan blocks. Then \\(A^{t}=S^{t}J^{t}(S^{t})^{-1}\\) (since \\((S^{-1})^{t}=(S^{t})^{-1}\\)); that is, \\(A^{t}\\) is similar to \\(J^{t}\\). Moreover, \\(J^{t}\\) is the direct sum of the transposes of the Jordan blocks whose direct sum is \\(J\\). It will, thus, suffice to prove that each of these Jordan blocks is similar to its transpose. In other words, it will suffice to prove the statement for the case where \\(A\\) is a Jordan block.\n\nLet \\(A\\) be an \\(n\\times n\\) Jordan block:\n\n\\[A=\\left(\\begin{array}{ccccc}\\lambda&1&0&\\cdots&0\\\\ 0&\\lambda&1&\\cdots&\\vdots\\\\ \\vdots&\\vdots&\\ddots&\\ddots&0\\\\ 0&0&\\cdots&\\lambda&1\\end{array}\\right)\\]\n\nLet \\(e*{1},\\ldots,e*{n}\\) be the standard basis vectors for \\(\\mathbb{C}\\)\\({}^{n}\\), so that \\(Ae*{j}=\\lambda e*{j}+e*{j-1}\\) for \\(j>1\\) and \\(Ae*{1}=\\lambda e*{1}\\). Let the matrix \\(S\\) be defined by \\(Se*{j}=e\\_{n-j+1}\\). Then \\(S=S^{-1}\\), and\n\n\\[S^{-1}ASe*{j} =SAe*{n-j}\\] \\[=\\left\\{\\begin{array}{ll}S(\\lambda e*{n-j+1}+e*{n-j})\\,&j<n\\\\ S(\\lambda e*{n-j+1})\\,&j=n\\end{array}\\right.\\] \\[=\\left\\{\\begin{array}{ll}\\lambda e*{j}+e*{j+1}\\,&j<n\\\\ \\lambda e*{j}\\,&j=n\\end{array}\\right.\\]\n\nwhich shows that \\(S^{-1}AS=A^{t}\\)."
    },
    "Problem 7.7.13": {
        "Problem": "_Let \\(M\\) be a real nonsingular 3\\(\\times\\)3 matrix. Prove there are real matrices \\(S\\) and \\(U\\) such that \\(M=SU=US\\), all the eigenvalues of \\(U\\) equal \\(1\\), and \\(S\\) is diagonalizable over \\(\\mathbb{C}\\)._",
        "Solution": null
    },
    "Problem 7.7.15": {
        "Problem": "_Let \\(A\\) and \\(B\\) be nonsingular \\(n\\times n\\) complex matrices with the same minimal and the same characteristic polynomial. Show that \\(n\\geq 4\\) and the minimal polynomial is not equal to the characteristic polynomial._",
        "Solution": "Since \\(A\\) and \\(B\\) have the same characteristic polynomial, they have the same \\(n\\) distinct eigenvalues \\(l*{1},\\ldots,l*{n}\\). Let \\(\\chi(x)=(x-l*{1})^{c*{1}}\\cdots(x-l*{n})^{c*{n}}\\) be the characteristic polynomial and let \\(\\mu(x)=(x-l*{1})^{m*{1}}\\cdots(x-l*{n})^{m*{n}}\\) be the minimal polynomial. Since a nondiagonal Jordan block [11, pag. 247] must be at least \\(2\\times 2\\), there can be, at most, one nondiagonal Jordan block for \\(N\\leq 3\\). Hence, the Jordan Canonical Form is completely determined by \\(\\mu(x)\\) and \\(\\chi(x)\\) for \\(N\\leq 3\\). If \\(\\mu(x)=\\chi(x)\\), then each distinct eigenvalue corresponds to a single Jordan block of size equal to the multiplicity of the eigenvalue as a root of \\(\\chi(x)\\), so the Jordan Canonical Form is completely determined by \\(\\chi(x)\\), and \\(A\\) and \\(B\\) must then be similar.\n\n### 8.8 Bilinear, Quadratic Forms, and Inner Product Spaces"
    },
    "Problem 7.7.16": {
        "Problem": "_Let \\(A\\) be an \\(n\\times n\\) complex matrix with \\(\\operatorname{trace}(A)=0\\). Show that \\(A\\) is similar to a matrix with all \\(0\\)'s along the main diagonal._\n\n### 7.8 Bilinear, Quadratic Forms, and Inner Product Spaces",
        "Solution": null
    },
    "Problem 7.8.1": {
        "Problem": "_Let \\(A,B,\\ldots,F\\) be real coefficients. Show that the quadratic form_\n\n\\[Ax^{2}+2Bxy+Cy^{2}+2Dxz+2Eyz+Fz^{2}\\]\n\n_is positive definite if and only if_\n\n\\[A>0,\\qquad\\left|\\begin{array}{cc}A&B\\\\ B&C\\end{array}\\right|>0,\\qquad\\left|\\begin{array}{ccc}A&B&D\\\\ B&C&E\\\\ D&E&F\\end{array}\\right|>0.\\]",
        "Solution": null
    },
    "Problem 7.8.2": {
        "Problem": "_Let \\(\\mathbb{R}^{3}\\) be 3-space with the usual inner product, and \\((a,b,c)\\in\\mathbb{R}^{3}\\) a vector of length \\(1\\). Let \\(W\\) be the plane defined by \\(ax+by+cz=0\\). Find, in the standard basis, the matrix representing the orthogonal projection of \\(\\mathbb{R}^{3}\\) onto \\(W\\)._",
        "Solution": "Every vector in \\(W\\) is orthogonal to \\(v=(a,b,c)\\). Let \\(Q\\) be the orthogonal projection of \\(\\mathbb{R}^{3}\\) onto the space spanned by \\(v\\), identified with its matrix. The columns of \\(Q\\) are \\(Qe*{j}\\), \\(1\\leq j\\leq 3\\), where the \\(e*{j}\\)'s are the standard basis vectors in \\(\\mathbb{R}^{3}\\). But\n\n\\[Qe*{1}=\\langle v,e*{1}\\rangle v=(a^{2},ab,ac)\\]\n\n\\[Qe*{2}=\\langle v,e*{2}\\rangle v=(ab,b^{2},bc)\\]\n\n\\[Qe*{3}=\\langle v,e*{3}\\rangle v=(ac,bc,c^{2}).\\]\n\nTherefore, the orthogonal projection onto \\(W\\) is given by\n\n\\[P=I-Q=\\left(\\begin{array}{ccc}1-a^{2}&-ab&-ac\\\\ -ab&1-b^{2}&-bc\\\\ -ac&-bc&1-c^{2}\\end{array}\\right).\\]"
    },
    "Problem 7.8.3": {
        "Problem": "_Let \\(w\\) be a positive continuous function on \\([0,1]\\), \\(n\\) a positive integer, and \\(P_{n}\\) the vector space of real polynomials whose degrees are at most \\(n\\), equipped with the inner product\\_\n\n\\[\\langle p,q\\rangle=\\int\\_{0}^{1}\\ p(t)q(t)w(t)\\ dt\\,.\\]\n\n1. _Prove that_ \\(P*{n}\\) \\_has an orthonormal basis* \\(p*{0},p*{1},\\ldots,p*{n}\\) *(i.e.,_ \\(\\langle p_{j},p*{k}\\rangle=1\\) \\_for* \\(j=k\\) _and_ \\(0\\) _for_ \\(j\\neq k\\)_) such that_ \\(\\deg p*{k}=k\\) \\_for each* \\(k\\)_._\n2. _Prove that_ \\(\\langle p*{k},p*{k}^{\\prime}\\rangle=0\\) _for each_ \\(k\\)_._",
        "Solution": "1. The monomials \\(1,t,t^{2},\\ldots,t^{n}\\) form a basis for \\(P*{n}\\). Applying the Gram-Schmidt Procedure [11, pag. 280] to this basis gives us an orthonormal basis \\(p*{0},p*{1},\\ldots,p*{n}\\). The \\((k+1)^{th}\\) vector in the latter basis, \\(p*{k}\\), is a linear combination of \\(1,t,\\ldots,t^{k}\\), the first \\(k+1\\) vectors in the former basis, with \\(t^{k}\\) having a nonzero coefficient. (This is built into the Gram-Schmidt Procedure.) Hence, \\(\\deg\\,p*{k}=k\\).\n\n2. Since \\(p*{k}^{\\prime}\\) has degree \\(k-1\\), it is a linear combination of \\(p*{0},p*{1},\\ldots,p*{k-1}\\), for those functions form an orthonormal basis for \\(P*{k-1}\\). Since \\(p*{k}\\) is orthogonal to \\(p*{0},p*{1},\\ldots,p*{k-1}\\), it is orthogonal to \\(p*{k}^{\\prime}\\)."
    },
    "Problem 7.8.4": {
        "Problem": "_For continuous real valued functions \\(f,g\\) on the interval \\([-1,1]\\) define the inner product \\(\\langle f,g\\rangle=\\int_{-1}^{1}f(x)g(x)dx\\). Find that polynomial of the form \\(p(x)=a+bx^{2}-x^{4}\\) which is orthogonal on \\([-1,1]\\) to all lower order polynomials.\\_",
        "Solution": null
    },
    "Problem 7.8.6": {
        "Problem": "_Let \\(A\\) be a real \\(n\\times n\\) matrix such that \\(\\langle Ax,x\\rangle\\geq 0\\) for every real \\(n\\)-vector \\(x\\). Show that \\(Au=0\\) if and only if \\(A^{t}u=0\\)._",
        "Solution": null
    },
    "Problem 7.8.7": {
        "Problem": "_An n\\(\\times\\)n real matrix \\(T\\) is positive definite if \\(T\\) is symmetric and \\(\\langle Tx,x\\rangle>0\\) for all nonzero vectors \\(x\\in\\mathbb{R}^{n}\\), where \\(\\langle u,v\\rangle\\) is the standard inner product. Suppose that \\(A\\) and \\(B\\) are two positive definite real matrices._\n\n1. _Show that there is a basis_ \\(\\{v*{1},v*{2},\\ldots,v*{n}\\}\\) \\_of* \\(\\mathbb{R}^{n}\\) _and real numbers_ \\(\\lambda*{1},\\lambda*{2},\\ldots,\\lambda*{n}\\) \\_such that, for* \\(1\\leq i,j\\leq n\\)_:_ \\[\\langle Av*{i},v*{j}\\rangle=\\left\\{\\begin{array}{ll}1&i=j\\\\ 0&i\\neq j\\end{array}\\right.\\] _and_ \\[\\langle Bv*{i},v*{j}\\rangle=\\left\\{\\begin{array}{ll}\\lambda\\_{i}&i=j\\\\ 0&i\\neq j\\end{array}\\right.\\]\n2. _Deduce from Part 1 that there is an invertible real matrix_ \\(U\\) _such that_ \\(U^{t}AU\\) _is the identity matrix and_ \\(U^{t}BU\\) _is diagonal._",
        "Solution": "1. Since \\(A\\) is positive definite, one can define a new inner product \\(\\langle\\,,\\rangle\\_{A}\\) on \\(\\mathbb{R}^{n}\\) by\n\n\\[\\langle x,y\\rangle\\_{A}=\\langle Ax,y\\rangle.\\]\n\nThe linear operator \\(A^{-1}B\\) is a symmetric with respect to this inner product, that is,\n\n\\[\\langle A^{-1}Bx,y\\rangle*{A} =\\langle Bx,y\\rangle=\\langle x,B^{t}y\\rangle=\\langle x,By\\rangle\\] \\[=\\langle A^{-1}Ax,By\\rangle=\\langle Ax,A^{-1}By\\rangle=\\langle x,A^{-1}By\\rangle*{A}\\]\n\nSo there is a basis \\(\\{v*{1},...v*{n}\\}\\) of \\(\\mathbb{R}^{n}\\), orthonormal with respect to \\(\\langle\\,,\\rangle*{A}\\), in which the matrix for \\(A^{-1}B\\) is diagonal. This is the basis we are looking for; in particular, \\(v*{i}\\) is an eigenvector for \\(A^{-1}B\\), with eigenvalue \\(\\lambda\\_{i}\\) and\n\n\\[\\langle v*{i},v*{j}\\rangle*{A}=\\delta*{ij}\\]\n\n\\[\\langle Bv*{i},v*{j}\\rangle=\\langle A^{-1}Bv*{i},v*{j}\\rangle*{A}=\\langle \\lambda*{i}v*{i},v*{j}\\rangle*{A}=\\lambda*{i}\\delta\\_{ij}.\\]\n\n2. Let \\(U\\) be the matrix which takes the standard basis to \\(\\{v*{1},...v*{n}\\}\\) above, that is, \\(Ue*{i}=v*{i}\\). Since the \\(e\\_{i}\\) form an orthonormal basis, for any matrix\\(M\\),\n\n\\[Mx=\\sum*{j=1}^{n}\\langle Mx,e*{j}\\rangle e\\_{j}\\]\n\nin particular\n\n\\[U^{t}AUe*{i} =\\sum*{j=1}^{n}\\langle U^{t}AUe*{i},e*{j}\\rangle e*{j}\\] \\[=\\sum*{j=1}^{n}\\langle AUe*{i},Ue*{j}\\rangle e*{j}\\] \\[=\\sum*{j=1}^{n}\\langle Av*{i},v*{j}\\rangle e*{j}\\] \\[=\\sum*{j=1}^{n}\\delta*{ij}e*{j}=e\\_{i}\\]\n\nshowing that \\(U^{t}AU=I\\).\n\nUsing the same decomposition for \\(U^{t}BU\\), we have\n\n\\[U^{t}BUe*{i} =\\sum*{j=1}^{n}\\langle U^{t}BUe*{i},e*{j}\\rangle e*{j}\\] \\[=\\sum*{j=1}^{n}\\langle Bv*{i},v*{j}\\rangle e*{j}\\] \\[=\\sum*{j=1}^{n}\\lambda*{i}\\delta*{ij}e*{i}=\\lambda*{i}e\\_{i}\\]\n\nso \\(U^{t}BU\\) is diagonal."
    },
    "Problem 7.8.8": {
        "Problem": "_Let \\(V\\) be a real vector space of dimension \\(n\\), and let \\(S:V\\times V\\to\\mathbb{R}\\) be a nondegenerate bilinear form. Suppose that \\(W\\) is a linear subspace of \\(V\\) such that the restriction of \\(S\\) to \\(W\\times W\\) is identically \\(0\\). Show that we have \\(\\dim W\\leq n/2\\)._",
        "Solution": null
    },
    "Problem 7.8.9": {
        "Problem": "_Let \\(A\\) be the symmetric matrix_\n\n\\[\\frac{1}{6}\\left(\\begin{array}{rr}13&-5&-2\\\\ -5&13&-2\\\\ -2&-2&10\\end{array}\\right).\\]\n\n_Let x denote the column vector_\n\n\\[\\left(\\begin{array}{c}x*{1}\\\\ x*{2}\\\\ x\\_{3}\\end{array}\\right)\\]\n\n\\(x*{i}\\in\\mathbb{R}\\)*, and let \\(x^{t}\\) denote its transpose \\((x*{1},x*{2},x*{3})\\). Let \\(|x|\\) denote the length of the vector \\(x\\). As x ranges over the set of vectors for which \\(x^{t}Ax=1\\), show that \\(|x|\\) is bounded, and determine its least upper bound.*",
        "Solution": null
    },
    "Problem 7.8.10": {
        "Problem": "_Define the index of a real symmetric matrix \\(A\\) to be the number of strictly positive eigenvalues of \\(A\\) minus the number of strictly negative eigenvalues. Suppose \\(A\\), and \\(B\\) are real symmetric \\(n\\times n\\) matrices such that \\(x^{t}Ax\\leq x^{t}Bx\\) for all \\(n\\times 1\\) matrices \\(x\\). Prove the the index of \\(A\\) is less than or equal to the index of \\(B\\)._",
        "Solution": null
    },
    "Problem 7.8.11": {
        "Problem": "_For \\(x,y\\in\\mathbb{C}\\,^{n}\\), let \\(\\langle x,y\\rangle\\) be the Hermitian inner product \\(\\sum_{j}x*{j}\\vec{y}*{j}\\). Let \\(T\\) be a linear operator on \\(\\mathbb{C}\\,^{n}\\) such that \\(\\langle Tx,Ty\\rangle=0\\) if \\(\\langle x,y\\rangle=0\\). Prove that \\(T=kS\\) for some scalar \\(k\\) and some operator \\(S\\) which is unitary: \\(\\langle Sx,Sy\\rangle=\\langle x,y\\rangle\\) for all \\(x\\) and \\(y\\).\\_",
        "Solution": null
    },
    "Problem 7.8.12": {
        "Problem": "_Let \\(E\\) denote a finite-dimensional complex vector space with a Hermitian inner product \\(\\langle x,y\\rangle\\)._\n\n1. _Prove that_ \\(E\\) _has an orthonormal basis._\n2. _Let_ \\(f:E\\to\\mathbb{C}\\,\\) _be such that_ \\(f(x,y)\\) _is linear in_ \\(x\\) _and conjugate linear in_ \\(y\\)_. Show there is a linear map_ \\(A:E\\to E\\) _such that_ \\(f(x,y)=\\langle Ax,y\\rangle\\)_._",
        "Solution": null
    },
    "Problem 7.8.13": {
        "Problem": "_Let \\(a\\) and \\(b\\) be real numbers. Prove that there are mutually orthogonal unit vectors \\(u\\) and \\(v\\) in \\(\\mathbb{R}^{3}\\) such that \\(u=(u_{1},u*{2},a)\\) and \\(v=(v*{1},v*{2},b)\\) if and only if \\(a^{2}+b^{2}\\leq 1\\).*\n\n### General Theory of Matrices",
        "Solution": "Suppose we have such \\(u\\) and \\(v\\). By Cauchy-Schwarz Inequality [10, pag. 69], we have\n\n\\[(u*{1}v*{1}+u*{2}v*{2})^{2}\\leq(u*{1}^{2}+u*{2}^{2})(v*{1}^{2}+v*{2}^{2}).\\]\n\nSince \\(u\\cdot v=0\\), \\((u*{1}v*{1}+u*{2}v*{2})^{2}=(ab)^{2}\\); since \\(\\|u\\|=\\|v\\|=1\\), \\(1-a^{2}=u*{1}^{2}+u*{2}^{2}\\), and \\(1-b^{2}=v*{1}^{2}+v*{2}^{2}\\). Combining these, we get\n\n\\[(ab)^{2}\\leq(1-a^{2})(1-b^{2})=1-a^{2}-b^{2}+(ab)^{2},\\]\n\nwhich implies \\(a^{2}+b^{2}\\leq 1\\).\n\nConversely, suppose that \\(a^{2}+b^{2}\\leq 1\\). Let \\(u=(0,\\sqrt{1-a^{2}},a)\\). \\(\\|u\\|=1\\), and we now find \\(v*{1}\\) and \\(v*{2}\\) such that \\(v*{1}^{2}+v*{2}^{2}+b^{2}=1\\) and \\(u*{2}v*{2}+ab=0\\). If \\(a=1\\), then \\(b=0\\), so we can take \\(v=(0,1,0)\\). If \\(a\\neq 1\\), solving the second equation for \\(v\\_{2}\\), we get\n\n\\[v*{2}=\\frac{-ab}{\\sqrt{1-a^{2}}}.\\]Using this to solve for \\(v*{1}\\), we get\n\n\\[v\\_{1}=\\frac{\\sqrt{1-a^{2}-b^{2}}}{\\sqrt{1-a^{2}}}.\\]\n\nBy our condition on \\(a\\) and \\(b\\), both of these are real, so \\(u\\) and \\(v=(v*{1},v*{2},b)\\) are the desired vectors.\n\n### 7.9 General Theory of Matrices"
    },
    "Problem 7.9.1": {
        "Problem": "_Prove the following three statements about real \\(n\\times n\\) matrices._\n\n1. _If_ \\(A\\) _is an orthogonal matrix whose eigenvalues are all different from_ \\(-1\\)_, then_ \\(I*{n}+A\\) \\_is nonsingular and* \\(S=(I*{n}-A)(I*{n}+A)^{-1}\\) _is skew-symmetric._\n2. _If_ \\(S\\) _is a skew-symmetric matrix, then_ \\(A=(I*{n}-S)(I*{n}+S)^{-1}\\) _is an orthogonal matrix with no eigenvalue equal to_ \\(-1\\)_._\n3. _The correspondence_ \\(A\\leftrightarrow S\\) _from Parts 1 and 2 is one-to-one._",
        "Solution": null
    },
    "Problem 7.9.2": {
        "Problem": "_Let \\(B\\) denote the matrix_\n\n\\[\\left(\\begin{array}{ccc}a&0&0\\\\ 0&b&0\\\\ 0&0&c\\end{array}\\right)\\]\n\n_where \\(a\\), \\(b\\), and \\(c\\) are real and \\(|a|\\), \\(|b|\\), and \\(|c|\\) are distinct. Show that there are exactly four symmetric matrices of the form \\(BQ\\), where \\(Q\\) is a real orthogonal matrix of determinant \\(1\\)._",
        "Solution": null
    },
    "Problem 7.9.3": {
        "Problem": "_Let \\(P\\) be a 9\\(\\times\\)9 real matrix such that \\(x^{t}Py=y^{t}Px\\) for all column vectors \\(x,y\\) in \\(\\mathbb{R}^{9}\\). Prove that \\(P\\) is singular._",
        "Solution": null
    },
    "Problem 7.9.4": {
        "Problem": "_Let \\(A\\) be a real skew-symmetric matrix \\((A_{ij}=-A*{ji})\\). Prove that \\(A\\) has even rank.*",
        "Solution": "We will use a powerful result on the structure of real normal operators, not commonly found in the literature. We provide also a second solution, not using the normal form, but which is inspired on it.\n\n**Lemma (Structure of Real Normal Operators):** Given a normal operator on an euclidean space \\(\\mathbb{R}^{n}\\), \\(\\Lambda\\), there exists an orthonormal basis in which the matrix of \\(\\Lambda\\) has the form\n\n\\[\\left(\\begin{array}{cc}\\left(\\begin{array}{cc}\\sigma*{1}&\\tau*{1}\\\\ -\\tau*{1}&\\sigma*{1}\\end{array}\\right)\\\\ &\\left(\\begin{array}{cc}\\sigma*{2}&\\tau*{2}\\\\ -\\tau*{2}&\\sigma*{2}\\end{array}\\right)\\\\ &&\\ddots\\\\ &&\\left(\\begin{array}{cc}\\sigma*{k}&\\tau*{k}\\\\ -\\tau*{k}&\\sigma*{k}\\end{array}\\right)\\\\ &&\\lambda*{2k+1}\\\\ &&\\ddots\\\\ &&\\lambda*{n}\\end{array}\\right)\\]\n\nwhere the numbers \\(\\lambda*{j}=\\sigma*{j}+i\\tau*{j}\\), \\(j=1,\\ldots,k\\) and \\(\\lambda*{2k+1},\\ldots,\\lambda\\_{n}\\) are the characteristic values of \\(\\Lambda\\).\n\nThe proof is obtained by embedding each component of \\(\\mathbb{R}^{n}\\) as the real slice of each component of \\(\\mathbb{C}\\,^{n}\\), extending \\(\\Lambda\\) to a normal operator on \\(\\mathbb{C}\\,^{n}\\), and noticing that the new operator has the same real matrix (on the same basis) and over \\(\\mathbb{C}\\,^{n}\\) has basis of characteristic vectors. A change of basis, picking the new vectors as the real and imaginary parts of the eigenvectors associated with the imaginary eigenvalues, reduces it to the desired form. For details on the proof we refer the reader to [20, pag. 265-271] or [19, pag. 117].\n\nThe matrix of an anti-symmetric operator \\(\\Lambda\\) has the property\n\n\\[a*{ij}=\\langle Ae*{i},e*{j}\\rangle=\\langle e*{i},A^{\\*}e*{j}\\rangle=\\langle e*{ i},-Ae*{j}\\rangle=-\\overline{\\langle Ae*{j},e*{i}\\rangle}=-\\overline{a*{ji}}\\,.\\]\n\nSince anti-symmetric operators are normal, they have a basis of characteristic values, these satisfy the above equality and are all pure imaginary.\n\nThus, in the standard decomposition described above all characteristic values are pure imaginary, i.e., \\(\\sigma*{1}=\\cdots=\\sigma*{k}=\\lambda*{2k+1}=\\cdots=\\lambda*{n}=0\\) and the decomposition in this case is\n\n\\[\\left(\\begin{array}{cc}\\left(\\begin{array}{cc}0&\\tau*{1}\\\\ -\\tau*{1}&0\\end{array}\\right)\\\\ &\\left(\\begin{array}{cc}0&\\tau*{2}\\\\ -\\tau*{2}&0\\end{array}\\right)\\\\ &&\\ddots\\\\ &&\\left(\\begin{array}{cc}0&\\tau*{k}\\\\ -\\tau*{k}&0\\end{array}\\right)\\\\ &&0\\\\ &&\\ddots\\\\ &&0\\end{array}\\right)\\]\n\nwhich obviously has even rank.\n\n_Solution 2_. Consider \\(\\hat{A}\\) the _complexification_ of \\(A\\), that is, the linear operator from \\(\\mathbb{C}\\,^{n}\\) to \\(\\mathbb{C}\\,^{n}\\) with the same matrix as \\(A\\) with respect to the standard basis. Since \\(A\\) is skew-symmetric, all its eigenvalues are pure imaginary and from the fact that the characteristic polynomial has real coefficients, the non-real eigenvalues show up in conjugate pairs, therefore, the polynomial has the form\n\n\\[\\chi*{A}(t)=t^{k}p*{1}(t)^{n*{1}}\\cdots p*{r}(t)^{n\\_{r}},\\]\n\nwhere the \\(p\\_{i}\\)'s are real, irreducible quadratics.\n\nFrom the diagonal form of \\(\\hat{A}\\) over \\(\\mathbb{C}\\,\\) we can see that the minimal polynomial has the factor in \\(t\\) with power \\(1\\), that is, of the form\n\n\\[\\mu*{A}(t)=\\mu*{\\hat{A}}(t)=tp*{1}(t)^{m*{1}}\\cdots p*{r}(t)^{m*{r}}.\\]\n\nNow consider the Rational Canonical Form [10, pag. 238] of \\(A\\). It is a block diagonal matrix composed of blocks of even size and full rank, together with a block of a zero matrix corresponding to the zero eigenvalues, showing that \\(A\\) has even rank."
    },
    "Problem 7.9.6": {
        "Problem": "_Suppose \\(A\\) is a real \\(n\\times n\\) matrix._\n\n1. _Is it true that_ \\(A\\) _must commute with its transpose?_\n2. _Suppose the columns of_ \\(A\\) _(considered as vectors) form an orthonormal set; is it true that the rows of_ \\(A\\) _must also form an orthonormal set?_",
        "Solution": null
    },
    "Problem 7.9.7": {
        "Problem": "_Let \\(M_{1}=\\left(\\begin{smallmatrix}3&2\\\\ 1&4\\end{smallmatrix}\\right)\\), \\(M*{2}=\\left(\\begin{smallmatrix}5&7\\\\ -3&-4\\end{smallmatrix}\\right)\\), \\(M*{3}=\\left(\\begin{smallmatrix}5&6.9\\\\ -3&-4\\end{smallmatrix}\\right)\\). For which (if any) \\(i\\), \\(1\\leq i\\leq 3\\), is the sequence \\((M*{i}^{n})\\) bounded away from \\(\\infty\\)? For which \\(i\\) is the sequence bounded away from \\(0\\)?*",
        "Solution": null
    },
    "Problem 7.9.8": {
        "Problem": "_Let \\(A\\) be an \\(n\\times n\\) complex matrix, all of whose eigenvalues are equal to \\(1\\). Suppose that the set \\(\\{A^{n}\\mid n=1,2,\\ldots\\}\\) is bounded. Show that \\(A\\) is the identity matrix._",
        "Solution": null
    },
    "Problem 7.9.9": {
        "Problem": "_Consider the complex 3\\(\\times\\)3 matrix_\n\n\\[A=\\left(\\begin{array}{ccc}a*{0}&a*{1}&a*{2}\\\\ a*{2}&a*{0}&a*{1}\\\\ a*{1}&a*{2}&a\\_{0}\\end{array}\\right),\\]\n\n_where \\(a_{0},a*{1},a*{2}\\in\\mathbb{C}\\).\\_\n\n1. _Show that_ \\(A=a*{0}I*{3}+a*{1}E+a*{2}E^{2}\\)_, where_ \\[E=\\left(\\begin{array}{ccc}0&1&0\\\\ 0&0&1\\\\ 1&0&0\\end{array}\\right).\\]\n2. _Use Part 1 to find the complex eigenvalues of_ \\(A\\)_._\n3. _Generalize Parts 1 and 2 to_ \\(n\\times n\\) _matrices._",
        "Solution": null
    },
    "Problem 7.9.10": {
        "Problem": "_Let \\(A\\) be a \\(n\\times n\\) real matrix._\n\n1. _If the sum of each column element of_ \\(A\\) _is_ \\(1\\) _prove that there is a nonzero column vector_ \\(x\\) _such that_ \\(Ax=x\\)_._\n2. _Suppose that_ \\(n=2\\) _and all entries in_ \\(A\\) _are positive. Prove there is a nonzero column vector_ \\(y\\) _and a number_ \\(\\lambda>0\\) _such that_ \\(Ay=\\lambda y\\)",
        "Solution": null
    },
    "Problem 7.9.11": {
        "Problem": "_Let the real \\(2n\\times 2n\\) matrix \\(X\\) have the form_\n\n\\[\\left(\\begin{array}{cc}A&B\\\\ C&D\\end{array}\\right)\\]\n\n_where \\(A\\), \\(B\\), \\(C\\), and \\(D\\) are \\(n\\times n\\) matrices that commute with one another. Prove that \\(X\\) is invertible if and only if \\(AD-BC\\) is invertible._",
        "Solution": "Let \\(Y=AD-BC\\). We have\n\n\\[\\left(\\begin{array}{cc}A&B\\\\ C&D\\end{array}\\right)\\left(\\begin{array}{cc}D&-B\\\\ -C&A\\end{array}\\right)=\\left(\\begin{array}{cc}AD-BC&-AB+BA\\\\ CD-DC&-CB+DA\\end{array}\\right)=\\left(\\begin{array}{cc}Y&0\\\\ 0&Y\\end{array}\\right).\\]\n\nIf \\(Y\\) is invertible, then so are \\(\\left(\\begin{array}{cc}Y&0\\\\ 0&Y\\end{array}\\right)\\) and \\(X\\).\n\nAssume now that \\(X\\) is invertible, and let \\(v\\) be vector in the kernel of \\(Y\\) : \\((AD-BC)v=0\\). Then\n\n\\[\\left(\\begin{array}{cc}A&B\\\\ C&D\\end{array}\\right)\\left(\\begin{array}{c}Dv\\\\ -Cv\\end{array}\\right)=0=\\left(\\begin{array}{cc}A&B\\\\ C&D\\end{array}\\right)\\left(\\begin{array}{c}-Bv\\\\ Av\\end{array}\\right)\\]\n\nimplying that \\(Dv=Cv=Bv=Av=0\\). But then \\(X\\left(\\begin{array}{c}v\\\\ v\\end{array}\\right)=0\\), so, by the invertibility of \\(X\\), \\(v=0\\), proving that \\(Y\\) is invertible."
    },
    "Problem 7.9.12": {
        "Problem": "_Let \\(B=(b_{ij})_{i,j=1}^{20}\\) be a real \\(20\\times 20\\) matrix such that_\n\n\\[b\\_{ii}=0\\quad for\\quad 1\\leq i\\leq 20,\\]\n\n\\[b\\_{ij}\\in\\{1,-1\\}\\quad for\\quad 1\\leq i,j\\leq 20,\\ \\ i\\neq j.\\]\n\n_Prove that \\(B\\) is nonsingular._",
        "Solution": "If \\(\\det B=0\\), then \\(\\det B\\equiv 0\\pmod{2}\\). Hence, if we can show that \\(\\det B\\neq 0\\) over the field \\(\\mathbb{Z}_{2}\\), we are done. In the field \\(\\mathbb{Z}_{2}\\), \\(1=-1\\), so \\(B\\) is equal to the matrix with zeros along the diagonal and \\(1\\)'s everywhere else. Since adding one row of a matrix to another row does not change the determinant, we can replace \\(B\\) by the matrix obtained by adding the first nineteen rows of \\(B\\) to the last row. Since each column of \\(B\\) contains exactly nineteen \\(1\\)'s, \\(B\\) becomes\n\n\\[\\left(\\begin{array}{cccccc}0&1&1&\\cdots&1&1\\\\ 1&0&1&\\cdots&1&1\\\\ \\vdots&\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\ 1&1&1&\\cdots&0&1\\\\ 1&1&1&\\cdots&1&1\\end{array}\\right).\\]\n\nBy adding the last row of \\(B\\) to each of the other rows, \\(B\\) becomes\n\n\\[\\left(\\begin{array}{cccccc}1&0&0&\\cdots&0&0\\\\ 0&1&0&\\cdots&0&0\\\\ \\vdots&\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\ 0&0&0&\\cdots&1&0\\\\ 1&1&1&\\cdots&1&1\\end{array}\\right).\\]\n\nThis is a lower-triangular matrix, so its determinant is the product of its diagonal elements. Hence, the determinant of \\(B\\) is equal to \\(1\\) over \\(\\mathbb{Z}\\_{2}\\), and we are done.\n\n_Solution 2._ In the matrix modulo \\(2\\), the sum of all columns except column \\(i\\) is the \\(i^{th}\\) standard basis vector, so the span of the columns has dimension \\(20\\), and the matrix is nonsingular."
    },
    "Problem 7.9.13": {
        "Problem": "_Let_\n\n\\[A=\\left(\\begin{array}{cc}1&2\\\\ 3&4\\end{array}\\right).\\]\n\n_Show that every real matrix \\(B\\) such that \\(AB=BA\\) has the form \\(sI+tA\\), where \\(s,t\\in\\mathbb{R}\\)._",
        "Solution": "Let \\(\\mathcal{C}\\) be the set of real matrices that commute with \\(A\\). It is clearly a vector space of dimension, at most, \\(4\\). The set \\(\\{sI+tA\\,|\\,s,t\\in\\mathbb{R}\\}\\) is a two-dimensional subspace of \\(\\mathcal{C}\\), so it suffices to show that there are two linearly independent matrices which do not commute with \\(A\\). A calculation show that the matrices \\(\\left(\\begin{smallmatrix}0&1\\\\ 1&0\\end{smallmatrix}\\right)\\) and \\(\\left(\\begin{smallmatrix}0&1\\\\ -1&0\\end{smallmatrix}\\right)\\) are such matrices."
    },
    "Problem 7.9.14": {
        "Problem": "_Let \\(A\\) be a 2\\(\\times\\)2 matrix over \\(\\mathbb{C}\\) which is not a scalar multiple of the identity matrix \\(I\\). Show that any 2\\(\\times\\)2 matrix \\(X\\) over \\(\\mathbb{C}\\) commuting with \\(A\\) has the form \\(X=\\alpha I+\\beta A\\), where \\(\\alpha,\\beta\\in\\mathbb{C}\\)._",
        "Solution": "Let\n\n\\[A=\\left(\\begin{array}{cc}a&b\\\\ c&d\\end{array}\\right)\\qquad\\text{ and }\\qquad X=\\left(\\begin{array}{cc}x&y\\\\ z&w\\end{array}\\right).\\]\n\nIf \\(AX=XA\\), we have the three equations: \\(bz=yc\\), \\(ay+bw=xb+yd\\), and \\(cx+dz=za+wc\\).\n\n- \\(b=c=0\\). Since \\(A\\) is not a multiple of the identity, \\(a\\neq d\\). The above equations reduce to \\(ay=dy\\) and \\(dz=az\\), which, in turn, imply that \\(y=z=0\\). Hence, \\[X=\\left(\\begin{array}{cc}x&0\\\\ 0&w\\end{array}\\right)=\\left(x-\\frac{a}{a-d}(x-w)\\right)\\operatorname{I}+\\left( \\frac{x-w}{a-d}\\right)A.\\]\\* \\(b\\neq 0\\) or \\(c\\neq 0\\). We can assume, without loss of generality, that \\(b\\neq 0\\), as the other case is identical. Then \\(z=cy/b\\) and \\(w=x-y(a-d)/b\\). Hence, \\[X=\\frac{1}{b}\\left(\\begin{array}{cc}bx-ay+ay&by\\\\ cy&bx-ay+dy\\end{array}\\right)=\\left(\\frac{bx-ay}{b}\\right)\\mathrm{I}+\\frac{y}{ b}A.\\]\n\nSolution to 7.9.17: Define the norm of a matrix \\(X=(x*{ij})\\) by \\(\\|X\\|=\\sum*{i,j}|x*{ij}|\\). Notice that if \\(B*{k}\\), \\(k=0,1,\\ldots\\), are matrices such that \\(\\sum\\|B*{k}\\|<\\infty\\), then \\(\\sum B*{k}\\) converges, because the convergence of the norms clearly implies the absolute entrywise convergence.\n\nIn our case, we have \\(B\\_{k}=A^{k}\\). The desired result follows from the fact that \\(\\sum\\|A\\|^{k}/k!\\) converges for any matrix \\(A\\).\n\nSolution to 7.9.18: Note that, if \\(A\\) is an invertible matrix, we have\n\n\\[Ae^{M}A^{-1}=e^{AMA^{-1}}\\]\n\nso we may assume that \\(M\\) is upper triangular. Under this assumption \\(e^{M}\\) is also upper triangular, and if \\(a*{1},\\ldots,a*{n}\\) are \\(M\\)'s diagonal entries, then the diagonal entries of \\(e^{M}\\) are\n\n\\[e^{a*{1}},\\ldots,e^{a*{n}}\\]\n\nand we get\n\n\\[\\det\\left(e^{M}\\right)=\\prod*{i=1}^{n}e^{a*{i}}=e^{\\sum*{i=1}^{n}a*{i}}=e^{ \\mathrm{tr}(M)}.\\]\n\nSolution to 7.9.23: 1. Let \\(\\alpha=\\frac{1}{n}\\mathrm{tr}(M)\\), so that \\(\\mathrm{tr}(M-\\alpha I)=0\\). Let\n\n\\[A=\\frac{1}{2}(M-M^{t}),\\qquad S=\\frac{1}{2}(M+M^{t})-\\alpha I.\\]\n\nThe desired conditions are then satisfied.\n\n2. We have\n\n\\[M^{2}=A^{2}+S^{2}+\\alpha^{2}I+2\\alpha A+2\\alpha S+AS+SA.\\]\n\nThe trace of \\(M\\) is the sum of the traces of the seven terms on the right. We have \\(\\mathrm{tr}(A)=\\mathrm{tr}(B)=0\\). Also,\n\n\\[\\mathrm{tr}(AS)=\\mathrm{tr}\\left((AS)^{t}\\right)=\\mathrm{tr}\\left(S^{t}A^{t} \\right)=\\mathrm{tr}(-SA)=-\\mathrm{tr}(SA),\\]\n\nso \\(\\mathrm{tr}(AS+SA)=0\\) (in fact, \\(\\mathrm{tr}(AS)=0\\) since \\(\\mathrm{tr}(AS)=\\mathrm{tr}(SA)\\)). The desired equality now follows."
    },
    "Problem 7.9.16": {
        "Problem": "_Consider the family of square matrices \\(A(\\theta)\\) defined by the solution of the matrix differential equation_\n\n\\[\\frac{dA(\\theta)}{d\\theta}=BA(\\theta)\\]\n\n_with the initial condition \\(A(0)=I\\), where \\(B\\) is a constant square matrix._\n\n1. _Find a property of_ \\(B\\) _which is necessary and sufficient for_ \\(A(\\theta)\\) _to be_ orthogonal _for all_ \\(\\theta\\)_; that is,_ \\(A(\\theta)^{t}=A(\\theta)^{-1}\\)_, where_ \\(A(\\theta)^{t}=\\)transpose of \\(A(\\theta)\\)_._ _Hint: What is_ \\(\\frac{d}{d\\theta}A^{-1}(\\theta)\\)2. _Find the matrices_ \\(A(\\theta)\\) _corresponding to_ \\[B=\\left(\\begin{array}{cc}0&1\\\\ -1&0\\end{array}\\right)\\] _and give a geometric interpretation._",
        "Solution": null
    },
    "Problem 7.9.17": {
        "Problem": "_Let \\(A\\) be an \\(r\\times r\\) matrix of real numbers. Prove that the infinite sum_\n\n\\[e^{A}=I+A+\\frac{A^{2}}{2}+\\cdots+\\frac{A^{n}}{n!}+\\cdots\\]\n\n_of matrices converges (i.e., for each \\(i,j\\), the sum of \\((i,j)^{th}\\) entries converges), and hence that \\(e^{A}\\) is a well-defined matrix._",
        "Solution": null
    },
    "Problem 7.9.18": {
        "Problem": "_Show that_\n\n\\[\\det(\\exp(M))=e^{\\operatorname{tr}(M)}\\]\n\n_for any complex \\(n\\times n\\) matrix \\(M\\), where \\(\\exp(M)\\) is defined as in Problem 7.9.17._",
        "Solution": null
    },
    "Problem 7.9.19": {
        "Problem": "_Let \\(T\\) be an \\(n\\times n\\) complex matrix. Show that_\n\n\\[\\lim\\_{k\\to\\infty}T^{k}=0\\]\n\n_if and only if all the eigenvalues of \\(T\\) have absolute value less than \\(1\\)._",
        "Solution": null
    },
    "Problem 7.9.20": {
        "Problem": "_Let \\(A\\) and \\(B\\) be n\\(\\times\\)n complex matrices. Prove that_\n\n\\[|\\operatorname{tr}(AB^{_})|^{2}\\leq\\operatorname{tr}(AA^{_})\\operatorname{ tr}(BB^{\\*}).\\]",
        "Solution": null
    },
    "Problem 7.9.21": {
        "Problem": "_Let \\(F(t)=(f_{ij}(t))\\) be an n\\(\\times\\)n matrix of continuously differentiable functions \\(f*{ij}:\\mathbb{R}\\to\\mathbb{R}\\), and let*\n\n\\[u(t)=\\operatorname{tr}\\left(F(t)^{3}\\right).\\]\n\n_Show that \\(u\\) is differentiable and_\n\n\\[u^{\\prime}(t)=3\\operatorname{tr}\\left(F(t)^{2}F^{\\prime}(t)\\right).\\]",
        "Solution": null
    },
    "Problem 7.9.22": {
        "Problem": "_Let \\(A\\) and \\(B\\) be n\\(\\times\\)n real matrices, and \\(k\\) a positive integer. Find_\n\n1. \\[\\lim*{t\\to 0}\\frac{1}{t}\\left((A+tB)^{k}-A^{k}\\right).\\]2. \\[\\frac{d}{dt}\\mathrm{tr}\\left(A+tB\\right)^{k}\\bigg{|}*{t=0}.\\]",
        "Solution": null
    },
    "Problem 7.9.23": {
        "Problem": "1. _Prove that any real_ \\(n\\times n\\) _matrix_ \\(M\\) _can be written as_ \\(M=A+S+cI\\)_, where_ \\(A\\) _is antisymmetric,_ \\(S\\) _is symmetric,_ \\(c\\) _is a scalar,_ \\(I\\) _is the identity matrix, and_ \\(\\mathrm{tr}\\,S=0\\)_._\n2. _Prove that with the above notation,_ \\[\\mathrm{tr}(M^{2})=\\mathrm{tr}(A^{2})+\\mathrm{tr}(S^{2})+\\frac{1}{n}(\\mathrm{ tr}\\,M)^{2}.\\]",
        "Solution": null
    },
    "Problem 7.9.24": {
        "Problem": "1. _Let_ \\(N\\) _be a nilpotent complex matrix. Let_ \\(r\\) _be a positive integer. Show that there is a_ \\(n\\times n\\) _complex matrix_ \\(A\\) _with_ \\[A^{r}=I+N.\\]",
        "Solution": null
    },
    "Problem 7.9.25": {
        "Problem": "1. _Let_ \\(A=\\left(a*{ij}\\right)*{i,j=1}^{n}\\) _be a real_ \\(n\\times n\\) _matrix such that_ \\(a*{ii}\\geq 1\\) \\_for all* \\(i\\)_, and_ \\[\\sum*{i\\neq j}a*{ij}^{2}<1.\\]\n\n_Prove that_ \\(A\\) _is invertible._",
        "Solution": "We will prove the equivalent result that the kernel of \\(A\\) is trivial. Let \\(x=(x*{1},\\ldots,x*{n})^{t}\\) be a nonzero vector in \\(\\mathbb{R}^{n}\\). We have\n\n\\[Ax\\cdot x =\\sum*{i,j=1}^{n}a*{ij}x*{i}x*{j}\\] \\[=\\sum*{i=1}^{n}a*{ii}x*{i}^{2}+\\sum*{i\\neq j}a*{ij}x*{i}x*{j}\\] \\[\\geq\\sum*{i=1}^{n}x*{i}^{2}-\\sum*{i\\neq j}|a*{ij}x*{i}x*{j}|\\] \\[\\geq\\sum*{i=1}^{n}x*{i}^{2}-\\sqrt{\\sum*{i\\neq j}a*{ij}^{2}\\sum*{i \\neq j}x*{i}^{2}x*{j}^{2}}\\] \\[>\\sum*{i=1}^{n}x*{i}^{2}-\\sqrt{\\sum*{i\\neq j}x*{i}^{2}x*{j}^{2}}\\] \\[\\geq\\sum*{i=1}^{n}x*{i}^{2}-\\sqrt{\\sum*{i=1}^{n}x*{i}^{2}\\sum*{j =1}^{n}x\\_{j}^{2}}\\] \\[\\geq 0.\\]\n\nSo \\(\\langle Ax,x\\rangle\\neq 0\\) and \\(Ax\\neq 0\\), therefore, the kernel of \\(A\\) is trivial."
    },
    "Problem 7.9.26": {
        "Problem": "1. _Show that an_ \\(n\\times n\\) _matrix of complex numbers A satisfying_ \\[|a*{ii}|>\\sum*{j\\neq i}|a\\_{ij}|\\]\n\n_for_ \\(1\\leq i\\leq n\\) _must be invertible._",
        "Solution": "Suppose \\(x\\) is in the kernel. Then\n\n\\[a*{ij}x*{i}=-\\sum*{j\\neq i}a*{ij}x\\_{j}\\]\n\nfor each \\(i\\). Let \\(i\\) be such that \\(|x*{i}|=\\max*{k}|x\\_{k}|=M\\), say. Then\n\n\\[|a*{ii}|M\\leq\\sum*{j\\neq i}|a*{ij}||x*{j}|\\leq\\sum*{j\\neq i}|a*{ij}|M\\]\n\nso\n\n\\[\\left(|a*{ii}|\\sum*{j\\neq i}|a\\_{ij}|\\right)M\\leq 0\\.\\]\n\nSince the therm inside the parenthesis is strictly positive by assumption, we must have \\(M=0\\), so \\(x=0\\) and \\(A\\) is invertible."
    },
    "Problem 7.9.27": {
        "Problem": "1. _Let_ \\(A=\\left(a*{ij}\\right)\\) \\_be an* \\(n\\times n\\) _matrix such that_ \\(\\sum*{j=1}^{n}|a*{ij}|<1\\) _for each_ \\(i\\)_. Prove that_ \\(I-A\\) _is invertible._",
        "Solution": "It will suffice to prove that \\(\\ker(I-A)\\) is trivial. Let \\(x=(x*{1},x*{2},\\cdots,x*{n})^{t}\\) be a nonzero vector in \\(\\mathbb{R}^{n}\\), and let \\(y=(I-A)x\\). Pick \\(k\\) such that \\(|x*{k}|=\\max\\{|x*{1}|,\\ldots,|x*{n}|\\}\\). Then\n\n\\[|y*{k}|=|x*{k}-\\sum*{j=1}^{n}a*{kj}x*{j}|\\]\\[\\geq|x*{k}|-\\sum*{j=1}^{n}|a*{kj}|\\ |x*{j}|\\] \\[\\geq|x*{k}|-\\sum*{j=1}^{n}|a*{kj}|\\ |x*{k}|\\] \\[=|x*{k}|\\left(1-\\sum*{j=1}^{n}|a*{kj}|\\right)>0\\.\\]\n\nHence, \\(y\\neq 0\\), as desired.\n\n_Solution 2._ Let \\(\\alpha<1\\) be a positive number such that, for all \\(i\\), we have \\(\\sum*{j=1}^{n}|a*{ij}|\\leq\\alpha\\). Then,\n\n\\[\\sum*{j,k}|a*{ij}a*{jk}|=\\sum*{j}\\left(|a*{ij}|\\sum*{k}|a*{jk}|\\right)\\leq \\alpha\\sum*{j}|a\\_{ij}|\\leq\\alpha^{2}\\.\\]\n\nAnd so, inductively, the sum of the absolute values of the terms in one row of \\(A^{n}\\) is bounded by \\(\\alpha^{n}\\). Thus, the entries in the infinite sum\n\n\\[I+A+A^{2}+A^{3}+\\cdots\\]\n\nare all bounded by the geometric series \\(1+\\alpha+\\alpha^{2}+\\cdots\\), and so are absolutely convergent; thus, this sum exists and the product\n\n\\[(I-A)(I+A+A^{2}+\\cdots)=I\\]\n\nis valid, so that the inverse of \\(I-A\\) is this infinite sum."
    },
    "Problem 7.9.28": {
        "Problem": "1. _Let_ \\(A\\) _be a real_ \\(n\\times n\\) _matrix. Let_ \\(M\\) _denote the maximum of the absolute values of the eigenvalues of_ \\(A\\)_._\n2. _Prove that if_ \\(A\\) _is symmetric, then_ \\(\\|Ax\\|\\leq M\\|x\\|\\) _for all_ \\(x\\) _in_ \\(\\mathbb{R}^{n}\\)_. (Here,_ \\(\\|\\cdot\\|\\) _denotes the Euclidean norm.)_\n3. _Prove that the preceding inequality can fail if_ \\(A\\) _is not symmetric._",
        "Solution": "1. If \\(A\\) is symmetric then, by the Spectral Theorem [10, pag. 335], [20, pag. 235], there is an orthonormal basis \\(\\{e*{1},e*{2},\\ldots,e*{n}\\}\\) for \\(\\mathbb{R}^{n}\\) with respect to which \\(A\\) is diagonal: \\(Ae*{j}=\\lambda*{j}e*{j}\\), \\(j=1,\\ldots,n\\). Let \\(x\\) be any vector in \\(\\mathbb{R}^{n}\\). We can write \\(x=c*{1}e*{1}+\\cdots+c*{n}e*{n}\\) for some scalars \\(c*{1},\\ldots,c*{n}\\), and have \\(Ax=\\lambda*{1}c*{1}e*{1}+\\cdots+\\lambda*{n}c*{n}e*{n}\\). Moreover,\n\n\\[\\|x\\|^{2} = c*{1}^{2}+\\cdots+c*{n}^{2}\\] \\[\\|Ax\\|^{2} = \\lambda*{1}^{2}c*{1}^{2}+\\cdots+\\lambda*{n}^{2}c*{n}^{2}\\] \\[\\leq \\max\\{\\lambda*{1}^{2},\\ldots,\\lambda*{n}^{2}\\}(c*{1}^{2}+\\cdots+c* {n}^{2})=M^{2}\\|x\\|^{2}\\,\\]\n\nwhich is the desired inequality.\n\n2. The matrix \\(\\left(\\begin{array}{cc}0&1\\\\ 0&0\\end{array}\\right)\\) gives a counterexample with \\(n=2\\). Its only eigenvalue is \\(0\\), yet it is not the zero matrix."
    },
    "Problem 7.9.29": {
        "Problem": "1. _Let_ \\(R\\) _be the ring of_ \\(n\\times n\\) _matrices over a field. Suppose_ \\(S\\) _is a ring and_ \\(h:R\\to S\\) _is a homomorphism. Show that_ \\(h\\) _is either injective or zero._\n\n### 6.1 Examples of Groups and General Theory",
        "Solution": null
    }
}